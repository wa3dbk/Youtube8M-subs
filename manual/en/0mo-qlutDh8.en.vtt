WEBVTT
Kind: captions
Language: en

00:00:00.580 --> 00:00:02.940
So let's also look at
what is the performance

00:00:02.940 --> 00:00:05.670
impact of simultaneous
multithreading.

00:00:05.670 --> 00:00:09.630
OK, so if you just write
a thread by itself,

00:00:09.630 --> 00:00:11.590
it's going to have
pretty high performance,

00:00:11.590 --> 00:00:13.820
because it has access
to the entire issueq,

00:00:13.820 --> 00:00:17.020
the entire reorder buffer,
all the four functional units,

00:00:17.020 --> 00:00:17.930
and so on, right?

00:00:17.930 --> 00:00:20.380
So that's the best way
to optimize performance

00:00:20.380 --> 00:00:21.820
for one thread.

00:00:21.820 --> 00:00:23.620
When I do simultaneous
multithreading, what

00:00:23.620 --> 00:00:25.380
I'm trying to do is
I'm trying to increase

00:00:25.380 --> 00:00:26.700
my overall throughput.

00:00:26.700 --> 00:00:29.780
I'm trying to maximize the
number of instructions that

00:00:29.780 --> 00:00:32.120
get executed every single cycle.

00:00:32.120 --> 00:00:33.700
And I'm trying to
minimize the under

00:00:33.700 --> 00:00:35.780
utilization of the processor.

00:00:35.780 --> 00:00:43.950
OK, so if one thread, let's say,
had an IPC of-- let's say 1.0.

00:00:43.950 --> 00:00:46.772
So if it ran by itself,
it has an IPC of 1.0.

00:00:46.772 --> 00:00:52.250
When I run two
threads together, each

00:00:52.250 --> 00:00:57.090
may have an IPC
of-- let's say 0.5.

00:00:57.090 --> 00:00:58.390
0.75.

00:00:58.390 --> 00:01:00.670
OK, so why does each
thread get slowed down?

00:01:00.670 --> 00:01:03.180
It is because all the
resources are being shed.

00:01:03.180 --> 00:01:05.250
Now one thread no
longer has access

00:01:05.250 --> 00:01:07.959
to all the registers, and
the entire reorder buffer,

00:01:07.959 --> 00:01:09.500
and all four functional
units, right?

00:01:09.500 --> 00:01:11.760
There are some cycles
where one of the threads

00:01:11.760 --> 00:01:14.280
is going to get slowed down
because some of the resources

00:01:14.280 --> 00:01:16.540
are being used by
the other thread.

00:01:16.540 --> 00:01:19.420
So when you do simultaneous
multithreading,

00:01:19.420 --> 00:01:22.040
each thread by itself
is going to slow down.

00:01:22.040 --> 00:01:27.890
But on average, my total
IPC is now 1.5, right?

00:01:27.890 --> 00:01:30.960
So before, in any given cycle,
only one out of the four units

00:01:30.960 --> 00:01:32.660
were being kept busy.

00:01:32.660 --> 00:01:36.260
Now in any given cycle, out
of the four units, 1.5 of them

00:01:36.260 --> 00:01:37.650
are being kept busy.

00:01:37.650 --> 00:01:41.370
So overall throughput and
utilization has gone up,

00:01:41.370 --> 00:01:43.000
and my under utilization
has gone down.

00:01:43.000 --> 00:01:45.530
So this is why I'm doing
simultaneous multithreading,

00:01:45.530 --> 00:01:47.914
to maximize my
overall throughput.

00:01:47.914 --> 00:01:49.830
But you have to realize
that this is happening

00:01:49.830 --> 00:01:51.540
at the expense of
the performance

00:01:51.540 --> 00:01:53.700
of each individual thread.

00:01:53.700 --> 00:01:55.140
And you could
minimize this effect

00:01:55.140 --> 00:01:56.810
by saying that of
these two threads,

00:01:56.810 --> 00:01:59.130
one will have really
high priority.

00:01:59.130 --> 00:02:00.740
OK, so as far as
possible, it'll get

00:02:00.740 --> 00:02:04.120
really close to its
standalone IPC of one.

00:02:04.120 --> 00:02:06.780
OK, so that's what you
can do by prioritizing

00:02:06.780 --> 00:02:08.810
one thread over the
other, and this just

00:02:08.810 --> 00:02:10.210
means that the
second thread just

00:02:10.210 --> 00:02:12.960
gets any leftover issue slots.

00:02:12.960 --> 00:02:22.380
OK, so it in that case the
IPCs might be 0.97 and 0.52,

00:02:22.380 --> 00:02:26.862
let's say, which gives you a
total throughput of 1.49 IPC.

00:02:26.862 --> 00:02:28.570
I'm just making up
numbers at this point,

00:02:28.570 --> 00:02:31.800
but this gives you an idea of
how at least one thread can

00:02:31.800 --> 00:02:34.019
come pretty close
to a standalone IPC,

00:02:34.019 --> 00:02:36.560
and simultaneous multithreading
will still improve throughput

00:02:36.560 --> 00:02:39.150
because there are many cycles
where the high priority

00:02:39.150 --> 00:02:41.880
thread is not using
up all the resources,

00:02:41.880 --> 00:02:44.680
and it can be use the
other idle slots to get

00:02:44.680 --> 00:02:45.770
at least some work done.

00:02:45.770 --> 00:02:48.760
And that helps improve
overall throughput.

00:02:48.760 --> 00:02:51.200
So one thing that people
have studied very carefully

00:02:51.200 --> 00:02:55.940
with SMT process is how
do you influence fetch,

00:02:55.940 --> 00:02:59.020
how would instructions get
injected into the execution

00:02:59.020 --> 00:03:01.550
engine to try and maximize
overall throughput?

00:03:01.550 --> 00:03:04.510
So you had to recognize when
some thread is being stalled.

00:03:04.510 --> 00:03:07.100
And that time you will
prioritize some other thread,

00:03:07.100 --> 00:03:08.210
for example.

00:03:08.210 --> 00:03:09.080
OK?

00:03:09.080 --> 00:03:13.410
So let me just talk
about one empirical study

00:03:13.410 --> 00:03:15.910
that was done to try and
understand this behavior.

00:03:15.910 --> 00:03:18.300
So this is a case study
of a real processor.

00:03:18.300 --> 00:03:21.760
This is a Pentium4, which
was built in the early 2000s,

00:03:21.760 --> 00:03:23.760
but its form of
multithreading is

00:03:23.760 --> 00:03:25.720
very similar to the
kind of SMT that you

00:03:25.720 --> 00:03:29.170
might see in modern
day Intel processors.

00:03:29.170 --> 00:03:31.290
So the Pentium4, and
even modern processors

00:03:31.290 --> 00:03:34.755
usually allow two threads
to execute at the same time.

00:03:34.755 --> 00:03:37.130
There are some resources that
are statically divided when

00:03:37.130 --> 00:03:38.420
you have two threads running.

00:03:38.420 --> 00:03:41.000
So when two threads run,
they get an equal share

00:03:41.000 --> 00:03:44.120
of the reorder buffer,
LSQ, and the issueq,

00:03:44.120 --> 00:03:47.420
and this end shows that one
thread which which tends

00:03:47.420 --> 00:03:50.230
to hog resources is
not going to cripple

00:03:50.230 --> 00:03:51.772
the throughput for
the second thread.

00:03:51.772 --> 00:03:54.146
Then there are some other
resources which are dynamically

00:03:54.146 --> 00:03:56.050
shared, such as the
trace cache, which

00:03:56.050 --> 00:03:59.757
is a cache to fetch
sequences of instructions.

00:03:59.757 --> 00:04:02.340
The decode units are dynamically
shared, the functional units,

00:04:02.340 --> 00:04:05.270
data cache, branch
predictor, and so on.

00:04:05.270 --> 00:04:09.050
OK, so let's look at how
the spec benchmarks behaved

00:04:09.050 --> 00:04:10.619
on this processor.

00:04:10.619 --> 00:04:12.410
OK, so this table is
a little hard to read.

00:04:12.410 --> 00:04:14.710
Let me explain what is going on.

00:04:14.710 --> 00:04:17.600
So let's say there are
26 programs over here.

00:04:17.600 --> 00:04:20.600
The first program is jZip.

00:04:20.600 --> 00:04:22.720
OK, so I've taken
jZip and I've run it

00:04:22.720 --> 00:04:25.810
with every one of
these other programs.

00:04:25.810 --> 00:04:29.240
So jZip has run
with JZip itself,

00:04:29.240 --> 00:04:33.350
it's run with the next program,
VPR, with GCC, and so on.

00:04:33.350 --> 00:04:35.850
So essentially, if you ran
it with 26 other programs,

00:04:35.850 --> 00:04:39.100
you would get 26 different
throughput numbers.

00:04:39.100 --> 00:04:41.190
And what is being
recorded in the stable

00:04:41.190 --> 00:04:44.290
is the best throughput I
observed for jZip and somebody

00:04:44.290 --> 00:04:47.520
else, the first throughput I
observed for jZip and somebody

00:04:47.520 --> 00:04:49.662
else, and what is
the average speed up?

00:04:49.662 --> 00:04:51.120
When I run jZip
with somebody else,

00:04:51.120 --> 00:04:54.840
what is the average
throughput I can expect?

00:04:54.840 --> 00:04:59.500
So you'll see that
the IPC ranges from--

00:04:59.500 --> 00:05:04.630
or we normalize IPC ranges
from 1.14 to 1.48, right?

00:05:04.630 --> 00:05:08.912
So if I ran jZip by itself,
the IPC would be 1.0.

00:05:08.912 --> 00:05:10.370
When I run jZip
with somebody else,

00:05:10.370 --> 00:05:13.750
the total IPC can
be as high as 1.48,

00:05:13.750 --> 00:05:17.100
and you could interpret that
as maybe jZip running at 0.74,

00:05:17.100 --> 00:05:20.045
and the other problem
running at 0.74.

00:05:20.045 --> 00:05:21.420
So this is similar
to the numbers

00:05:21.420 --> 00:05:23.770
I just had in my
earlier example.

00:05:23.770 --> 00:05:26.760
So if you look at the
final average throughput,

00:05:26.760 --> 00:05:28.930
it says 1.2.

00:05:28.930 --> 00:05:31.140
So if your programs
are well represented

00:05:31.140 --> 00:05:33.320
by the spec workloads,
then it that when

00:05:33.320 --> 00:05:36.300
you run two threads together,
your overall throughput,

00:05:36.300 --> 00:05:39.390
instead of being
1.0, becomes 1.2.

00:05:39.390 --> 00:05:41.720
So you can say that
on average, when

00:05:41.720 --> 00:05:44.270
I run two threads
together, each thread's IPC

00:05:44.270 --> 00:05:46.452
goes from 1.0 to 1.6.

00:05:46.452 --> 00:05:47.910
But since I'm
running two of these,

00:05:47.910 --> 00:05:51.900
the net effect is that my
throughput improves to 1.2.

00:05:51.900 --> 00:05:54.040
So SMT does help,
but it can severely

00:05:54.040 --> 00:05:57.860
impact the performance of
each individual thread.

