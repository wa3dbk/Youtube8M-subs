WEBVTT
Kind: captions
Language: en

00:00:04.360 --> 00:00:09.110
When we implant electrodes into brains
for therapeutic reasons

00:00:09.110 --> 00:00:12.410
all of a sudden we may be stimulating
regions of the brain

00:00:12.410 --> 00:00:17.029
and that means we're controlling regions
of the brain. That raises

00:00:17.029 --> 00:00:21.019
some ethical concerns.

00:00:21.019 --> 00:00:24.230
 

00:00:24.230 --> 00:00:28.880
If I can just parse this a little bit,

00:00:28.880 --> 00:00:32.619
so let me just say what neuro engineering 
is. It's really the confluence of three

00:00:32.619 --> 00:00:33.510
domains.

00:00:33.510 --> 00:00:36.610
It's the confluence of neuroscience,

00:00:36.610 --> 00:00:42.660
device technology, and computing. Those three
together form this sort of special

00:00:42.660 --> 00:00:47.420
neuro engineering domain.

00:00:47.420 --> 00:00:50.649
With that comes some ethical issues

00:00:50.649 --> 00:00:54.120
and in the sciences we're going to
benefit from a partnership

00:00:54.120 --> 00:00:59.429
with ethicist and philosophers. 
So scientists are trained to do science

00:00:59.429 --> 00:01:03.320
and it's not as if they can't talk about
values questions. I find generally they're

00:01:03.320 --> 00:01:06.300
very interested but they don't feel
trained to talk about it.

00:01:06.300 --> 00:01:09.680
So when we talk to the scientists and
engineers in the Center,

00:01:09.680 --> 00:01:12.600
they thought it was really important to
do something about these issues but they

00:01:12.600 --> 00:01:14.210
didn't feel prepared to do it

00:01:14.210 --> 00:01:18.700
themselves.

00:01:18.700 --> 00:01:21.980
There have been questions about-- or
interest anyway-- in doing deep brain

00:01:21.980 --> 00:01:22.850
stimulators

00:01:22.850 --> 00:01:27.370
for depression but now, let's say, it's on
a closed loop so any time your

00:01:27.370 --> 00:01:30.840
levels--neurotransmitter levels or
electrical activity levels--

00:01:30.840 --> 00:01:34.260
go down below a certain level, it
automatically boosts you back up.

00:01:34.260 --> 00:01:37.830
That might be good, but on the other hand,
something bad could happen in your life--

00:01:37.830 --> 00:01:38.920
your mother dies--

00:01:38.920 --> 00:01:42.480
of course you should feel poorly. You would
expect that

00:01:42.480 --> 00:01:46.570
but then if your levels go down and your
machine automatically boosts you back up,

00:01:46.570 --> 00:01:49.570
what would the internal feeling

00:01:49.570 --> 00:01:53.260
be like? 
Just

00:01:53.260 --> 00:01:57.170
having the graduate students and Sara
present

00:01:57.170 --> 00:02:02.570
in our space and asking us questions
raises our consciousness--

00:02:02.570 --> 00:02:06.280
our neural systems-- to be thinking a
little more

00:02:06.280 --> 00:02:11.629
about the ethical implications of what we do.

00:02:11.629 --> 00:02:14.950
I do a lot of work in disability studies
as well, another interdisciplinary

00:02:14.950 --> 00:02:16.209
program on campus.

00:02:16.209 --> 00:02:19.700
And one of the worries from disability
studies is we have a lot of investment

00:02:19.700 --> 00:02:21.129
in these technologies that are

00:02:21.129 --> 00:02:24.579
intended to help people with
disabilities but some people with

00:02:24.579 --> 00:02:27.200
disabilities may not actually want the
benefit that's

00:02:27.200 --> 00:02:31.260
intended for them or they may not see it
as a benefit.

00:02:31.260 --> 00:02:35.150
In some centers for neural engineering
they're working on exoskeletons

00:02:35.150 --> 00:02:39.470
that would allow a person who would
otherwise be using a wheelchair to stand

00:02:39.470 --> 00:02:40.319
up and move,

00:02:40.319 --> 00:02:43.840
with this sort of robotics
structure around their legs.

00:02:43.840 --> 00:02:46.879
But some people with disabilities would say,
"I'm not really interested in

00:02:46.879 --> 00:02:51.079
standing up. I'm fine using a wheelchair.
If there's a problem, it's that

00:02:51.079 --> 00:02:54.689
not enough places are accessible
and we need to work on that."

00:02:54.689 --> 00:02:58.269
Sara gives a really fantastic lecture on

00:02:58.269 --> 00:03:02.099
what the area of philosophy can do and
cannot do.

00:03:02.099 --> 00:03:05.299
It's actually interesting what it
doesn't do. It doesn't apply rules.

00:03:05.299 --> 00:03:08.709
It's not law. It's not legal judgment. It
doesn't

00:03:08.709 --> 00:03:12.069
issue a right or wrong. It issues a
question

00:03:12.069 --> 00:03:16.799
of not what could be done, but what
should be done.

00:03:16.799 --> 00:03:20.129
That "could-should" thinking is pretty

00:03:20.129 --> 00:03:24.519
unique to philosophy and it's
something that science

00:03:24.519 --> 00:03:26.180
should embrace more and more.

