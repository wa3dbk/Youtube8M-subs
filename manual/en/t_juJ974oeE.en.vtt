WEBVTT
Kind: captions
Language: en

00:00:00.859 --> 00:00:09.780
Could I.T. fix your broken heart?

00:00:09.780 --> 00:00:14.059
Valentine's Day is right around the corner,
and whether you look at it as a celebration

00:00:14.059 --> 00:00:19.930
of romance or a conspiracy among chocolatiers
and florists, you can't deny that love is

00:00:19.930 --> 00:00:21.260
all around us.

00:00:21.260 --> 00:00:24.840
Which made me think about a popular theme
in science fiction.

00:00:24.840 --> 00:00:29.570
The idea of humans and A.I. having a relationship,
like in the movie Her.

00:00:29.570 --> 00:00:30.570
Is that actually realistic?

00:00:30.570 --> 00:00:35.040
Will we ever see romantic relationships between
humans and, say, robots?

00:00:35.040 --> 00:00:39.650
You know, we've talked a lot about A.I. in
the past, about how it would be challenging

00:00:39.650 --> 00:00:41.920
to make a self-aware, conscious machine.

00:00:41.920 --> 00:00:47.390
I've also talked about how we humans develop
these odd emotional attachments to our gadgets,

00:00:47.390 --> 00:00:49.649
like robotic vacuum cleaners.

00:00:49.649 --> 00:00:53.210
But what happens when the robotic vacuum cleaners
start having feelings for us?

00:00:53.210 --> 00:00:58.410
I know it sounds silly but that hasn't stopped
people from asking these questions, like Dr.

00:00:58.410 --> 00:00:59.410
David Levy.

00:00:59.410 --> 00:01:05.720
He's a chess master who, in his chess tournament
days, noticed the rise of A.I. and now predicts

00:01:05.720 --> 00:01:10.980
that in 2050 it'll be legal, at least in some
places in the United States, for a human to

00:01:10.980 --> 00:01:12.700
marry a robot.

00:01:12.700 --> 00:01:19.220
Now he's not just some A.I. enthusiast, he
actually earned his PhD with a thesis on human-robotic

00:01:19.220 --> 00:01:20.270
relationships.

00:01:20.270 --> 00:01:23.770
And he says it's not just probable that this
is gonna happen - it's inevitable.

00:01:23.770 --> 00:01:24.770
Why?

00:01:24.770 --> 00:01:29.580
Well, eventually we're going to be able to
build personalities and behavior sets that

00:01:29.580 --> 00:01:33.610
are really compelling, and once we do, it's
just a matter of time before someone falls

00:01:33.610 --> 00:01:34.890
in love with it.

00:01:34.890 --> 00:01:39.970
Now for some people this could mean a chance
at a really emotionally supportive relationship

00:01:39.970 --> 00:01:46.140
with a robot who seems at least on the surface
to be as real and genuine as a human being.

00:01:46.140 --> 00:01:48.170
But it does raise some pretty tricky questions.

00:01:48.170 --> 00:01:53.409
For example, if I'm having relationship issues
with my robot, does that mean I could reprogram

00:01:53.409 --> 00:01:54.409
it?

00:01:54.409 --> 00:01:55.409
Change out the personality?

00:01:55.409 --> 00:01:56.409
Give it a new set of behaviors?

00:01:56.409 --> 00:01:58.610
I mean I wouldn't do that to another person.

00:01:58.610 --> 00:02:02.320
Or maybe I go even more extreme and just throw
the robot away.

00:02:02.320 --> 00:02:07.250
And if robots are just simulating thoughts
and emotions, can we really have a meaningful

00:02:07.250 --> 00:02:09.030
relationship with one?

00:02:09.030 --> 00:02:13.880
Or if that simulation is truly compelling,
can we really be sure the robot isn't having

00:02:13.880 --> 00:02:16.230
some sort of individual experience?

00:02:16.230 --> 00:02:20.670
Because if it is, maybe we should grant it
the same sort of rights that human beings

00:02:20.670 --> 00:02:21.670
have.

00:02:21.670 --> 00:02:23.590
I'm not the only person asking these questions.

00:02:23.590 --> 00:02:29.620
Back in 2007, the government of South Korea
formed a Robotics Ethics Charter.

00:02:29.620 --> 00:02:34.349
It's a guideline to create responsible development
in robotic intelligence.

00:02:34.349 --> 00:02:39.370
But machine ethics is a can of worms all on
itself, because no one really agrees on what

00:02:39.370 --> 00:02:40.370
it is yet.

00:02:40.370 --> 00:02:44.520
In fact, we still have trouble agreeing what's
ethical for a human to do to another human,

00:02:44.520 --> 00:02:46.540
much less a human to a robot.

00:02:46.540 --> 00:02:51.020
Ultimately, I think the rise of social robots
is going to teach us a lot about ourselves,

00:02:51.020 --> 00:02:52.990
which is pretty cool.

00:02:52.990 --> 00:02:58.569
And along the way, maybe we find out that
programming a robot just right, doesn't necessarily

00:02:58.569 --> 00:03:00.700
mean he's Mr. Right.

00:03:00.700 --> 00:03:03.420
That leads me to a question for all of you
out there.

00:03:03.420 --> 00:03:08.140
What is the most romantic, geeky gesture you've
ever made to someone to let them know how

00:03:08.140 --> 00:03:09.270
you feel.

00:03:09.270 --> 00:03:11.710
Maybe you wrote a love song in chiptunes.

00:03:11.710 --> 00:03:13.500
Let us know in the comments below.

00:03:13.500 --> 00:03:16.860
And if you enjoyed this video, make sure you
'like' it and subscribe to our channel and

00:03:16.860 --> 00:03:19.310
then check these other videos out over here.

00:03:19.310 --> 00:03:26.730
I think you're gonna love them.

