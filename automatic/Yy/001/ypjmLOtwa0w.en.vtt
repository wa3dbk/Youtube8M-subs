WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.060 

team<00:00:00.810> picnic<00:00:01.260> from<00:00:01.469> the<00:00:01.530> University<00:00:01.979> of

00:00:02.060 --> 00:00:02.070 
team picnic from the University of

00:00:02.070 --> 00:00:03.830 
team picnic from the University of
Colorado<00:00:02.429> Boulder<00:00:02.460> represents<00:00:03.210> an<00:00:03.300> update<00:00:03.810> on

00:00:03.830 --> 00:00:03.840 
Colorado Boulder represents an update on

00:00:03.840 --> 00:00:05.480 
Colorado Boulder represents an update on
progress<00:00:04.200> for<00:00:04.680> the<00:00:04.710> Amazon<00:00:05.130> picking

00:00:05.480 --> 00:00:05.490 
progress for the Amazon picking

00:00:05.490 --> 00:00:07.639 
progress for the Amazon picking
challenge<00:00:05.879> here<00:00:06.660> we<00:00:06.779> picked<00:00:07.020> four<00:00:07.259> products

00:00:07.639 --> 00:00:07.649 
challenge here we picked four products

00:00:07.649 --> 00:00:09.379 
challenge here we picked four products
and<00:00:07.799> rail<00:00:07.980> using<00:00:08.280> Baxter<00:00:08.730> and<00:00:08.880> hard<00:00:09.059> coded

00:00:09.379 --> 00:00:09.389 
and rail using Baxter and hard coded

00:00:09.389 --> 00:00:11.330 
and rail using Baxter and hard coded
product<00:00:09.719> locations<00:00:10.200> we<00:00:10.469> demonstrate<00:00:10.950> motion

00:00:11.330 --> 00:00:11.340 
product locations we demonstrate motion

00:00:11.340 --> 00:00:13.129 
product locations we demonstrate motion
planning<00:00:11.490> calibration<00:00:12.420> grasping<00:00:13.049> and

00:00:13.129 --> 00:00:13.139 
planning calibration grasping and

00:00:13.139 --> 00:00:17.490 
planning calibration grasping and
control<00:00:13.469> and<00:00:13.740> our<00:00:13.830> robot<00:00:14.099> agnostic<00:00:14.549> framework

00:00:17.490 --> 00:00:17.500 

00:00:17.500 --> 00:00:20.400 

our<00:00:18.270> overall<00:00:19.270> approach<00:00:19.480> is<00:00:19.810> based<00:00:20.020> on<00:00:20.320> the

00:00:20.400 --> 00:00:20.410 
our overall approach is based on the

00:00:20.410 --> 00:00:21.780 
our overall approach is based on the
movie<00:00:20.650> paradigm<00:00:21.040> using<00:00:21.490> probabilistic

00:00:21.780 --> 00:00:21.790 
movie paradigm using probabilistic

00:00:21.790 --> 00:00:23.640 
movie paradigm using probabilistic
motion<00:00:22.510> planning<00:00:22.690> combined<00:00:23.350> with<00:00:23.560> an

00:00:23.640 --> 00:00:23.650 
motion planning combined with an

00:00:23.650 --> 00:00:25.380 
motion planning combined with an
internally<00:00:24.070> developed<00:00:24.340> experience<00:00:25.119> based

00:00:25.380 --> 00:00:25.390 
internally developed experience based

00:00:25.390 --> 00:00:27.929 
internally developed experience based
planning<00:00:25.810> approach<00:00:26.050> Thunder<00:00:26.980> that<00:00:27.490> increases

00:00:27.929 --> 00:00:27.939 
planning approach Thunder that increases

00:00:27.939 --> 00:00:29.490 
planning approach Thunder that increases
the<00:00:28.060> speed<00:00:28.150> and<00:00:28.510> reliability<00:00:28.570> of<00:00:29.170> motion

00:00:29.490 --> 00:00:29.500 
the speed and reliability of motion

00:00:29.500 --> 00:00:32.249 
the speed and reliability of motion
plans<00:00:29.880> Thunder<00:00:30.880> allows<00:00:31.090> for<00:00:31.390> rapid<00:00:31.420> recall<00:00:32.080> of

00:00:32.249 --> 00:00:32.259 
plans Thunder allows for rapid recall of

00:00:32.259 --> 00:00:34.020 
plans Thunder allows for rapid recall of
low-cost<00:00:32.619> solutions<00:00:33.250> suited<00:00:33.610> for<00:00:33.760> cluttered

00:00:34.020 --> 00:00:34.030 
low-cost solutions suited for cluttered

00:00:34.030 --> 00:00:44.050 
low-cost solutions suited for cluttered
spaces<00:00:34.449> that<00:00:34.570> contain<00:00:34.870> narrow<00:00:35.080> passages

00:00:44.050 --> 00:00:44.060 

00:00:44.060 --> 00:00:50.060 

you

00:00:50.060 --> 00:00:50.070 

00:00:50.070 --> 00:00:52.470 

we<00:00:51.070> have<00:00:51.190> been<00:00:51.340> developing<00:00:51.550> and<00:00:52.030> exploring

00:00:52.470 --> 00:00:52.480 
we have been developing and exploring

00:00:52.480 --> 00:00:54.150 
we have been developing and exploring
several<00:00:52.719> different<00:00:53.199> in<00:00:53.320> defector<00:00:53.680> options

00:00:54.150 --> 00:00:54.160 
several different in defector options

00:00:54.160 --> 00:00:55.410 
several different in defector options
for<00:00:54.370> the<00:00:54.430> competition<00:00:54.969> using<00:00:55.300> embedded

00:00:55.410 --> 00:00:55.420 
for the competition using embedded

00:00:55.420 --> 00:00:57.270 
for the competition using embedded
sensors<00:00:55.930> that<00:00:56.260> provide<00:00:56.559> feedback<00:00:56.860> for

00:00:57.270 --> 00:00:57.280 
sensors that provide feedback for

00:00:57.280 --> 00:00:59.759 
sensors that provide feedback for
typical<00:00:57.640> manipulation<00:00:58.269> tasks<00:00:58.750> shown<00:00:59.410> here<00:00:59.649> is

00:00:59.759 --> 00:00:59.769 
typical manipulation tasks shown here is

00:00:59.769 --> 00:01:01.740 
typical manipulation tasks shown here is
a<00:00:59.800> Yale<00:01:00.070> open<00:01:00.399> hand<00:01:00.610> model<00:01:00.940> o<00:01:01.059> demonstrating

00:01:01.740 --> 00:01:01.750 
a Yale open hand model o demonstrating

00:01:01.750 --> 00:01:03.660 
a Yale open hand model o demonstrating
both<00:01:01.899> pinched<00:01:02.230> and<00:01:02.469> developing<00:01:03.039> graphs<00:01:03.339> using

00:01:03.660 --> 00:01:03.670 
both pinched and developing graphs using

00:01:03.670 --> 00:01:05.609 
both pinched and developing graphs using
the<00:01:03.879> articulated<00:01:04.390> finger<00:01:04.629> joints<00:01:04.960> our

00:01:05.609 --> 00:01:05.619 
the articulated finger joints our

00:01:05.619 --> 00:01:07.410 
the articulated finger joints our
sensors<00:01:05.890> can<00:01:06.280> tell<00:01:06.490> how<00:01:06.610> far<00:01:06.820> away<00:01:06.940> the<00:01:07.180> finger

00:01:07.410 --> 00:01:07.420 
sensors can tell how far away the finger

00:01:07.420 --> 00:01:09.330 
sensors can tell how far away the finger
pad<00:01:07.630> is<00:01:07.810> from<00:01:07.990> an<00:01:08.080> object<00:01:08.229> and<00:01:08.649> how<00:01:08.740> much<00:01:08.979> force

00:01:09.330 --> 00:01:09.340 
pad is from an object and how much force

00:01:09.340 --> 00:01:12.020 
pad is from an object and how much force
exerted<00:01:09.820> audit<00:01:10.509> we<00:01:11.170> are<00:01:11.320> using<00:01:11.530> sophisticated

00:01:12.020 --> 00:01:12.030 
exerted audit we are using sophisticated

00:01:12.030 --> 00:01:14.340 
exerted audit we are using sophisticated
manipulation<00:01:13.030> visualizations<00:01:13.600> to<00:01:13.990> develop

00:01:14.340 --> 00:01:14.350 
manipulation visualizations to develop

00:01:14.350 --> 00:01:15.840 
manipulation visualizations to develop
reliable<00:01:14.799> collision<00:01:15.159> worlds<00:01:15.490> that<00:01:15.610> allow

00:01:15.840 --> 00:01:15.850 
reliable collision worlds that allow

00:01:15.850 --> 00:01:17.460 
reliable collision worlds that allow
motion<00:01:16.360> planning<00:01:16.509> and<00:01:16.780> cluttered<00:01:17.049> spaces

00:01:17.460 --> 00:01:17.470 
motion planning and cluttered spaces

00:01:17.470 --> 00:01:19.920 
motion planning and cluttered spaces
these<00:01:18.369> visualizations<00:01:18.759> allow<00:01:19.360> us<00:01:19.540> to<00:01:19.690> overlay

00:01:19.920 --> 00:01:19.930 
these visualizations allow us to overlay

00:01:19.930 --> 00:01:21.779 
these visualizations allow us to overlay
data<00:01:20.229> coming<00:01:20.560> from<00:01:20.649> our<00:01:20.890> perception<00:01:21.369> pipeline

00:01:21.779 --> 00:01:21.789 
data coming from our perception pipeline

00:01:21.789 --> 00:01:23.669 
data coming from our perception pipeline
with<00:01:21.940> kinematic<00:01:22.450> grasping<00:01:23.050> and<00:01:23.229> planning

00:01:23.669 --> 00:01:23.679 
with kinematic grasping and planning

00:01:23.679 --> 00:01:26.340 
with kinematic grasping and planning
information<00:01:24.450> also<00:01:25.450> shown<00:01:25.660> here<00:01:25.929> is<00:01:26.080> our<00:01:26.200> new

00:01:26.340 --> 00:01:26.350 
information also shown here is our new

00:01:26.350 --> 00:01:28.410 
information also shown here is our new
gantry<00:01:26.770> based<00:01:26.979> robot<00:01:27.310> Jacob<00:01:27.729> that<00:01:28.300> is

00:01:28.410 --> 00:01:28.420 
gantry based robot Jacob that is

00:01:28.420 --> 00:01:30.210 
gantry based robot Jacob that is
currently<00:01:28.780> in<00:01:28.869> development<00:01:29.020> it<00:01:29.710> is<00:01:29.920> based<00:01:30.130> on

00:01:30.210 --> 00:01:30.220 
currently in development it is based on

00:01:30.220 --> 00:01:32.040 
currently in development it is based on
the<00:01:30.340> canova<00:01:30.670> jayco<00:01:31.000> to<00:01:31.210> robot<00:01:31.539> a<00:01:31.690> six<00:01:31.929> degree

00:01:32.040 --> 00:01:32.050 
the canova jayco to robot a six degree

00:01:32.050 --> 00:01:33.690 
the canova jayco to robot a six degree
of<00:01:32.289> freedom<00:01:32.440> arm<00:01:32.770> that<00:01:32.950> is<00:01:33.009> compact<00:01:33.490> yet

00:01:33.690 --> 00:01:33.700 
of freedom arm that is compact yet

00:01:33.700 --> 00:01:35.940 
of freedom arm that is compact yet
powerful<00:01:34.240> for<00:01:34.450> a<00:01:34.479> slight<00:01:34.750> weight<00:01:34.960> augmenting

00:01:35.940 --> 00:01:35.950 
powerful for a slight weight augmenting

00:01:35.950 --> 00:01:37.859 
powerful for a slight weight augmenting
the<00:01:36.069> arm<00:01:36.250> is<00:01:36.399> a<00:01:36.429> z-axis<00:01:36.640> gantry<00:01:37.450> to<00:01:37.630> increase

00:01:37.859 --> 00:01:37.869 
the arm is a z-axis gantry to increase

00:01:37.869 --> 00:01:39.540 
the arm is a z-axis gantry to increase
the<00:01:38.020> work<00:01:38.140> space<00:01:38.409> along<00:01:38.830> the<00:01:39.009> tall<00:01:39.220> shelf

00:01:39.540 --> 00:01:39.550 
the work space along the tall shelf

00:01:39.550 --> 00:01:41.309 
the work space along the tall shelf
though<00:01:39.940> it<00:01:40.000> is<00:01:40.090> not<00:01:40.119> arrived<00:01:40.539> to<00:01:40.810> our<00:01:40.840> lab<00:01:41.080> yet

00:01:41.309 --> 00:01:41.319 
though it is not arrived to our lab yet

00:01:41.319 --> 00:01:45.480 
though it is not arrived to our lab yet
our<00:01:43.259> perception<00:01:44.259> approach<00:01:44.560> utilizes<00:01:44.860> color

00:01:45.480 --> 00:01:45.490 
our perception approach utilizes color

00:01:45.490 --> 00:01:47.570 
our perception approach utilizes color
based<00:01:45.789> unsupervised<00:01:46.659> spatiotemporal

00:01:47.570 --> 00:01:47.580 
based unsupervised spatiotemporal

00:01:47.580 --> 00:01:50.249 
based unsupervised spatiotemporal
segmentation<00:01:48.580> and<00:01:48.789> motion<00:01:49.509> based<00:01:49.690> object

00:01:50.249 --> 00:01:50.259 
segmentation and motion based object

00:01:50.259 --> 00:01:51.660 
segmentation and motion based object
replication<00:01:50.800> to<00:01:50.920> accurately<00:01:51.340> build<00:01:51.610> a

00:01:51.660 --> 00:01:51.670 
replication to accurately build a

00:01:51.670 --> 00:01:54.240 
replication to accurately build a
representation<00:01:52.060> of<00:01:52.450> the<00:01:52.539> bins<00:01:52.890> here<00:01:53.890> we<00:01:54.009> track

00:01:54.240 --> 00:01:54.250 
representation of the bins here we track

00:01:54.250 --> 00:01:56.310 
representation of the bins here we track
and<00:01:54.430> reconstruct<00:01:54.880> a<00:01:54.969> dense<00:01:55.210> 3d<00:01:55.630> model<00:01:56.140> of<00:01:56.200> the

00:01:56.310 --> 00:01:56.320 
and reconstruct a dense 3d model of the

00:01:56.320 --> 00:01:58.709 
and reconstruct a dense 3d model of the
shelf<00:01:56.530> using<00:01:56.829> SDF<00:01:57.250> fusion<00:01:57.640> this<00:01:58.270> is<00:01:58.390> achieved

00:01:58.709 --> 00:01:58.719 
shelf using SDF fusion this is achieved

00:01:58.719 --> 00:02:00.389 
shelf using SDF fusion this is achieved
by<00:01:58.810> moving<00:01:59.079> the<00:01:59.259> robot<00:01:59.530> arm<00:01:59.680> across<00:01:59.890> the<00:02:00.159> shelf

00:02:00.389 --> 00:02:00.399 
by moving the robot arm across the shelf

00:02:00.399 --> 00:02:02.429 
by moving the robot arm across the shelf
we<00:02:01.149> detect<00:02:01.479> and<00:02:01.630> calibrate<00:02:02.020> the<00:02:02.200> six

00:02:02.429 --> 00:02:02.439 
we detect and calibrate the six

00:02:02.439 --> 00:02:03.870 
we detect and calibrate the six
degree-of-freedom<00:02:02.710> pose<00:02:03.250> of<00:02:03.460> the<00:02:03.490> shelf<00:02:03.759> and

00:02:03.870 --> 00:02:03.880 
degree-of-freedom pose of the shelf and

00:02:03.880 --> 00:02:05.700 
degree-of-freedom pose of the shelf and
an<00:02:04.000> arbitrary<00:02:04.420> seen<00:02:04.719> by<00:02:05.289> subtracting

00:02:05.700 --> 00:02:05.710 
an arbitrary seen by subtracting

00:02:05.710 --> 00:02:07.980 
an arbitrary seen by subtracting
possible<00:02:06.310> poses<00:02:06.670> of<00:02:06.759> the<00:02:06.820> shelf<00:02:07.060> and<00:02:07.240> finding

00:02:07.980 --> 00:02:07.990 
possible poses of the shelf and finding

00:02:07.990 --> 00:02:10.560 
possible poses of the shelf and finding
the<00:02:08.079> best<00:02:08.289> mass<00:02:08.619> with<00:02:08.950> the<00:02:09.039> ICP<00:02:09.519> algorithm<00:02:10.000> the

00:02:10.560 --> 00:02:10.570 
the best mass with the ICP algorithm the

00:02:10.570 --> 00:02:12.479 
the best mass with the ICP algorithm the
shelf<00:02:10.810> is<00:02:10.929> in<00:02:11.049> tract<00:02:11.320> with<00:02:11.620> the<00:02:11.709> ICP<00:02:12.100> algorithm

00:02:12.479 --> 00:02:12.489 
shelf is in tract with the ICP algorithm

00:02:12.489 --> 00:02:13.890 
shelf is in tract with the ICP algorithm
while<00:02:12.880> detecting<00:02:13.239> the<00:02:13.329> shape<00:02:13.510> and<00:02:13.720> color

00:02:13.890 --> 00:02:13.900 
while detecting the shape and color

00:02:13.900 --> 00:02:16.800 
while detecting the shape and color
outline<00:02:14.170> of<00:02:14.620> the<00:02:14.799> 3d<00:02:15.100> model<00:02:15.400> we<00:02:16.329> initialize

00:02:16.800 --> 00:02:16.810 
outline of the 3d model we initialize

00:02:16.810 --> 00:02:19.229 
outline of the 3d model we initialize
tracked<00:02:17.530> and<00:02:17.799> reconstruct<00:02:18.340> both<00:02:18.609> 2d<00:02:18.910> and<00:02:19.060> 3d

00:02:19.229 --> 00:02:19.239 
tracked and reconstruct both 2d and 3d

00:02:19.239 --> 00:02:22.020 
tracked and reconstruct both 2d and 3d
models<00:02:19.870> of<00:02:19.959> objects<00:02:20.410> online<00:02:20.769> the<00:02:21.459> 3d<00:02:21.760> models

00:02:22.020 --> 00:02:22.030 
models of objects online the 3d models

00:02:22.030 --> 00:02:23.819 
models of objects online the 3d models
of<00:02:22.090> the<00:02:22.120> final<00:02:22.480> online<00:02:22.660> reconstructed<00:02:23.470> object

00:02:23.819 --> 00:02:23.829 
of the final online reconstructed object

00:02:23.829 --> 00:02:25.500 
of the final online reconstructed object
are<00:02:23.950> matched<00:02:24.190> with<00:02:24.519> a<00:02:24.549> known<00:02:24.790> objects<00:02:25.299> in<00:02:25.450> the

00:02:25.500 --> 00:02:25.510 
are matched with a known objects in the

00:02:25.510 --> 00:02:27.539 
are matched with a known objects in the
database<00:02:25.650> based<00:02:26.650> on<00:02:26.920> the<00:02:27.010> appearance<00:02:27.100> and

00:02:27.539 --> 00:02:27.549 
database based on the appearance and

00:02:27.549 --> 00:02:29.910 
database based on the appearance and
shape<00:02:27.850> using<00:02:28.090> RGB<00:02:28.540> histograms<00:02:29.109> and<00:02:29.320> point

00:02:29.910 --> 00:02:29.920 
shape using RGB histograms and point

00:02:29.920 --> 00:02:32.340 
shape using RGB histograms and point
paired<00:02:30.100> features<00:02:30.570> we<00:02:31.570> send<00:02:31.810> the<00:02:31.930> estimated

00:02:32.340 --> 00:02:32.350 
paired features we send the estimated

00:02:32.350 --> 00:02:34.319 
paired features we send the estimated
poses<00:02:32.769> and<00:02:32.950> generated<00:02:33.400> messages<00:02:33.700> of<00:02:33.850> objects

00:02:34.319 --> 00:02:34.329 
poses and generated messages of objects

00:02:34.329 --> 00:02:37.590 
poses and generated messages of objects
to<00:02:34.420> the<00:02:34.510> robot<00:02:34.630> for<00:02:34.930> grasping<00:02:36.600> finally

00:02:37.590 --> 00:02:37.600 
to the robot for grasping finally

00:02:37.600 --> 00:02:39.390 
to the robot for grasping finally
hundreds<00:02:38.049> of<00:02:38.109> potential<00:02:38.410> graphs<00:02:38.739> are<00:02:39.040> created

00:02:39.390 --> 00:02:39.400 
hundreds of potential graphs are created

00:02:39.400 --> 00:02:40.920 
hundreds of potential graphs are created
using<00:02:39.430> a<00:02:39.700> cuboid<00:02:40.000> based<00:02:40.329> graph<00:02:40.690> generator

00:02:40.920 --> 00:02:40.930 
using a cuboid based graph generator

00:02:40.930 --> 00:02:42.930 
using a cuboid based graph generator
then<00:02:41.530> filtered<00:02:41.950> using<00:02:42.010> inverse<00:02:42.400> kinematics

00:02:42.930 --> 00:02:42.940 
then filtered using inverse kinematics

00:02:42.940 --> 00:02:44.849 
then filtered using inverse kinematics
in<00:02:43.090> collision<00:02:43.420> section<00:02:43.810> the<00:02:44.260> best<00:02:44.440> candidate

00:02:44.849 --> 00:02:44.859 
in collision section the best candidate

00:02:44.859 --> 00:02:46.680 
in collision section the best candidate
grass<00:02:45.010> was<00:02:45.190> chosen<00:02:45.430> based<00:02:45.760> on<00:02:45.970> approach<00:02:46.329> angle

00:02:46.680 --> 00:02:46.690 
grass was chosen based on approach angle

00:02:46.690 --> 00:02:47.910 
grass was chosen based on approach angle
and<00:02:46.780> straight-line<00:02:47.079> Cartesian<00:02:47.530> path<00:02:47.709> is

00:02:47.910 --> 00:02:47.920 
and straight-line Cartesian path is

00:02:47.920 --> 00:02:49.229 
and straight-line Cartesian path is
calculating<00:02:48.459> the<00:02:48.579> iterative<00:02:48.880> inverse

00:02:49.229 --> 00:02:49.239 
calculating the iterative inverse

00:02:49.239 --> 00:02:51.270 
calculating the iterative inverse
kinematic<00:02:49.690> calculations<00:02:50.230> the<00:02:50.859> object<00:02:51.190> is

00:02:51.270 --> 00:02:51.280 
kinematic calculations the object is

00:02:51.280 --> 00:02:52.830 
kinematic calculations the object is
grasped<00:02:51.579> and<00:02:51.880> raised<00:02:52.120> slightly<00:02:52.299> then<00:02:52.600> pulled

00:02:52.830 --> 00:02:52.840 
grasped and raised slightly then pulled

00:02:52.840 --> 00:02:56.280 
grasped and raised slightly then pulled
back<00:02:53.019> in<00:02:53.200> the<00:02:53.290> inverse<00:02:53.560> Cartesian<00:02:54.100> path

