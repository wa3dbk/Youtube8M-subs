WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.389 

the<00:00:00.810> next<00:00:01.079> generation<00:00:01.140> CUDA<00:00:01.949> computing

00:00:02.389 --> 00:00:02.399 
the next generation CUDA computing

00:00:02.399 --> 00:00:04.579 
the next generation CUDA computing
graphics<00:00:02.790> architecture<00:00:03.449> codenamed<00:00:03.929> Fermi<00:00:04.410> is

00:00:04.579 --> 00:00:04.589 
graphics architecture codenamed Fermi is

00:00:04.589 --> 00:00:06.590 
graphics architecture codenamed Fermi is
the<00:00:05.040> most<00:00:05.220> advanced<00:00:05.700> GPU<00:00:06.120> computing

00:00:06.590 --> 00:00:06.600 
the most advanced GPU computing

00:00:06.600 --> 00:00:08.720 
the most advanced GPU computing
architecture<00:00:07.259> ever<00:00:07.470> built<00:00:07.770> it<00:00:08.010> features

00:00:08.720 --> 00:00:08.730 
architecture ever built it features

00:00:08.730 --> 00:00:11.870 
architecture ever built it features
several<00:00:09.000> major<00:00:09.360> innovations<00:00:10.309> 512<00:00:11.309> CUDA<00:00:11.670> cores

00:00:11.870 --> 00:00:11.880 
several major innovations 512 CUDA cores

00:00:11.880 --> 00:00:14.480 
several major innovations 512 CUDA cores
and<00:00:12.179> video<00:00:12.690> parallel<00:00:13.230> data<00:00:13.440> cache<00:00:13.769> technology

00:00:14.480 --> 00:00:14.490 
and video parallel data cache technology

00:00:14.490 --> 00:00:17.750 
and video parallel data cache technology
Nvidia<00:00:15.299> Giga<00:00:15.570> thread<00:00:15.870> engine<00:00:16.320> and<00:00:16.529> full<00:00:17.070> ECC

00:00:17.750 --> 00:00:17.760 
Nvidia Giga thread engine and full ECC

00:00:17.760 --> 00:00:21.349 
Nvidia Giga thread engine and full ECC
support<00:00:19.100> Fermi's<00:00:20.100> cuda<00:00:20.400> cores<00:00:20.609> are<00:00:20.850> optimized

00:00:21.349 --> 00:00:21.359 
support Fermi's cuda cores are optimized

00:00:21.359 --> 00:00:23.090 
support Fermi's cuda cores are optimized
for<00:00:21.510> both<00:00:21.779> exceptional<00:00:22.170> performance<00:00:22.949> and

00:00:23.090 --> 00:00:23.100 
for both exceptional performance and

00:00:23.100 --> 00:00:25.370 
for both exceptional performance and
accuracy<00:00:23.670> double<00:00:24.210> precision<00:00:24.510> performance<00:00:25.260> is

00:00:25.370 --> 00:00:25.380 
accuracy double precision performance is

00:00:25.380 --> 00:00:27.560 
accuracy double precision performance is
up<00:00:25.529> to<00:00:25.619> 8<00:00:25.740> times<00:00:25.980> faster<00:00:26.580> than<00:00:26.609> prior<00:00:26.970> GPU

00:00:27.560 --> 00:00:27.570 
up to 8 times faster than prior GPU

00:00:27.570 --> 00:00:29.960 
up to 8 times faster than prior GPU
architectures<00:00:28.349> and<00:00:28.500> all<00:00:29.070> calculations<00:00:29.310> are

00:00:29.960 --> 00:00:29.970 
architectures and all calculations are

00:00:29.970 --> 00:00:31.759 
architectures and all calculations are
performed<00:00:30.000> in<00:00:30.480> compliance<00:00:30.960> with<00:00:31.140> industry

00:00:31.759 --> 00:00:31.769 
performed in compliance with industry

00:00:31.769 --> 00:00:33.970 
performed in compliance with industry
standards<00:00:32.309> for<00:00:32.489> floating-point<00:00:32.910> arithmetic

00:00:33.970 --> 00:00:33.980 
standards for floating-point arithmetic

00:00:33.980 --> 00:00:35.600 
standards for floating-point arithmetic
high-performance<00:00:34.980> computing<00:00:35.460> applications

00:00:35.600 --> 00:00:35.610 
high-performance computing applications

00:00:35.610 --> 00:00:38.060 
high-performance computing applications
relying<00:00:36.480> on<00:00:36.690> numerical<00:00:37.170> simulation<00:00:37.860> or

00:00:38.060 --> 00:00:38.070 
relying on numerical simulation or

00:00:38.070 --> 00:00:40.880 
relying on numerical simulation or
linear<00:00:38.520> algebra<00:00:39.180> will<00:00:39.719> particularly<00:00:40.230> benefit

00:00:40.880 --> 00:00:40.890 
linear algebra will particularly benefit

00:00:40.890 --> 00:00:43.010 
linear algebra will particularly benefit
from<00:00:41.070> Fermi's<00:00:41.489> revolutionary<00:00:42.270> increase<00:00:42.809> in

00:00:43.010 --> 00:00:43.020 
from Fermi's revolutionary increase in

00:00:43.020 --> 00:00:46.430 
from Fermi's revolutionary increase in
double<00:00:43.710> precision<00:00:44.040> performance<00:00:45.320> Fermi<00:00:46.320> is

00:00:46.430 --> 00:00:46.440 
double precision performance Fermi is

00:00:46.440 --> 00:00:48.650 
double precision performance Fermi is
the<00:00:46.590> first<00:00:46.800> GPU<00:00:47.370> architecture<00:00:47.399> to<00:00:48.210> support<00:00:48.600> a

00:00:48.650 --> 00:00:48.660 
the first GPU architecture to support a

00:00:48.660 --> 00:00:50.750 
the first GPU architecture to support a
full<00:00:48.899> cache<00:00:49.170> hierarchy<00:00:49.469> in<00:00:50.039> combination<00:00:50.730> with

00:00:50.750 --> 00:00:50.760 
full cache hierarchy in combination with

00:00:50.760 --> 00:00:53.990 
full cache hierarchy in combination with
onship<00:00:51.390> shared<00:00:51.750> memory<00:00:52.140> the<00:00:52.739> l1<00:00:53.190> cache<00:00:53.520> which

00:00:53.990 --> 00:00:54.000 
onship shared memory the l1 cache which

00:00:54.000 --> 00:00:56.869 
onship shared memory the l1 cache which
is<00:00:54.180> a<00:00:54.329> configurable<00:00:54.960> 64<00:00:55.829> kilobytes<00:00:56.399> combined

00:00:56.869 --> 00:00:56.879 
is a configurable 64 kilobytes combined

00:00:56.879 --> 00:00:59.630 
is a configurable 64 kilobytes combined
shared<00:00:57.239> memory<00:00:57.690> in<00:00:57.870> cash<00:00:58.140> improves<00:00:59.039> bandwidth

00:00:59.630 --> 00:00:59.640 
shared memory in cash improves bandwidth

00:00:59.640 --> 00:01:02.150 
shared memory in cash improves bandwidth
and<00:00:59.850> reduces<00:01:00.390> latency<00:01:00.989> for<00:01:01.199> a<00:01:01.260> wide<00:01:01.559> range<00:01:01.890> of

00:01:02.150 --> 00:01:02.160 
and reduces latency for a wide range of

00:01:02.160 --> 00:01:04.850 
and reduces latency for a wide range of
GPU<00:01:02.699> computing<00:01:03.180> applications<00:01:03.500> such<00:01:04.500> as<00:01:04.530> ray

00:01:04.850 --> 00:01:04.860 
GPU computing applications such as ray

00:01:04.860 --> 00:01:07.250 
GPU computing applications such as ray
tracing<00:01:05.189> fluid<00:01:06.000> physics<00:01:06.240> simulations<00:01:07.140> and

00:01:07.250 --> 00:01:07.260 
tracing fluid physics simulations and

00:01:07.260 --> 00:01:10.910 
tracing fluid physics simulations and
sorting<00:01:07.979> operations<00:01:08.610> the<00:01:09.290> 768<00:01:10.290> kilobytes

00:01:10.910 --> 00:01:10.920 
sorting operations the 768 kilobytes

00:01:10.920 --> 00:01:14.210 
sorting operations the 768 kilobytes
unified<00:01:11.549> l2<00:01:12.060> cache<00:01:12.390> enables<00:01:12.960> fast<00:01:13.320> coherent

00:01:14.210 --> 00:01:14.220 
unified l2 cache enables fast coherent

00:01:14.220 --> 00:01:16.969 
unified l2 cache enables fast coherent
data<00:01:14.490> sharing<00:01:14.880> across<00:01:15.210> the<00:01:15.509> GPU<00:01:16.020> benefiting

00:01:16.969 --> 00:01:16.979 
data sharing across the GPU benefiting

00:01:16.979 --> 00:01:19.399 
data sharing across the GPU benefiting
most<00:01:17.220> computing<00:01:17.790> applications<00:01:18.409> physics

00:01:19.399 --> 00:01:19.409 
most computing applications physics

00:01:19.409 --> 00:01:21.770 
most computing applications physics
solvers<00:01:19.860> ray<00:01:20.220> tracing<00:01:20.610> and<00:01:20.939> sparse<00:01:21.240> matrix

00:01:21.770 --> 00:01:21.780 
solvers ray tracing and sparse matrix

00:01:21.780 --> 00:01:24.020 
solvers ray tracing and sparse matrix
multiplication<00:01:22.619> algorithms<00:01:23.369> especially

00:01:24.020 --> 00:01:24.030 
multiplication algorithms especially

00:01:24.030 --> 00:01:28.130 
multiplication algorithms especially
benefit<00:01:24.479> from<00:01:24.659> this<00:01:24.810> cache<00:01:25.110> hierarchy<00:01:27.140> firmly

00:01:28.130 --> 00:01:28.140 
benefit from this cache hierarchy firmly

00:01:28.140 --> 00:01:30.170 
benefit from this cache hierarchy firmly
features<00:01:28.740> the<00:01:28.950> Giga<00:01:29.159> thread<00:01:29.520> engine<00:01:29.939> that

00:01:30.170 --> 00:01:30.180 
features the Giga thread engine that

00:01:30.180 --> 00:01:32.120 
features the Giga thread engine that
supports<00:01:30.600> concurrent<00:01:31.079> kernel<00:01:31.470> execution

00:01:32.120 --> 00:01:32.130 
supports concurrent kernel execution

00:01:32.130 --> 00:01:34.069 
supports concurrent kernel execution
which<00:01:32.549> allows<00:01:32.850> different<00:01:33.360> program<00:01:33.780> functions

00:01:34.069 --> 00:01:34.079 
which allows different program functions

00:01:34.079 --> 00:01:36.590 
which allows different program functions
to<00:01:34.500> execute<00:01:35.040> in<00:01:35.189> parallel<00:01:35.600> improving

00:01:36.590 --> 00:01:36.600 
to execute in parallel improving

00:01:36.600 --> 00:01:38.359 
to execute in parallel improving
efficiency<00:01:37.200> in<00:01:37.350> speed<00:01:37.680> of<00:01:37.950> the<00:01:38.070> overall

00:01:38.359 --> 00:01:38.369 
efficiency in speed of the overall

00:01:38.369 --> 00:01:40.730 
efficiency in speed of the overall
application<00:01:38.869> Fermi<00:01:39.869> also<00:01:40.320> includes

00:01:40.730 --> 00:01:40.740 
application Fermi also includes

00:01:40.740 --> 00:01:43.010 
application Fermi also includes
dedicated<00:01:41.400> bi-directional<00:01:42.270> data<00:01:42.299> transfer

00:01:43.010 --> 00:01:43.020 
dedicated bi-directional data transfer

00:01:43.020 --> 00:01:45.499 
dedicated bi-directional data transfer
engines<00:01:43.439> that<00:01:43.860> intelligently<00:01:44.549> manage<00:01:45.119> tens

00:01:45.499 --> 00:01:45.509 
engines that intelligently manage tens

00:01:45.509 --> 00:01:50.510 
engines that intelligently manage tens
of<00:01:45.750> thousands<00:01:46.290> of<00:01:46.409> threads

00:01:50.510 --> 00:01:50.520 

00:01:50.520 --> 00:01:53.190 

Fermi<00:01:51.520> is<00:01:51.670> the<00:01:51.850> world's<00:01:52.210> first<00:01:52.420> GPU

00:01:53.190 --> 00:01:53.200 
Fermi is the world's first GPU

00:01:53.200 --> 00:01:55.140 
Fermi is the world's first GPU
architecture<00:01:53.230> to<00:01:54.100> support<00:01:54.490> error<00:01:54.730> checking

00:01:55.140 --> 00:01:55.150 
architecture to support error checking

00:01:55.150 --> 00:01:58.130 
architecture to support error checking
and<00:01:55.570> correction<00:01:56.020> also<00:01:56.710> called<00:01:57.040> ECC

00:01:58.130 --> 00:01:58.140 
and correction also called ECC

00:01:58.140 --> 00:02:00.450 
and correction also called ECC
naturally-occurring<00:01:59.190> radiation<00:02:00.190> can<00:02:00.400> cause

00:02:00.450 --> 00:02:00.460 
naturally-occurring radiation can cause

00:02:00.460 --> 00:02:02.610 
naturally-occurring radiation can cause
data<00:02:00.940> steward<00:02:01.330> in<00:02:01.480> memory<00:02:01.840> to<00:02:02.080> be<00:02:02.200> altered

00:02:02.610 --> 00:02:02.620 
data steward in memory to be altered

00:02:02.620 --> 00:02:06.180 
data steward in memory to be altered
resulting<00:02:03.550> in<00:02:03.640> a<00:02:03.730> soft<00:02:04.060> error<00:02:04.560> ECC<00:02:05.560> technology

00:02:06.180 --> 00:02:06.190 
resulting in a soft error ECC technology

00:02:06.190 --> 00:02:08.340 
resulting in a soft error ECC technology
detects<00:02:06.610> incorrect<00:02:07.120> soft<00:02:07.450> errors<00:02:07.780> before

00:02:08.340 --> 00:02:08.350 
detects incorrect soft errors before

00:02:08.350 --> 00:02:10.919 
detects incorrect soft errors before
they<00:02:08.649> affect<00:02:08.979> the<00:02:09.100> system<00:02:09.460> Fermi's<00:02:10.390> register

00:02:10.919 --> 00:02:10.929 
they affect the system Fermi's register

00:02:10.929 --> 00:02:13.950 
they affect the system Fermi's register
files<00:02:11.260> shared<00:02:11.800> memories<00:02:12.430> caches<00:02:13.180> and<00:02:13.300> DRAM

00:02:13.950 --> 00:02:13.960 
files shared memories caches and DRAM

00:02:13.960 --> 00:02:16.920 
files shared memories caches and DRAM
are<00:02:14.200> all<00:02:14.500> fully<00:02:14.830> ECC<00:02:15.550> protected<00:02:16.120> making<00:02:16.810> it

00:02:16.920 --> 00:02:16.930 
are all fully ECC protected making it

00:02:16.930 --> 00:02:19.260 
are all fully ECC protected making it
not<00:02:17.110> only<00:02:17.260> the<00:02:17.530> most<00:02:17.740> powerful<00:02:18.250> GPU<00:02:18.670> for

00:02:19.260 --> 00:02:19.270 
not only the most powerful GPU for

00:02:19.270 --> 00:02:21.390 
not only the most powerful GPU for
professional<00:02:19.810> computing<00:02:20.260> applications<00:02:20.400> but

00:02:21.390 --> 00:02:21.400 
professional computing applications but

00:02:21.400 --> 00:02:24.600 
professional computing applications but
also<00:02:21.550> the<00:02:21.850> most<00:02:22.030> reliable

