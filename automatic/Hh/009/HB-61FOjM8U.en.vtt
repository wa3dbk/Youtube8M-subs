WEBVTT
Kind: captions
Language: en

00:00:05.040 --> 00:00:08.640 

robots<00:00:06.779> they're<00:00:07.779> becoming<00:00:08.109> part<00:00:08.440> of<00:00:08.500> our

00:00:08.640 --> 00:00:08.650 
robots they're becoming part of our

00:00:08.650 --> 00:00:12.620 
robots they're becoming part of our
lives<00:00:09.629> are<00:00:10.629> currently<00:00:11.020> experiencing<00:00:11.770> a<00:00:11.889> mass

00:00:12.620 --> 00:00:12.630 
lives are currently experiencing a mass

00:00:12.630 --> 00:00:14.930 
lives are currently experiencing a mass
robotic<00:00:13.200> technology<00:00:13.890> in<00:00:14.160> health<00:00:14.640> care

00:00:14.930 --> 00:00:14.940 
robotic technology in health care

00:00:14.940 --> 00:00:17.420 
robotic technology in health care
education<00:00:15.260> transportation<00:00:16.260> the<00:00:16.980> military

00:00:17.420 --> 00:00:17.430 
education transportation the military

00:00:17.430 --> 00:00:21.170 
education transportation the military
our<00:00:18.210> households<00:00:19.080> our<00:00:19.290> children's<00:00:19.740> toys<00:00:20.010> and<00:00:20.520> a

00:00:21.170 --> 00:00:21.180 
our households our children's toys and a

00:00:21.180 --> 00:00:23.089 
our households our children's toys and a
lot<00:00:21.300> of<00:00:21.390> these<00:00:21.480> areas<00:00:21.869> raise<00:00:22.080> ethical<00:00:22.320> issues

00:00:23.089 --> 00:00:23.099 
lot of these areas raise ethical issues

00:00:23.099 --> 00:00:25.040 
lot of these areas raise ethical issues
and<00:00:23.340> i'm<00:00:23.610> not<00:00:23.730> sure<00:00:23.790> that<00:00:24.480> our<00:00:24.630> current<00:00:24.779> laws

00:00:25.040 --> 00:00:25.050 
and i'm not sure that our current laws

00:00:25.050 --> 00:00:27.200 
and i'm not sure that our current laws
are<00:00:25.290> fully<00:00:25.470> equipped<00:00:25.650> to<00:00:25.860> deal<00:00:26.040> with<00:00:26.160> them<00:00:26.340> so

00:00:27.200 --> 00:00:27.210 
are fully equipped to deal with them so

00:00:27.210 --> 00:00:29.359 
are fully equipped to deal with them so
we<00:00:27.330> have<00:00:27.540> just<00:00:27.750> a<00:00:27.869> handful<00:00:28.500> of<00:00:28.560> scholars

00:00:29.359 --> 00:00:29.369 
we have just a handful of scholars

00:00:29.369 --> 00:00:31.280 
we have just a handful of scholars
working<00:00:29.669> on<00:00:29.970> legal<00:00:30.480> issues<00:00:30.779> around<00:00:30.960> robot

00:00:31.280 --> 00:00:31.290 
working on legal issues around robot

00:00:31.290 --> 00:00:33.110 
working on legal issues around robot
ethics<00:00:31.590> right<00:00:31.800> now<00:00:31.950> and<00:00:32.489> I<00:00:32.579> think<00:00:32.610> there<00:00:32.940> needs

00:00:33.110 --> 00:00:33.120 
ethics right now and I think there needs

00:00:33.120 --> 00:00:34.640 
ethics right now and I think there needs
to<00:00:33.239> be<00:00:33.270> more<00:00:33.420> and<00:00:33.930> we<00:00:34.260> need<00:00:34.380> to<00:00:34.410> be<00:00:34.530> working

00:00:34.640 --> 00:00:34.650 
to be more and we need to be working

00:00:34.650 --> 00:00:36.800 
to be more and we need to be working
together<00:00:34.950> with<00:00:35.370> roboticists<00:00:36.090> to<00:00:36.300> find<00:00:36.630> and

00:00:36.800 --> 00:00:36.810 
together with roboticists to find and

00:00:36.810 --> 00:00:39.590 
together with roboticists to find and
dress<00:00:37.050> these<00:00:37.260> issues<00:00:37.710> now<00:00:38.210> so<00:00:39.210> to<00:00:39.300> give<00:00:39.450> you<00:00:39.480> a

00:00:39.590 --> 00:00:39.600 
dress these issues now so to give you a

00:00:39.600 --> 00:00:42.590 
dress these issues now so to give you a
quick<00:00:39.690> overview<00:00:41.030> current<00:00:42.030> issues<00:00:42.270> in<00:00:42.360> robot

00:00:42.590 --> 00:00:42.600 
quick overview current issues in robot

00:00:42.600 --> 00:00:45.190 
quick overview current issues in robot
ethics<00:00:42.899> fit<00:00:43.110> into<00:00:43.260> three<00:00:43.590> broad<00:00:43.770> categories

00:00:45.190 --> 00:00:45.200 
ethics fit into three broad categories

00:00:45.200 --> 00:00:49.700 
ethics fit into three broad categories
the<00:00:46.200> first<00:00:46.230> is<00:00:46.650> safety<00:00:47.030> so<00:00:48.030> as<00:00:48.710> technology

00:00:49.700 --> 00:00:49.710 
the first is safety so as technology

00:00:49.710 --> 00:00:51.590 
the first is safety so as technology
gets<00:00:49.830> more<00:00:50.100> and<00:00:50.190> more<00:00:50.280> autonomous<00:00:50.970> the<00:00:51.270> chain

00:00:51.590 --> 00:00:51.600 
gets more and more autonomous the chain

00:00:51.600 --> 00:00:54.440 
gets more and more autonomous the chain
of<00:00:51.810> causality<00:00:52.740> for<00:00:53.160> harm<00:00:53.370> gets<00:00:53.610> longer<00:00:54.150> gets

00:00:54.440 --> 00:00:54.450 
of causality for harm gets longer gets

00:00:54.450 --> 00:00:56.090 
of causality for harm gets longer gets
incredibly<00:00:54.690> complex<00:00:55.380> so<00:00:55.650> we<00:00:55.680> might<00:00:55.950> need<00:00:56.040> to

00:00:56.090 --> 00:00:56.100 
incredibly complex so we might need to

00:00:56.100 --> 00:00:57.890 
incredibly complex so we might need to
rethink<00:00:56.190> where<00:00:56.760> do<00:00:56.880> assign<00:00:57.150> responsibility

00:00:57.890 --> 00:00:57.900 
rethink where do assign responsibility

00:00:57.900 --> 00:01:00.560 
rethink where do assign responsibility
when<00:00:58.020> something<00:00:58.260> goes<00:00:58.380> wrong<00:00:59.360> also<00:01:00.360> who's

00:01:00.560 --> 00:01:00.570 
when something goes wrong also who's

00:01:00.570 --> 00:01:02.150 
when something goes wrong also who's
responsible<00:01:01.080> for<00:01:01.140> programming<00:01:01.680> ethical

00:01:02.150 --> 00:01:02.160 
responsible for programming ethical

00:01:02.160 --> 00:01:05.030 
responsible for programming ethical
decisions<00:01:02.520> into<00:01:02.730> machines<00:01:03.680> the<00:01:04.680> second

00:01:05.030 --> 00:01:05.040 
decisions into machines the second

00:01:05.040 --> 00:01:09.230 
decisions into machines the second
category<00:01:05.189> is<00:01:05.579> privacy<00:01:07.549> this<00:01:08.549> is<00:01:08.729> obviously<00:01:08.909> a

00:01:09.230 --> 00:01:09.240 
category is privacy this is obviously a

00:01:09.240 --> 00:01:11.240 
category is privacy this is obviously a
big<00:01:09.720> deal<00:01:09.930> generally<00:01:10.350> not<00:01:10.500> just<00:01:10.740> in<00:01:10.860> robotics

00:01:11.240 --> 00:01:11.250 
big deal generally not just in robotics

00:01:11.250 --> 00:01:13.010 
big deal generally not just in robotics
but<00:01:11.430> robotic<00:01:11.909> technology<00:01:12.420> does<00:01:12.659> introduce

00:01:13.010 --> 00:01:13.020 
but robotic technology does introduce

00:01:13.020 --> 00:01:14.690 
but robotic technology does introduce
new<00:01:13.380> ways<00:01:13.530> of<00:01:13.680> collecting<00:01:14.009> data<00:01:14.400> and

00:01:14.690 --> 00:01:14.700 
new ways of collecting data and

00:01:14.700 --> 00:01:16.819 
new ways of collecting data and
interestingly<00:01:15.329> people<00:01:16.020> are<00:01:16.110> reacting<00:01:16.259> more

00:01:16.819 --> 00:01:16.829 
interestingly people are reacting more

00:01:16.829 --> 00:01:19.099 
interestingly people are reacting more
viscerally<00:01:17.460> to<00:01:17.609> these<00:01:17.759> ways<00:01:18.030> than<00:01:18.390> they<00:01:18.719> do<00:01:18.780> to

00:01:19.099 --> 00:01:19.109 
viscerally to these ways than they do to

00:01:19.109 --> 00:01:21.789 
viscerally to these ways than they do to
having<00:01:19.469> their<00:01:19.619> emails<00:01:19.950> monitored<00:01:20.429> by<00:01:20.549> the<00:01:20.609> NSA

00:01:21.789 --> 00:01:21.799 
having their emails monitored by the NSA

00:01:21.799 --> 00:01:25.760 
having their emails monitored by the NSA
the<00:01:22.799> third<00:01:23.039> category<00:01:23.310> is<00:01:23.969> social<00:01:24.929> issues<00:01:25.289> so

00:01:25.760 --> 00:01:25.770 
the third category is social issues so

00:01:25.770 --> 00:01:27.709 
the third category is social issues so
the<00:01:26.219> ethics<00:01:26.549> of<00:01:26.670> our<00:01:26.819> social<00:01:27.060> interactions

00:01:27.709 --> 00:01:27.719 
the ethics of our social interactions

00:01:27.719 --> 00:01:30.080 
the ethics of our social interactions
with<00:01:27.899> robots<00:01:28.289> this<00:01:29.189> includes<00:01:29.549> the<00:01:29.700> ethics<00:01:29.999> of

00:01:30.080 --> 00:01:30.090 
with robots this includes the ethics of

00:01:30.090 --> 00:01:33.200 
with robots this includes the ethics of
child<00:01:30.420> care<00:01:30.600> of<00:01:31.200> elderly<00:01:31.409> care<00:01:31.950> moral<00:01:32.819> value

00:01:33.200 --> 00:01:33.210 
child care of elderly care moral value

00:01:33.210 --> 00:01:35.719 
child care of elderly care moral value
issues<00:01:33.539> like<00:01:33.719> sexual<00:01:34.139> behavior<00:01:34.740> and<00:01:35.009> this<00:01:35.609> is

00:01:35.719 --> 00:01:35.729 
issues like sexual behavior and this is

00:01:35.729 --> 00:01:37.520 
issues like sexual behavior and this is
what<00:01:35.909> I'm<00:01:36.090> particularly<00:01:36.689> interested<00:01:36.899> in<00:01:37.200> and

00:01:37.520 --> 00:01:37.530 
what I'm particularly interested in and

00:01:37.530 --> 00:01:39.200 
what I'm particularly interested in and
want<00:01:38.039> to<00:01:38.069> talk<00:01:38.249> a<00:01:38.280> little<00:01:38.369> bit<00:01:38.549> more<00:01:38.700> about<00:01:38.909> and

00:01:39.200 --> 00:01:39.210 
want to talk a little bit more about and

00:01:39.210 --> 00:01:41.059 
want to talk a little bit more about and
there's<00:01:39.659> one<00:01:39.840> aspect<00:01:40.079> of<00:01:40.530> this<00:01:40.679> in<00:01:40.859> particular

00:01:41.059 --> 00:01:41.069 
there's one aspect of this in particular

00:01:41.069 --> 00:01:43.730 
there's one aspect of this in particular
that<00:01:41.579> fascinates<00:01:42.179> me<00:01:42.329> and<00:01:42.719> that's<00:01:43.409> our

00:01:43.730 --> 00:01:43.740 
that fascinates me and that's our

00:01:43.740 --> 00:01:46.760 
that fascinates me and that's our
tendency<00:01:44.490> to<00:01:44.819> project<00:01:45.030> life<00:01:45.990> like<00:01:46.259> qualities

00:01:46.760 --> 00:01:46.770 
tendency to project life like qualities

00:01:46.770 --> 00:01:50.989 
tendency to project life like qualities
onto<00:01:47.069> robotic<00:01:47.579> objects<00:01:48.319> so<00:01:49.490> studies<00:01:50.490> are

00:01:50.989 --> 00:01:50.999 
onto robotic objects so studies are

00:01:50.999 --> 00:01:53.749 
onto robotic objects so studies are
showing<00:01:51.030> that<00:01:51.810> we<00:01:52.560> perceive<00:01:53.100> and<00:01:53.369> we<00:01:53.429> treat

00:01:53.749 --> 00:01:53.759 
showing that we perceive and we treat

00:01:53.759 --> 00:01:55.760 
showing that we perceive and we treat
robotic<00:01:54.149> objects<00:01:54.600> very<00:01:54.749> differently<00:01:55.289> than

00:01:55.760 --> 00:01:55.770 
robotic objects very differently than

00:01:55.770 --> 00:01:58.309 
robotic objects very differently than
other<00:01:55.979> objects<00:01:56.639> we<00:01:57.119> project<00:01:57.359> onto<00:01:57.840> them<00:01:58.170> we

00:01:58.309 --> 00:01:58.319 
other objects we project onto them we

00:01:58.319 --> 00:02:00.649 
other objects we project onto them we
give<00:01:58.469> them<00:01:58.789> personality<00:01:59.789> in<00:01:59.939> states<00:02:00.240> of<00:02:00.420> mind

00:02:00.649 --> 00:02:00.659 
give them personality in states of mind

00:02:00.659 --> 00:02:02.899 
give them personality in states of mind
and<00:02:00.990> intend<00:02:01.439> and<00:02:01.740> feelings<00:02:02.189> and<00:02:02.399> we<00:02:02.520> react<00:02:02.880> to

00:02:02.899 --> 00:02:02.909 
and intend and feelings and we react to

00:02:02.909 --> 00:02:04.849 
and intend and feelings and we react to
them<00:02:03.270> and<00:02:03.389> they<00:02:03.450> make<00:02:03.569> us<00:02:03.779> feel<00:02:04.139> things<00:02:04.499> like

00:02:04.849 --> 00:02:04.859 
them and they make us feel things like

00:02:04.859 --> 00:02:09.620 
them and they make us feel things like
happy<00:02:05.639> or<00:02:05.880> guilty<00:02:06.359> and<00:02:07.849> more<00:02:08.849> and<00:02:09.000> more<00:02:09.060> robots

00:02:09.620 --> 00:02:09.630 
happy or guilty and more and more robots

00:02:09.630 --> 00:02:11.059 
happy or guilty and more and more robots
are<00:02:09.840> entering<00:02:10.170> into<00:02:10.319> our<00:02:10.530> lives<00:02:10.770> and<00:02:10.979> our

00:02:11.059 --> 00:02:11.069 
are entering into our lives and our

00:02:11.069 --> 00:02:13.100 
are entering into our lives and our
homes<00:02:11.370> that<00:02:11.700> are<00:02:11.730> specifically<00:02:12.450> designed<00:02:12.599> to

00:02:13.100 --> 00:02:13.110 
homes that are specifically designed to

00:02:13.110 --> 00:02:15.130 
homes that are specifically designed to
interact<00:02:13.440> with<00:02:13.530> us<00:02:13.740> on<00:02:13.860> a<00:02:13.890> social<00:02:14.220> level<00:02:14.430> and

00:02:15.130 --> 00:02:15.140 
interact with us on a social level and

00:02:15.140 --> 00:02:17.420 
interact with us on a social level and
psychologists<00:02:16.140> for<00:02:16.319> example<00:02:16.709> sherry<00:02:16.950> Turkle

00:02:17.420 --> 00:02:17.430 
psychologists for example sherry Turkle

00:02:17.430 --> 00:02:20.810 
psychologists for example sherry Turkle
at<00:02:17.520> MIT<00:02:17.819> are<00:02:18.420> showing<00:02:19.230> that<00:02:19.680> we<00:02:20.099> tend<00:02:20.400> to<00:02:20.520> bond

00:02:20.810 --> 00:02:20.820 
at MIT are showing that we tend to bond

00:02:20.820 --> 00:02:22.430 
at MIT are showing that we tend to bond
with<00:02:21.000> these<00:02:21.240> objects<00:02:21.660> when<00:02:21.840> we<00:02:21.960> engage<00:02:22.230> with

00:02:22.430 --> 00:02:22.440 
with these objects when we engage with

00:02:22.440 --> 00:02:26.330 
with these objects when we engage with
them<00:02:22.590> surprisingly<00:02:23.130> strongly<00:02:25.070> now<00:02:26.070> you<00:02:26.130> might

00:02:26.330 --> 00:02:26.340 
them surprisingly strongly now you might

00:02:26.340 --> 00:02:26.870 
them surprisingly strongly now you might
say

00:02:26.870 --> 00:02:26.880 
say

00:02:26.880 --> 00:02:30.230 
say
so<00:02:27.630> what<00:02:28.250> people<00:02:29.250> have<00:02:29.370> always<00:02:29.550> bonded<00:02:29.880> with

00:02:30.230 --> 00:02:30.240 
so what people have always bonded with

00:02:30.240 --> 00:02:32.540 
so what people have always bonded with
objects<00:02:30.870> that's<00:02:31.110> nothing<00:02:31.440> new<00:02:31.470> people<00:02:32.250> fall

00:02:32.540 --> 00:02:32.550 
objects that's nothing new people fall

00:02:32.550 --> 00:02:34.460 
objects that's nothing new people fall
in<00:02:32.700> love<00:02:32.850> with<00:02:32.940> cars<00:02:33.270> and<00:02:33.540> stuffed<00:02:34.020> animals

00:02:34.460 --> 00:02:34.470 
in love with cars and stuffed animals

00:02:34.470 --> 00:02:36.770 
in love with cars and stuffed animals
and<00:02:34.770> phones<00:02:35.070> they<00:02:35.910> bond<00:02:36.150> with<00:02:36.330> virtual

00:02:36.770 --> 00:02:36.780 
and phones they bond with virtual

00:02:36.780 --> 00:02:39.140 
and phones they bond with virtual
objects<00:02:37.320> and<00:02:37.470> video<00:02:37.680> games<00:02:37.920> and<00:02:38.310> that's<00:02:38.700> true

00:02:39.140 --> 00:02:39.150 
objects and video games and that's true

00:02:39.150 --> 00:02:41.420 
objects and video games and that's true
but<00:02:39.690> we<00:02:40.230> believe<00:02:40.590> that<00:02:40.620> this<00:02:40.860> effect<00:02:41.160> is<00:02:41.250> much

00:02:41.420 --> 00:02:41.430 
but we believe that this effect is much

00:02:41.430 --> 00:02:44.450 
but we believe that this effect is much
stronger<00:02:41.790> for<00:02:42.150> robots

