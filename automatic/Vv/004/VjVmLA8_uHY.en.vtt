WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.480 

just<00:00:00.570> recognition<00:00:01.290> is<00:00:01.530> a<00:00:01.589> novel<00:00:02.129> approach<00:00:02.460> to

00:00:02.480 --> 00:00:02.490 
just recognition is a novel approach to

00:00:02.490 --> 00:00:04.760 
just recognition is a novel approach to
human-computer<00:00:03.179> interaction<00:00:03.870> that<00:00:04.470> allows

00:00:04.760 --> 00:00:04.770 
human-computer interaction that allows

00:00:04.770 --> 00:00:07.789 
human-computer interaction that allows
to<00:00:05.040> use<00:00:05.339> the<00:00:06.089> natural<00:00:06.270> body<00:00:06.629> movement<00:00:07.230> to

00:00:07.789 --> 00:00:07.799 
to use the natural body movement to

00:00:07.799 --> 00:00:10.790 
to use the natural body movement to
interact<00:00:08.130> with<00:00:08.189> computers<00:00:09.200> because<00:00:10.200> gestures

00:00:10.790 --> 00:00:10.800 
interact with computers because gestures

00:00:10.800 --> 00:00:13.220 
interact with computers because gestures
are<00:00:11.070> a<00:00:11.130> form<00:00:11.759> of<00:00:11.880> human<00:00:12.269> communication<00:00:12.480> that

00:00:13.220 --> 00:00:13.230 
are a form of human communication that

00:00:13.230 --> 00:00:16.189 
are a form of human communication that
is<00:00:13.349> natural<00:00:13.980> and<00:00:14.160> expressive<00:00:14.820> they<00:00:15.750> allow<00:00:16.139> you

00:00:16.189 --> 00:00:16.199 
is natural and expressive they allow you

00:00:16.199 --> 00:00:19.160 
is natural and expressive they allow you
to<00:00:16.379> concentrate<00:00:17.279> on<00:00:17.430> the<00:00:17.520> task<00:00:17.730> itself<00:00:18.170> using

00:00:19.160 --> 00:00:19.170 
to concentrate on the task itself using

00:00:19.170 --> 00:00:21.349 
to concentrate on the task itself using
what<00:00:19.320> you<00:00:19.439> already<00:00:19.470> do<00:00:19.980> rather<00:00:20.850> than<00:00:21.090> having

00:00:21.349 --> 00:00:21.359 
what you already do rather than having

00:00:21.359 --> 00:00:24.679 
what you already do rather than having
to<00:00:21.570> learn<00:00:21.720> new<00:00:22.230> ways<00:00:22.260> to<00:00:22.500> interact<00:00:23.210> our<00:00:24.210> goal

00:00:24.679 --> 00:00:24.689 
to learn new ways to interact our goal

00:00:24.689 --> 00:00:27.620 
to learn new ways to interact our goal
is<00:00:24.990> to<00:00:25.500> enable<00:00:25.769> unmanned<00:00:26.369> vehicles<00:00:26.910> to

00:00:27.620 --> 00:00:27.630 
is to enable unmanned vehicles to

00:00:27.630 --> 00:00:30.230 
is to enable unmanned vehicles to
recognize<00:00:28.170> the<00:00:28.650> aircraft<00:00:29.189> handling<00:00:29.760> gestures

00:00:30.230 --> 00:00:30.240 
recognize the aircraft handling gestures

00:00:30.240 --> 00:00:33.530 
recognize the aircraft handling gestures
already<00:00:31.050> made<00:00:31.289> by<00:00:31.500> deck<00:00:31.769> crews<00:00:32.099> the<00:00:33.000> aircraft

00:00:33.530 --> 00:00:33.540 
already made by deck crews the aircraft

00:00:33.540 --> 00:00:36.470 
already made by deck crews the aircraft
handling<00:00:33.899> gestures<00:00:34.410> use<00:00:35.130> both<00:00:35.520> body<00:00:35.940> posture

00:00:36.470 --> 00:00:36.480 
handling gestures use both body posture

00:00:36.480 --> 00:00:39.709 
handling gestures use both body posture
and<00:00:36.690> hand<00:00:37.290> shapes<00:00:37.649> so<00:00:38.550> it<00:00:38.700> is<00:00:38.820> important<00:00:39.420> for

00:00:39.709 --> 00:00:39.719 
and hand shapes so it is important for

00:00:39.719 --> 00:00:43.729 
and hand shapes so it is important for
our<00:00:39.780> system<00:00:40.350> to<00:00:40.590> know<00:00:41.160> both<00:00:41.579> information<00:00:42.739> my

00:00:43.729 --> 00:00:43.739 
our system to know both information my

00:00:43.739 --> 00:00:45.950 
our system to know both information my
research<00:00:44.190> concentrates<00:00:44.940> on<00:00:45.180> developing<00:00:45.870> a

00:00:45.950 --> 00:00:45.960 
research concentrates on developing a

00:00:45.960 --> 00:00:48.860 
research concentrates on developing a
vision<00:00:46.260> based<00:00:46.710> system<00:00:47.160> that<00:00:47.399> recognizes<00:00:48.120> body

00:00:48.860 --> 00:00:48.870 
vision based system that recognizes body

00:00:48.870 --> 00:00:51.200 
vision based system that recognizes body
and<00:00:49.079> hand<00:00:49.379> gestures<00:00:49.469> from<00:00:50.399> continuous<00:00:51.000> input

00:00:51.200 --> 00:00:51.210 
and hand gestures from continuous input

00:00:51.210 --> 00:00:54.740 
and hand gestures from continuous input
stream<00:00:52.039> my<00:00:53.039> system<00:00:53.460> uses<00:00:53.879> a<00:00:54.030> single<00:00:54.510> stereo

00:00:54.740 --> 00:00:54.750 
stream my system uses a single stereo

00:00:54.750 --> 00:00:57.470 
stream my system uses a single stereo
camera<00:00:55.260> to<00:00:55.770> track<00:00:56.039> body<00:00:56.309> motion<00:00:56.820> and<00:00:57.000> hand

00:00:57.470 --> 00:00:57.480 
camera to track body motion and hand

00:00:57.480 --> 00:01:00.290 
camera to track body motion and hand
shaped<00:00:57.890> simultaneously<00:00:58.890> and<00:00:59.129> combine<00:01:00.000> these

00:01:00.290 --> 00:01:00.300 
shaped simultaneously and combine these

00:01:00.300 --> 00:01:03.049 
shaped simultaneously and combine these
information<00:01:01.109> together<00:01:01.440> to<00:01:02.160> recognize<00:01:02.399> body

00:01:03.049 --> 00:01:03.059 
information together to recognize body

00:01:03.059 --> 00:01:06.620 
information together to recognize body
and<00:01:03.270> hand<00:01:03.510> gestures<00:01:04.549> we<00:01:05.549> use<00:01:05.760> motion<00:01:06.210> learning

00:01:06.620 --> 00:01:06.630 
and hand gestures we use motion learning

00:01:06.630 --> 00:01:08.960 
and hand gestures we use motion learning
to<00:01:07.020> train<00:01:07.350> the<00:01:07.619> system<00:01:07.799> with<00:01:08.490> lots<00:01:08.790> of

00:01:08.960 --> 00:01:08.970 
to train the system with lots of

00:01:08.970 --> 00:01:11.899 
to train the system with lots of
examples<00:01:09.600> along<00:01:10.530> the<00:01:10.710> system<00:01:11.130> to<00:01:11.250> learn<00:01:11.490> how

00:01:11.899 --> 00:01:11.909 
examples along the system to learn how

00:01:11.909 --> 00:01:15.710 
examples along the system to learn how
to<00:01:11.939> recognize<00:01:12.600> each<00:01:12.900> gesture<00:01:14.210> there<00:01:15.210> are<00:01:15.390> four

00:01:15.710 --> 00:01:15.720 
to recognize each gesture there are four

00:01:15.720 --> 00:01:18.020 
to recognize each gesture there are four
steps<00:01:16.020> that<00:01:16.380> our<00:01:16.500> system<00:01:16.920> take<00:01:17.189> to<00:01:17.520> recognize

00:01:18.020 --> 00:01:18.030 
steps that our system take to recognize

00:01:18.030 --> 00:01:21.350 
steps that our system take to recognize
gestures<00:01:18.770> first<00:01:19.770> from<00:01:20.490> the<00:01:20.670> input<00:01:21.030> image

00:01:21.350 --> 00:01:21.360 
gestures first from the input image

00:01:21.360 --> 00:01:23.719 
gestures first from the input image
obtained<00:01:21.900> from<00:01:22.140> a<00:01:22.259> stayer<00:01:22.530> camera<00:01:22.979> we

00:01:23.719 --> 00:01:23.729 
obtained from a stayer camera we

00:01:23.729 --> 00:01:26.179 
obtained from a stayer camera we
calculate<00:01:24.210> 3d<00:01:24.689> images<00:01:25.170> and<00:01:25.409> remove<00:01:26.009> the

00:01:26.179 --> 00:01:26.189 
calculate 3d images and remove the

00:01:26.189 --> 00:01:29.270 
calculate 3d images and remove the
background<00:01:26.570> the<00:01:27.570> second<00:01:28.020> our<00:01:28.829> system

00:01:29.270 --> 00:01:29.280 
background the second our system

00:01:29.280 --> 00:01:32.090 
background the second our system
estimate<00:01:29.759> 3d<00:01:30.270> body<00:01:30.509> posture<00:01:31.079> by<00:01:31.560> fitting<00:01:31.890> a

00:01:32.090 --> 00:01:32.100 
estimate 3d body posture by fitting a

00:01:32.100 --> 00:01:34.700 
estimate 3d body posture by fitting a
skeletal<00:01:32.700> body<00:01:32.880> model<00:01:33.390> to<00:01:33.869> the<00:01:33.990> input<00:01:34.350> image

00:01:34.700 --> 00:01:34.710 
skeletal body model to the input image

00:01:34.710 --> 00:01:37.130 
skeletal body model to the input image
we<00:01:35.340> extract<00:01:35.790> various<00:01:36.360> visual<00:01:36.720> features

00:01:37.130 --> 00:01:37.140 
we extract various visual features

00:01:37.140 --> 00:01:39.800 
we extract various visual features
including<00:01:37.920> 3d<00:01:38.280> point<00:01:38.520> cloud<00:01:38.640> control<00:01:39.600> lines

00:01:39.800 --> 00:01:39.810 
including 3d point cloud control lines

00:01:39.810 --> 00:01:42.710 
including 3d point cloud control lines
and<00:01:40.200> the<00:01:40.560> history<00:01:40.950> of<00:01:41.100> motion<00:01:41.369> these<00:01:42.150> features

00:01:42.710 --> 00:01:42.720 
and the history of motion these features

00:01:42.720 --> 00:01:45.319 
and the history of motion these features
are<00:01:42.960> computed<00:01:43.530> both<00:01:44.009> from<00:01:44.369> the<00:01:44.490> image<00:01:44.850> and<00:01:45.060> the

00:01:45.319 --> 00:01:45.329 
are computed both from the image and the

00:01:45.329 --> 00:01:47.840 
are computed both from the image and the
skeletal<00:01:45.780> model<00:01:45.930> and<00:01:46.380> the<00:01:47.130> two<00:01:47.340> sets<00:01:47.640> of

00:01:47.840 --> 00:01:47.850 
skeletal model and the two sets of

00:01:47.850 --> 00:01:49.999 
skeletal model and the two sets of
futures<00:01:48.299> are<00:01:48.540> compared<00:01:49.020> allowing<00:01:49.890> our

00:01:49.999 --> 00:01:50.009 
futures are compared allowing our

00:01:50.009 --> 00:01:51.649 
futures are compared allowing our
program<00:01:50.579> to<00:01:50.759> come<00:01:50.939> up<00:01:51.060> with<00:01:51.210> the<00:01:51.360> most

00:01:51.649 --> 00:01:51.659 
program to come up with the most

00:01:51.659 --> 00:01:55.670 
program to come up with the most
probable<00:01:52.259> posture<00:01:53.299> 1/3<00:01:54.299> once<00:01:55.079> we<00:01:55.320> know<00:01:55.530> the

00:01:55.670 --> 00:01:55.680 
probable posture 1/3 once we know the

00:01:55.680 --> 00:01:58.700 
probable posture 1/3 once we know the
body<00:01:55.860> posture<00:01:56.430> we<00:01:57.119> know<00:01:57.360> approximately<00:01:57.710> where

00:01:58.700 --> 00:01:58.710 
body posture we know approximately where

00:01:58.710 --> 00:02:01.819 
body posture we know approximately where
their<00:01:58.950> hands<00:01:59.219> are<00:01:59.430> located<00:02:00.290> we<00:02:01.290> search<00:02:01.590> around

00:02:01.819 --> 00:02:01.829 
their hands are located we search around

00:02:01.829 --> 00:02:04.450 
their hands are located we search around
each<00:02:02.340> of<00:02:02.640> the<00:02:02.790> estimated<00:02:03.299> risk<00:02:03.689> positions

00:02:04.450 --> 00:02:04.460 
each of the estimated risk positions

00:02:04.460 --> 00:02:06.950 
each of the estimated risk positions
compute<00:02:05.460> visual<00:02:05.880> features<00:02:06.299> in<00:02:06.540> that<00:02:06.750> region

00:02:06.950 --> 00:02:06.960 
compute visual features in that region

00:02:06.960 --> 00:02:10.219 
compute visual features in that region
and<00:02:07.520> estimate<00:02:08.520> the<00:02:08.550> probability<00:02:09.420> that<00:02:09.929> what

00:02:10.219 --> 00:02:10.229 
and estimate the probability that what

00:02:10.229 --> 00:02:12.770 
and estimate the probability that what
we<00:02:10.440> see<00:02:10.739> there<00:02:11.039> is<00:02:11.250> one<00:02:11.940> of<00:02:12.060> the<00:02:12.180> known<00:02:12.420> hand

00:02:12.770 --> 00:02:12.780 
we see there is one of the known hand

00:02:12.780 --> 00:02:13.790 
we see there is one of the known hand
shapes<00:02:13.110> used

00:02:13.790 --> 00:02:13.800 
shapes used

00:02:13.800 --> 00:02:16.910 
shapes used
in<00:02:13.890> aircraft<00:02:14.520> handling<00:02:15.030> for<00:02:15.810> example<00:02:16.320> pump

00:02:16.910 --> 00:02:16.920 
in aircraft handling for example pump

00:02:16.920 --> 00:02:19.760 
in aircraft handling for example pump
opened<00:02:17.400> closed<00:02:18.030> and<00:02:18.330> thumb<00:02:18.930> up<00:02:19.140> and<00:02:19.350> thumb

00:02:19.760 --> 00:02:19.770 
opened closed and thumb up and thumb

00:02:19.770 --> 00:02:23.060 
opened closed and thumb up and thumb
down<00:02:20.040> as<00:02:20.390> the<00:02:21.390> last<00:02:21.630> tab<00:02:21.960> we<00:02:22.350> combined<00:02:22.830> the

00:02:23.060 --> 00:02:23.070 
down as the last tab we combined the

00:02:23.070 --> 00:02:26.000 
down as the last tab we combined the
estimated<00:02:23.760> body<00:02:24.030> posture<00:02:24.540> and<00:02:24.750> hand<00:02:25.170> shape<00:02:25.500> to

00:02:26.000 --> 00:02:26.010 
estimated body posture and hand shape to

00:02:26.010 --> 00:02:29.540 
estimated body posture and hand shape to
determine<00:02:26.490> gestures<00:02:27.230> we<00:02:28.230> collected<00:02:28.830> 24

00:02:29.540 --> 00:02:29.550 
determine gestures we collected 24

00:02:29.550 --> 00:02:32.510 
determine gestures we collected 24
aircraft<00:02:30.270> handling<00:02:30.810> gestures<00:02:31.290> from<00:02:32.040> 20

00:02:32.510 --> 00:02:32.520 
aircraft handling gestures from 20

00:02:32.520 --> 00:02:35.900 
aircraft handling gestures from 20
people<00:02:32.850> giving<00:02:33.840> us<00:02:33.960> a<00:02:34.140> 400<00:02:34.980> simple<00:02:35.460> gestures

00:02:35.900 --> 00:02:35.910 
people giving us a 400 simple gestures

00:02:35.910 --> 00:02:38.150 
people giving us a 400 simple gestures
to<00:02:36.570> use<00:02:36.750> to<00:02:36.960> teach<00:02:37.200> the<00:02:37.410> system<00:02:37.800> to<00:02:37.950> recognize

00:02:38.150 --> 00:02:38.160 
to use to teach the system to recognize

00:02:38.160 --> 00:02:41.360 
to use to teach the system to recognize
the<00:02:38.670> gestures<00:02:39.470> we<00:02:40.470> use<00:02:40.770> probabilistic

00:02:41.360 --> 00:02:41.370 
the gestures we use probabilistic

00:02:41.370 --> 00:02:44.420 
the gestures we use probabilistic
graphical<00:02:41.790> model<00:02:42.600> called<00:02:42.990> latent<00:02:43.830> dynamic

00:02:44.420 --> 00:02:44.430 
graphical model called latent dynamic

00:02:44.430 --> 00:02:47.180 
graphical model called latent dynamic
conditional<00:02:45.000> random<00:02:45.090> field<00:02:45.680> this<00:02:46.680> model

00:02:47.180 --> 00:02:47.190 
conditional random field this model

00:02:47.190 --> 00:02:49.550 
conditional random field this model
learns<00:02:47.400> the<00:02:47.700> distribution<00:02:48.090> of<00:02:48.660> the<00:02:49.110> patterns

00:02:49.550 --> 00:02:49.560 
learns the distribution of the patterns

00:02:49.560 --> 00:02:51.830 
learns the distribution of the patterns
of<00:02:49.740> each<00:02:50.040> gesture<00:02:50.400> as<00:02:50.820> well<00:02:51.390> as<00:02:51.600> the

00:02:51.830 --> 00:02:51.840 
of each gesture as well as the

00:02:51.840 --> 00:02:55.160 
of each gesture as well as the
transition<00:02:52.440> between<00:02:52.620> gestures<00:02:53.540> we<00:02:54.540> use<00:02:54.900> this

00:02:55.160 --> 00:02:55.170 
transition between gestures we use this

00:02:55.170 --> 00:02:57.590 
transition between gestures we use this
with<00:02:55.770> a<00:02:55.800> sliding<00:02:56.310> window<00:02:56.460> to<00:02:57.090> recognize

00:02:57.590 --> 00:02:57.600 
with a sliding window to recognize

00:02:57.600 --> 00:03:00.530 
with a sliding window to recognize
gestures<00:02:57.900> continuously<00:02:58.890> and<00:02:59.480> apply<00:03:00.480> the

00:03:00.530 --> 00:03:00.540 
gestures continuously and apply the

00:03:00.540 --> 00:03:02.840 
gestures continuously and apply the
multi-layered<00:03:01.410> filtering<00:03:02.190> technique<00:03:02.640> we

00:03:02.840 --> 00:03:02.850 
multi-layered filtering technique we

00:03:02.850 --> 00:03:05.720 
multi-layered filtering technique we
developed<00:03:03.390> to<00:03:04.170> make<00:03:04.380> the<00:03:04.650> recognition<00:03:05.460> more

00:03:05.720 --> 00:03:05.730 
developed to make the recognition more

00:03:05.730 --> 00:03:09.590 
developed to make the recognition more
robust<00:03:06.890> there<00:03:07.890> is<00:03:08.100> still<00:03:08.550> a<00:03:08.760> considerable

00:03:09.590 --> 00:03:09.600 
robust there is still a considerable

00:03:09.600 --> 00:03:10.640 
robust there is still a considerable
amount<00:03:09.870> of<00:03:09.930> work<00:03:10.170> to<00:03:10.320> be<00:03:10.410> done

00:03:10.640 --> 00:03:10.650 
amount of work to be done

00:03:10.650 --> 00:03:12.550 
amount of work to be done
in<00:03:10.890> the<00:03:11.070> field<00:03:11.310> of<00:03:11.460> gesture<00:03:11.910> recognition

00:03:12.550 --> 00:03:12.560 
in the field of gesture recognition

00:03:12.560 --> 00:03:15.290 
in the field of gesture recognition
things<00:03:13.560> we<00:03:13.740> continue<00:03:14.220> to<00:03:14.340> work<00:03:14.490> on<00:03:14.730> include

00:03:15.290 --> 00:03:15.300 
things we continue to work on include

00:03:15.300 --> 00:03:18.080 
things we continue to work on include
improving<00:03:15.930> the<00:03:16.140> reliability<00:03:17.090> adaptability

00:03:18.080 --> 00:03:18.090 
improving the reliability adaptability

00:03:18.090 --> 00:03:20.720 
improving the reliability adaptability
to<00:03:18.360> new<00:03:18.540> gestures<00:03:19.260> and<00:03:19.730> developing

00:03:20.720 --> 00:03:20.730 
to new gestures and developing

00:03:20.730 --> 00:03:23.240 
to new gestures and developing
appropriate<00:03:21.390> feedback<00:03:21.660> mechanisms<00:03:22.560> for

00:03:23.240 --> 00:03:23.250 
appropriate feedback mechanisms for

00:03:23.250 --> 00:03:26.000 
appropriate feedback mechanisms for
example<00:03:23.700> the<00:03:23.910> system<00:03:24.090> can<00:03:24.420> say<00:03:24.600> I<00:03:24.870> get<00:03:25.260> it<00:03:25.410> or<00:03:25.590> I

00:03:26.000 --> 00:03:26.010 
example the system can say I get it or I

00:03:26.010 --> 00:03:28.640 
example the system can say I get it or I
don't<00:03:26.100> get<00:03:26.490> it

