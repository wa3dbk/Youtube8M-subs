WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.869 

hi<00:00:00.390> I<00:00:00.630> am<00:00:00.690> Fred<00:00:01.140> from<00:00:01.319> the<00:00:01.469> elastic<00:00:01.650> team<00:00:02.040> here

00:00:02.869 --> 00:00:02.879 
hi I am Fred from the elastic team here

00:00:02.879 --> 00:00:04.789 
hi I am Fred from the elastic team here
I<00:00:03.060> will<00:00:03.389> give<00:00:03.510> a<00:00:03.540> high-level<00:00:03.810> overview<00:00:04.200> of

00:00:04.789 --> 00:00:04.799 
I will give a high-level overview of

00:00:04.799 --> 00:00:06.680 
I will give a high-level overview of
what<00:00:05.009> elastic<00:00:05.490> can<00:00:05.609> do<00:00:05.730> now<00:00:05.910> and<00:00:06.150> in<00:00:06.600> the

00:00:06.680 --> 00:00:06.690 
what elastic can do now and in the

00:00:06.690 --> 00:00:09.589 
what elastic can do now and in the
future<00:00:07.069> at<00:00:08.069> the<00:00:08.130> heart<00:00:08.490> of<00:00:08.639> version<00:00:09.000> zero<00:00:09.179> five

00:00:09.589 --> 00:00:09.599 
future at the heart of version zero five

00:00:09.599 --> 00:00:12.200 
future at the heart of version zero five
is<00:00:09.900> the<00:00:10.410> interactive<00:00:10.830> training<00:00:11.219> of<00:00:11.490> a<00:00:11.790> pixel

00:00:12.200 --> 00:00:12.210 
is the interactive training of a pixel

00:00:12.210 --> 00:00:14.930 
is the interactive training of a pixel
level<00:00:12.300> classifier<00:00:13.070> after<00:00:14.070> training<00:00:14.370> the

00:00:14.930 --> 00:00:14.940 
level classifier after training the

00:00:14.940 --> 00:00:16.790 
level classifier after training the
classifier<00:00:15.450> can<00:00:15.480> be<00:00:15.750> used<00:00:15.900> to<00:00:16.139> batch<00:00:16.289> process

00:00:16.790 --> 00:00:16.800 
classifier can be used to batch process

00:00:16.800 --> 00:00:19.580 
classifier can be used to batch process
very<00:00:17.160> large<00:00:17.369> numbers<00:00:17.789> of<00:00:17.880> images<00:00:18.320> the<00:00:19.320> images

00:00:19.580 --> 00:00:19.590 
very large numbers of images the images

00:00:19.590 --> 00:00:20.990 
very large numbers of images the images
can<00:00:19.830> have<00:00:19.949> two<00:00:20.189> or<00:00:20.340> three<00:00:20.520> spatial<00:00:20.970> dimensions

00:00:20.990 --> 00:00:21.000 
can have two or three spatial dimensions

00:00:21.000 --> 00:00:23.840 
can have two or three spatial dimensions
plus<00:00:21.779> a<00:00:21.900> spectral<00:00:22.380> dimension<00:00:22.859> everything<00:00:23.730> is

00:00:23.840 --> 00:00:23.850 
plus a spectral dimension everything is

00:00:23.850 --> 00:00:26.480 
plus a spectral dimension everything is
multi-threaded<00:00:24.150> for<00:00:24.630> speed<00:00:25.010> in<00:00:26.010> settings

00:00:26.480 --> 00:00:26.490 
multi-threaded for speed in settings

00:00:26.490 --> 00:00:28.370 
multi-threaded for speed in settings
where<00:00:26.970> the<00:00:27.060> pixel<00:00:27.449> level<00:00:27.539> classifier<00:00:28.170> won't

00:00:28.370 --> 00:00:28.380 
where the pixel level classifier won't

00:00:28.380 --> 00:00:28.820 
where the pixel level classifier won't
work

00:00:28.820 --> 00:00:28.830 
work

00:00:28.830 --> 00:00:30.380 
work
you<00:00:29.160> can<00:00:29.369> resort<00:00:29.670> to<00:00:29.760> an<00:00:30.029> interactive

00:00:30.380 --> 00:00:30.390 
you can resort to an interactive

00:00:30.390 --> 00:00:33.770 
you can resort to an interactive
watershed<00:00:32.030> unsupervised<00:00:33.030> decomposition

00:00:33.770 --> 00:00:33.780 
watershed unsupervised decomposition

00:00:33.780 --> 00:00:35.810 
watershed unsupervised decomposition
allows<00:00:34.380> you<00:00:34.559> to<00:00:34.710> find<00:00:34.950> low<00:00:35.219> dimensional

00:00:35.810 --> 00:00:35.820 
allows you to find low dimensional

00:00:35.820 --> 00:00:38.000 
allows you to find low dimensional
representations<00:00:36.540> for<00:00:36.750> spectral<00:00:37.170> images<00:00:37.559> and

00:00:38.000 --> 00:00:38.010 
representations for spectral images and

00:00:38.010 --> 00:00:39.910 
representations for spectral images and
the<00:00:38.520> plugin<00:00:38.820> mechanism<00:00:39.270> offers

00:00:39.910 --> 00:00:39.920 
the plugin mechanism offers

00:00:39.920 --> 00:00:44.810 
the plugin mechanism offers
extensibility<00:00:42.530> everything<00:00:43.530> is<00:00:43.860> open-source

00:00:44.810 --> 00:00:44.820 
extensibility everything is open-source

00:00:44.820 --> 00:00:47.900 
extensibility everything is open-source
and<00:00:45.300> we<00:00:46.260> share<00:00:46.500> binaries<00:00:47.010> for<00:00:47.579> major

00:00:47.900 --> 00:00:47.910 
and we share binaries for major

00:00:47.910 --> 00:00:51.410 
and we share binaries for major
platforms<00:00:49.640> now<00:00:50.640> I<00:00:50.670> want<00:00:51.090> to<00:00:51.180> give<00:00:51.300> you<00:00:51.329> an

00:00:51.410 --> 00:00:51.420 
platforms now I want to give you an

00:00:51.420 --> 00:00:52.910 
platforms now I want to give you an
intuition<00:00:51.719> for<00:00:51.989> how<00:00:52.050> the<00:00:52.199> interactive

00:00:52.910 --> 00:00:52.920 
intuition for how the interactive

00:00:52.920 --> 00:00:56.959 
intuition for how the interactive
machine<00:00:53.219> learning<00:00:53.489> works<00:00:55.370> features<00:00:56.370> can<00:00:56.850> be

00:00:56.959 --> 00:00:56.969 
machine learning works features can be

00:00:56.969 --> 00:00:59.150 
machine learning works features can be
selected<00:00:57.360> by<00:00:57.390> the<00:00:57.510> user<00:00:57.780> and<00:00:58.770> they<00:00:59.100> are

00:00:59.150 --> 00:00:59.160 
selected by the user and they are

00:00:59.160 --> 00:01:00.950 
selected by the user and they are
computed<00:00:59.670> for<00:00:59.850> each<00:00:59.910> and<00:01:00.210> every<00:01:00.390> pixel<00:01:00.719> in<00:01:00.840> the

00:01:00.950 --> 00:01:00.960 
computed for each and every pixel in the

00:01:00.960 --> 00:01:03.560 
computed for each and every pixel in the
image<00:01:01.340> so<00:01:02.340> every<00:01:02.609> pixel<00:01:02.940> on<00:01:03.030> the<00:01:03.149> image<00:01:03.359> is

00:01:03.560 --> 00:01:03.570 
image so every pixel on the image is

00:01:03.570 --> 00:01:06.980 
image so every pixel on the image is
mapped<00:01:04.049> to<00:01:04.290> one<00:01:04.619> point<00:01:04.890> in<00:01:05.070> future<00:01:05.430> space<00:01:05.990> now

00:01:06.980 --> 00:01:06.990 
mapped to one point in future space now

00:01:06.990 --> 00:01:08.810 
mapped to one point in future space now
the<00:01:07.049> user<00:01:07.380> can<00:01:07.530> provide<00:01:07.920> annotations<00:01:08.610> in<00:01:08.729> the

00:01:08.810 --> 00:01:08.820 
the user can provide annotations in the

00:01:08.820 --> 00:01:10.670 
the user can provide annotations in the
image<00:01:09.000> domain<00:01:09.210> which<00:01:10.170> are<00:01:10.350> of<00:01:10.439> course

00:01:10.670 --> 00:01:10.680 
image domain which are of course

00:01:10.680 --> 00:01:13.969 
image domain which are of course
transferred<00:01:11.520> to<00:01:11.729> the<00:01:12.330> feature<00:01:12.510> space<00:01:12.869> and<00:01:13.770> can

00:01:13.969 --> 00:01:13.979 
transferred to the feature space and can

00:01:13.979 --> 00:01:16.580 
transferred to the feature space and can
we<00:01:14.100> use<00:01:14.220> to<00:01:14.400> train<00:01:14.640> a<00:01:14.670> classifier<00:01:15.409> once<00:01:16.409> the

00:01:16.580 --> 00:01:16.590 
we use to train a classifier once the

00:01:16.590 --> 00:01:19.160 
we use to train a classifier once the
classifier<00:01:17.040> has<00:01:17.070> been<00:01:17.340> trained<00:01:18.170> predictions

00:01:19.160 --> 00:01:19.170 
classifier has been trained predictions

00:01:19.170 --> 00:01:21.170 
classifier has been trained predictions
can<00:01:19.320> be<00:01:19.380> made<00:01:19.619> for<00:01:20.250> each<00:01:20.400> and<00:01:20.610> every<00:01:20.790> point<00:01:21.000> in

00:01:21.170 --> 00:01:21.180 
can be made for each and every point in

00:01:21.180 --> 00:01:22.969 
can be made for each and every point in
feature<00:01:21.360> space<00:01:21.720> that<00:01:22.229> means<00:01:22.380> for<00:01:22.770> each<00:01:22.799> and

00:01:22.969 --> 00:01:22.979 
feature space that means for each and

00:01:22.979 --> 00:01:24.920 
feature space that means for each and
every<00:01:23.159> pixel<00:01:23.520> on<00:01:23.580> the<00:01:23.640> image<00:01:23.909> domain<00:01:24.119> if

00:01:24.920 --> 00:01:24.930 
every pixel on the image domain if

00:01:24.930 --> 00:01:27.679 
every pixel on the image domain if
you're<00:01:25.619> not<00:01:25.799> happy<00:01:26.159> with<00:01:26.310> the<00:01:26.939> current<00:01:27.270> result

00:01:27.679 --> 00:01:27.689 
you're not happy with the current result

00:01:27.689 --> 00:01:29.719 
you're not happy with the current result
for<00:01:28.320> instance<00:01:28.619> I<00:01:28.740> see<00:01:28.920> some<00:01:29.100> problems<00:01:29.460> here

00:01:29.719 --> 00:01:29.729 
for instance I see some problems here

00:01:29.729 --> 00:01:31.819 
for instance I see some problems here
you<00:01:30.180> can<00:01:30.360> different<00:01:30.720> rotations<00:01:31.409> and<00:01:31.619> thus

00:01:31.819 --> 00:01:31.829 
you can different rotations and thus

00:01:31.829 --> 00:01:34.969 
you can different rotations and thus
improve<00:01:32.640> your<00:01:32.820> classifier<00:01:33.439> all<00:01:34.439> of<00:01:34.680> this<00:01:34.799> is

00:01:34.969 --> 00:01:34.979 
improve your classifier all of this is

00:01:34.979 --> 00:01:36.920 
improve your classifier all of this is
shown<00:01:35.009> in<00:01:35.250> more<00:01:35.400> detail<00:01:35.700> in<00:01:35.939> the<00:01:36.329> other<00:01:36.540> videos

00:01:36.920 --> 00:01:36.930 
shown in more detail in the other videos

00:01:36.930 --> 00:01:38.330 
shown in more detail in the other videos
that<00:01:37.170> you<00:01:37.259> can<00:01:37.409> see<00:01:37.560> on<00:01:37.650> the<00:01:37.740> documentation

00:01:38.330 --> 00:01:38.340 
that you can see on the documentation

00:01:38.340 --> 00:01:42.859 
that you can see on the documentation
page<00:01:40.070> now<00:01:41.070> here<00:01:41.280> I<00:01:41.310> have<00:01:41.700> an<00:01:41.820> example<00:01:42.150> where

00:01:42.859 --> 00:01:42.869 
page now here I have an example where

00:01:42.869 --> 00:01:45.730 
page now here I have an example where
such<00:01:43.170> a<00:01:43.409> pixel<00:01:44.159> every<00:01:44.399> classifier<00:01:45.000> won't<00:01:45.299> work

00:01:45.730 --> 00:01:45.740 
such a pixel every classifier won't work

00:01:45.740 --> 00:01:49.429 
such a pixel every classifier won't work
for<00:01:46.740> example<00:01:46.890> I<00:01:47.399> want<00:01:47.729> to<00:01:47.790> carve<00:01:48.509> out<00:01:48.780> this

00:01:49.429 --> 00:01:49.439 
for example I want to carve out this

00:01:49.439 --> 00:01:51.170 
for example I want to carve out this
particular<00:01:49.680> neuron<00:01:50.310> out<00:01:50.640> of<00:01:50.820> the<00:01:51.000> three

00:01:51.170 --> 00:01:51.180 
particular neuron out of the three

00:01:51.180 --> 00:01:54.679 
particular neuron out of the three
dimensional<00:01:51.600> data<00:01:51.810> set<00:01:52.110> however<00:01:53.329> it<00:01:54.329> looks

00:01:54.679 --> 00:01:54.689 
dimensional data set however it looks

00:01:54.689 --> 00:01:57.830 
dimensional data set however it looks
just<00:01:54.960> the<00:01:55.110> same<00:01:55.439> as<00:01:55.710> its<00:01:56.250> neighbor<00:01:56.600> so<00:01:57.600> I<00:01:57.659> need

00:01:57.830 --> 00:01:57.840 
just the same as its neighbor so I need

00:01:57.840 --> 00:02:00.499 
just the same as its neighbor so I need
a<00:01:57.869> different<00:01:58.049> strategy<00:01:58.790> here<00:01:59.790> we<00:02:00.119> use<00:02:00.329> an

00:02:00.499 --> 00:02:00.509 
a different strategy here we use an

00:02:00.509 --> 00:02:03.350 
a different strategy here we use an
amortized<00:02:01.110> watershed<00:02:01.860> that<00:02:02.329> relies<00:02:03.329> on

00:02:03.350 --> 00:02:03.360 
amortized watershed that relies on

00:02:03.360 --> 00:02:05.870 
amortized watershed that relies on
boundary<00:02:03.899> information<00:02:04.020> and<00:02:04.619> on<00:02:05.460> labels

00:02:05.870 --> 00:02:05.880 
boundary information and on labels

00:02:05.880 --> 00:02:09.770 
boundary information and on labels
provided<00:02:06.450> by<00:02:06.600> the<00:02:06.659> users<00:02:07.610> so<00:02:08.610> here<00:02:08.970> these<00:02:09.569> are

00:02:09.770 --> 00:02:09.780 
provided by the users so here these are

00:02:09.780 --> 00:02:12.199 
provided by the users so here these are
seeds<00:02:10.110> and<00:02:10.410> by<00:02:11.039> providing<00:02:11.430> a<00:02:11.640> couple<00:02:11.910> of<00:02:11.970> seeds

00:02:12.199 --> 00:02:12.209 
seeds and by providing a couple of seeds

00:02:12.209 --> 00:02:12.940 
seeds and by providing a couple of seeds
you

00:02:12.940 --> 00:02:12.950 
you

00:02:12.950 --> 00:02:15.220 
you
can<00:02:13.099> distinguish<00:02:14.060> your<00:02:14.510> objects<00:02:14.930> of<00:02:15.080> interest

00:02:15.220 --> 00:02:15.230 
can distinguish your objects of interest

00:02:15.230 --> 00:02:16.360 
can distinguish your objects of interest
from<00:02:15.620> the<00:02:15.770> background

00:02:16.360 --> 00:02:16.370 
from the background

00:02:16.370 --> 00:02:20.559 
from the background
and<00:02:16.790> render<00:02:17.150> them<00:02:17.300> in<00:02:17.480> 3d<00:02:19.120> in<00:02:20.120> the<00:02:20.239> upcoming

00:02:20.559 --> 00:02:20.569 
and render them in 3d in the upcoming

00:02:20.569 --> 00:02:23.229 
and render them in 3d in the upcoming
version<00:02:20.810> 0.6<00:02:21.580> we<00:02:22.580> will<00:02:22.730> have<00:02:22.849> all<00:02:23.000> of<00:02:23.030> the

00:02:23.229 --> 00:02:23.239 
version 0.6 we will have all of the

00:02:23.239 --> 00:02:25.479 
version 0.6 we will have all of the
above<00:02:23.480> plus<00:02:23.540> the<00:02:24.379> possibility<00:02:24.950> to<00:02:25.129> combine

00:02:25.479 --> 00:02:25.489 
above plus the possibility to combine

00:02:25.489 --> 00:02:27.910 
above plus the possibility to combine
modules<00:02:25.970> into<00:02:26.180> specific<00:02:26.660> workflows<00:02:27.050> there

00:02:27.910 --> 00:02:27.920 
modules into specific workflows there

00:02:27.920 --> 00:02:29.589 
modules into specific workflows there
will<00:02:28.040> be<00:02:28.069> just-in-time<00:02:28.640> computation<00:02:29.420> that

00:02:29.589 --> 00:02:29.599 
will be just-in-time computation that

00:02:29.599 --> 00:02:32.589 
will be just-in-time computation that
will<00:02:30.380> bring<00:02:30.970> interactive<00:02:31.970> machine<00:02:32.269> learning

00:02:32.589 --> 00:02:32.599 
will bring interactive machine learning

00:02:32.599 --> 00:02:35.290 
will bring interactive machine learning
even<00:02:32.810> on<00:02:33.080> very<00:02:33.739> large<00:02:33.980> data<00:02:34.280> sets<00:02:34.580> to<00:02:35.180> the

00:02:35.290 --> 00:02:35.300 
even on very large data sets to the

00:02:35.300 --> 00:02:37.900 
even on very large data sets to the
notebook<00:02:35.510> and<00:02:35.930> it<00:02:36.860> will<00:02:37.010> allow<00:02:37.220> operations<00:02:37.790> on

00:02:37.900 --> 00:02:37.910 
notebook and it will allow operations on

00:02:37.910 --> 00:02:39.640 
notebook and it will allow operations on
structured<00:02:38.330> objects<00:02:38.780> such<00:02:39.110> as<00:02:39.140> reagent

00:02:39.640 --> 00:02:39.650 
structured objects such as reagent

00:02:39.650 --> 00:02:42.160 
structured objects such as reagent
adjacency<00:02:40.099> graphs<00:02:40.310> and<00:02:40.580> so<00:02:40.700> on<00:02:40.880> it<00:02:41.870> will<00:02:42.049> have

00:02:42.160 --> 00:02:42.170 
adjacency graphs and so on it will have

00:02:42.170 --> 00:02:43.780 
adjacency graphs and so on it will have
a<00:02:42.290> generic<00:02:42.530> object<00:02:42.739> feature<00:02:43.160> library<00:02:43.580> and

00:02:43.780 --> 00:02:43.790 
a generic object feature library and

00:02:43.790 --> 00:02:47.559 
a generic object feature library and
many<00:02:44.450> more<00:02:45.549> for<00:02:46.549> more<00:02:46.730> detailed<00:02:47.030> demos<00:02:47.299> please

00:02:47.559 --> 00:02:47.569 
many more for more detailed demos please

00:02:47.569 --> 00:02:49.080 
many more for more detailed demos please
have<00:02:47.690> a<00:02:47.720> look<00:02:47.840> at<00:02:47.959> the<00:02:48.110> other<00:02:48.230> videos<00:02:48.590> and

00:02:49.080 --> 00:02:49.090 
have a look at the other videos and

00:02:49.090 --> 00:02:51.550 
have a look at the other videos and
explore<00:02:50.090> and<00:02:50.209> enjoy<00:02:50.420> yourself<00:02:50.870> thanks<00:02:51.440> for

00:02:51.550 --> 00:02:51.560 
explore and enjoy yourself thanks for

00:02:51.560 --> 00:02:53.620 
explore and enjoy yourself thanks for
listening

