WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.210 

in<00:00:00.120> this<00:00:00.240> video<00:00:00.450> we<00:00:00.989> show<00:00:01.020> how<00:00:01.589> we<00:00:01.650> use<00:00:01.979> the

00:00:02.210 --> 00:00:02.220 
in this video we show how we use the

00:00:02.220 --> 00:00:04.640 
in this video we show how we use the
Microsoft<00:00:02.909> Kinect<00:00:03.149> sensor<00:00:03.810> to<00:00:04.290> create

00:00:04.640 --> 00:00:04.650 
Microsoft Kinect sensor to create

00:00:04.650 --> 00:00:08.509 
Microsoft Kinect sensor to create
personalized<00:00:05.520> 3d<00:00:05.910> face<00:00:06.480> avatars<00:00:07.140> here<00:00:07.980> we<00:00:08.280> see

00:00:08.509 --> 00:00:08.519 
personalized 3d face avatars here we see

00:00:08.519 --> 00:00:11.450 
personalized 3d face avatars here we see
our<00:00:08.550> general<00:00:09.000> hardware<00:00:09.809> setup<00:00:10.080> the<00:00:10.710> user<00:00:10.920> sits

00:00:11.450 --> 00:00:11.460 
our general hardware setup the user sits

00:00:11.460 --> 00:00:13.129 
our general hardware setup the user sits
in<00:00:11.670> front<00:00:11.820> of<00:00:12.030> the<00:00:12.120> Kinect<00:00:12.480> camera<00:00:12.870> and

00:00:13.129 --> 00:00:13.139 
in front of the Kinect camera and

00:00:13.139 --> 00:00:15.829 
in front of the Kinect camera and
receives<00:00:13.889> a<00:00:14.099> real-time<00:00:14.759> feedback<00:00:15.120> of<00:00:15.599> his

00:00:15.829 --> 00:00:15.839 
receives a real-time feedback of his

00:00:15.839 --> 00:00:18.410 
receives a real-time feedback of his
captured<00:00:16.350> face<00:00:16.619> on<00:00:16.859> the<00:00:16.980> screen<00:00:17.420> augmented

00:00:18.410 --> 00:00:18.420 
captured face on the screen augmented

00:00:18.420 --> 00:00:21.260 
captured face on the screen augmented
with<00:00:18.660> face<00:00:18.990> feature<00:00:19.350> points<00:00:20.010> this<00:00:20.760> allows<00:00:21.060> him

00:00:21.260 --> 00:00:21.270 
with face feature points this allows him

00:00:21.270 --> 00:00:23.720 
with face feature points this allows him
to<00:00:21.330> adjust<00:00:21.570> his<00:00:22.140> position<00:00:22.680> before<00:00:23.039> the<00:00:23.519> data

00:00:23.720 --> 00:00:23.730 
to adjust his position before the data

00:00:23.730 --> 00:00:26.660 
to adjust his position before the data
is<00:00:23.939> captured<00:00:24.269> at<00:00:25.050> his<00:00:25.380> own<00:00:25.650> description<00:00:26.099> the

00:00:26.660 --> 00:00:26.670 
is captured at his own description the

00:00:26.670 --> 00:00:28.640 
is captured at his own description the
user<00:00:27.060> pushes<00:00:27.480> the<00:00:27.570> button<00:00:27.930> to<00:00:28.080> capture<00:00:28.529> the

00:00:28.640 --> 00:00:28.650 
user pushes the button to capture the

00:00:28.650 --> 00:00:31.429 
user pushes the button to capture the
current<00:00:29.010> data<00:00:29.300> after<00:00:30.300> approximately<00:00:30.990> 18

00:00:31.429 --> 00:00:31.439 
current data after approximately 18

00:00:31.439 --> 00:00:33.889 
current data after approximately 18
seconds<00:00:32.040> he<00:00:32.399> receives<00:00:32.820> the<00:00:32.969> reconstructed

00:00:33.889 --> 00:00:33.899 
seconds he receives the reconstructed

00:00:33.899 --> 00:00:36.590 
seconds he receives the reconstructed
and<00:00:34.079> textured<00:00:34.710> 3d<00:00:35.250> avatar<00:00:35.790> in<00:00:36.030> the<00:00:36.270> following

00:00:36.590 --> 00:00:36.600 
and textured 3d avatar in the following

00:00:36.600 --> 00:00:39.020 
and textured 3d avatar in the following
we<00:00:36.930> described<00:00:37.410> the<00:00:37.590> individual<00:00:38.160> steps<00:00:38.550> of<00:00:38.850> our

00:00:39.020 --> 00:00:39.030 
we described the individual steps of our

00:00:39.030 --> 00:00:41.660 
we described the individual steps of our
algorithm<00:00:39.780> Stratton<00:00:40.590> with<00:00:40.890> the<00:00:40.980> preparation

00:00:41.660 --> 00:00:41.670 
algorithm Stratton with the preparation

00:00:41.670 --> 00:00:44.959 
algorithm Stratton with the preparation
of<00:00:41.790> the<00:00:42.120> raw<00:00:42.300> camera<00:00:42.660> data<00:00:43.489> this<00:00:44.489> is<00:00:44.670> the<00:00:44.789> raw

00:00:44.959 --> 00:00:44.969 
of the raw camera data this is the raw

00:00:44.969 --> 00:00:47.750 
of the raw camera data this is the raw
depth<00:00:45.390> data<00:00:45.719> received<00:00:46.440> by<00:00:46.680> the<00:00:46.739> Kinect<00:00:47.219> sensor

00:00:47.750 --> 00:00:47.760 
depth data received by the Kinect sensor

00:00:47.760 --> 00:00:50.150 
depth data received by the Kinect sensor
we<00:00:48.510> improved<00:00:48.870> its<00:00:49.140> noisy<00:00:49.649> and<00:00:49.829> whole

00:00:50.150 --> 00:00:50.160 
we improved its noisy and whole

00:00:50.160 --> 00:00:53.660 
we improved its noisy and whole
containing<00:00:50.820> data<00:00:50.969> in<00:00:51.420> two<00:00:51.750> steps<00:00:52.190> first<00:00:53.190> we

00:00:53.660 --> 00:00:53.670 
containing data in two steps first we

00:00:53.670 --> 00:00:56.000 
containing data in two steps first we
take<00:00:53.879> the<00:00:54.059> average<00:00:54.180> depth<00:00:54.840> values<00:00:55.469> of<00:00:55.710> 8

00:00:56.000 --> 00:00:56.010 
take the average depth values of 8

00:00:56.010 --> 00:00:58.610 
take the average depth values of 8
subsequent<00:00:56.730> frames<00:00:56.969> we<00:00:57.930> call<00:00:58.140> the<00:00:58.320> step

00:00:58.610 --> 00:00:58.620 
subsequent frames we call the step

00:00:58.620 --> 00:01:01.880 
subsequent frames we call the step
temporal<00:00:59.579> smoothing<00:01:00.180> in<00:01:01.140> addition<00:01:01.649> we

00:01:01.880 --> 00:01:01.890 
temporal smoothing in addition we

00:01:01.890 --> 00:01:04.039 
temporal smoothing in addition we
perform<00:01:02.340> a<00:01:02.489> filtering<00:01:03.000> with<00:01:03.239> a<00:01:03.270> three<00:01:03.719> times

00:01:04.039 --> 00:01:04.049 
perform a filtering with a three times

00:01:04.049 --> 00:01:06.969 
perform a filtering with a three times
three<00:01:04.439> goals<00:01:04.739> filter

00:01:06.969 --> 00:01:06.979 
three goals filter

00:01:06.979 --> 00:01:10.060 
three goals filter
on<00:01:07.369> the<00:01:07.700> corresponding<00:01:08.479> RGB<00:01:08.719> image<00:01:09.439> we<00:01:09.860> use

00:01:10.060 --> 00:01:10.070 
on the corresponding RGB image we use

00:01:10.070 --> 00:01:13.209 
on the corresponding RGB image we use
OpenCV<00:01:11.060> to<00:01:11.360> detect<00:01:11.719> directions<00:01:12.439> containing

00:01:13.209 --> 00:01:13.219 
OpenCV to detect directions containing

00:01:13.219 --> 00:01:16.749 
OpenCV to detect directions containing
face<00:01:13.520> features<00:01:14.950> mapping<00:01:15.950> these<00:01:16.130> regions<00:01:16.640> to

00:01:16.749 --> 00:01:16.759 
face features mapping these regions to

00:01:16.759 --> 00:01:19.419 
face features mapping these regions to
the<00:01:16.939> geometry<00:01:17.299> assists<00:01:18.259> us<00:01:18.439> to<00:01:18.740> find<00:01:19.009> the<00:01:19.250> eyes

00:01:19.419 --> 00:01:19.429 
the geometry assists us to find the eyes

00:01:19.429 --> 00:01:22.929 
the geometry assists us to find the eyes
the<00:01:19.969> nose<00:01:20.149> tip<00:01:20.420> and<00:01:20.780> the<00:01:21.140> chin<00:01:21.439> in<00:01:22.280> order<00:01:22.789> to

00:01:22.929 --> 00:01:22.939 
the nose tip and the chin in order to

00:01:22.939 --> 00:01:25.179 
the nose tip and the chin in order to
reduce<00:01:23.090> the<00:01:23.509> costs<00:01:23.929> in<00:01:24.049> the<00:01:24.170> subsequent<00:01:24.649> steps

00:01:25.179 --> 00:01:25.189 
reduce the costs in the subsequent steps

00:01:25.189 --> 00:01:27.609 
reduce the costs in the subsequent steps
we<00:01:25.610> separate<00:01:26.119> the<00:01:26.299> face<00:01:26.569> from<00:01:27.020> the<00:01:27.140> rest<00:01:27.350> of

00:01:27.609 --> 00:01:27.619 
we separate the face from the rest of

00:01:27.619 --> 00:01:29.919 
we separate the face from the rest of
the<00:01:27.770> input<00:01:27.920> data<00:01:28.280> this<00:01:29.030> is<00:01:29.179> the<00:01:29.210> resulting

00:01:29.919 --> 00:01:29.929 
the input data this is the resulting

00:01:29.929 --> 00:01:32.350 
the input data this is the resulting
data<00:01:30.110> from<00:01:30.590> the<00:01:30.710> acquisition<00:01:31.100> part<00:01:31.700> which<00:01:32.060> the

00:01:32.350 --> 00:01:32.360 
data from the acquisition part which the

00:01:32.360 --> 00:01:34.950 
data from the acquisition part which the
user<00:01:32.569> receives<00:01:32.960> as<00:01:33.469> real-time<00:01:33.979> feedback

00:01:34.950 --> 00:01:34.960 
user receives as real-time feedback

00:01:34.960 --> 00:01:37.359 
user receives as real-time feedback
after<00:01:35.960> capturing<00:01:36.560> the<00:01:36.770> data<00:01:37.009> is

00:01:37.359 --> 00:01:37.369 
after capturing the data is

00:01:37.369 --> 00:01:39.789 
after capturing the data is
automatically<00:01:38.149> handed<00:01:38.810> to<00:01:39.020> the<00:01:39.110> next<00:01:39.439> stage

00:01:39.789 --> 00:01:39.799 
automatically handed to the next stage

00:01:39.799 --> 00:01:43.870 
automatically handed to the next stage
the<00:01:40.549> geometry<00:01:41.330> reconstruction<00:01:42.459> we<00:01:43.459> use<00:01:43.670> the

00:01:43.870 --> 00:01:43.880 
the geometry reconstruction we use the

00:01:43.880 --> 00:01:46.240 
the geometry reconstruction we use the
detected<00:01:44.420> feature<00:01:44.659> points<00:01:45.319> to<00:01:45.590> estimate<00:01:45.740> a

00:01:46.240 --> 00:01:46.250 
detected feature points to estimate a

00:01:46.250 --> 00:01:48.010 
detected feature points to estimate a
rough<00:01:46.459> initial<00:01:47.030> alignment<00:01:47.630> with<00:01:47.869> our

00:01:48.010 --> 00:01:48.020 
rough initial alignment with our

00:01:48.020 --> 00:01:52.080 
rough initial alignment with our
template<00:01:48.530> mesh<00:01:49.659> then<00:01:50.659> we<00:01:50.990> use<00:01:51.200> non-rigid

00:01:52.080 --> 00:01:52.090 
template mesh then we use non-rigid

00:01:52.090 --> 00:01:54.580 
template mesh then we use non-rigid
registration<00:01:53.090> to<00:01:53.360> deform<00:01:53.750> the<00:01:53.869> template<00:01:54.349> mesh

00:01:54.580 --> 00:01:54.590 
registration to deform the template mesh

00:01:54.590 --> 00:01:59.649 
registration to deform the template mesh
such<00:01:55.280> that<00:01:55.700> it<00:01:55.819> best<00:01:56.119> fits<00:01:56.479> to<00:01:56.810> input<00:01:57.140> beta

00:01:59.649 --> 00:01:59.659 

00:01:59.659 --> 00:02:02.289 

here<00:02:00.439> you<00:02:00.619> can<00:02:00.829> see<00:02:01.070> the<00:02:01.189> deformed<00:02:01.729> template

00:02:02.289 --> 00:02:02.299 
here you can see the deformed template

00:02:02.299 --> 00:02:04.840 
here you can see the deformed template
mesh<00:02:02.600> which<00:02:03.229> contains<00:02:03.710> all<00:02:03.920> the<00:02:04.249> dents<00:02:04.520> and

00:02:04.840 --> 00:02:04.850 
mesh which contains all the dents and

00:02:04.850 --> 00:02:06.999 
mesh which contains all the dents and
bumps<00:02:05.329> that<00:02:05.569> were<00:02:05.719> originally<00:02:06.380> present<00:02:06.590> in

00:02:06.999 --> 00:02:07.009 
bumps that were originally present in

00:02:07.009 --> 00:02:09.550 
bumps that were originally present in
the<00:02:07.159> scanner<00:02:07.460> data<00:02:07.810> to<00:02:08.810> eliminate<00:02:09.319> these

00:02:09.550 --> 00:02:09.560 
the scanner data to eliminate these

00:02:09.560 --> 00:02:12.490 
the scanner data to eliminate these
artifacts<00:02:10.369> we<00:02:10.670> fit<00:02:10.879> a<00:02:11.030> morphable<00:02:11.569> face<00:02:11.989> model

00:02:12.490 --> 00:02:12.500 
artifacts we fit a morphable face model

00:02:12.500 --> 00:02:18.230 
artifacts we fit a morphable face model
to<00:02:12.680> the<00:02:12.799> deformed<00:02:13.280> template<00:02:13.760> mesh

00:02:18.230 --> 00:02:18.240 

00:02:18.240 --> 00:02:21.020 

here<00:02:18.960> we<00:02:19.200> see<00:02:19.410> our<00:02:19.590> final<00:02:20.030> reconstruction

00:02:21.020 --> 00:02:21.030 
here we see our final reconstruction

00:02:21.030 --> 00:02:23.600 
here we see our final reconstruction
which<00:02:21.540> can<00:02:21.780> be<00:02:21.930> textured<00:02:22.320> using<00:02:23.070> the<00:02:23.160> RGB

00:02:23.600 --> 00:02:23.610 
which can be textured using the RGB

00:02:23.610 --> 00:02:27.510 
which can be textured using the RGB
image<00:02:23.790> captured<00:02:24.780> by<00:02:25.050> the<00:02:25.110> Kinect<00:02:25.590> sensor

00:02:27.510 --> 00:02:27.520 
image captured by the Kinect sensor

00:02:27.520 --> 00:02:30.540 
image captured by the Kinect sensor
this<00:02:28.240> overlay<00:02:28.870> demonstrates<00:02:29.740> how<00:02:30.040> well<00:02:30.340> our

00:02:30.540 --> 00:02:30.550 
this overlay demonstrates how well our

00:02:30.550 --> 00:02:33.680 
this overlay demonstrates how well our
reconstruction<00:02:31.260> matches<00:02:32.260> the<00:02:32.500> input<00:02:32.680> scan

00:02:33.680 --> 00:02:33.690 
reconstruction matches the input scan

00:02:33.690 --> 00:02:37.050 
reconstruction matches the input scan
finally<00:02:34.690> we<00:02:34.990> compare<00:02:35.440> a<00:02:35.650> reconstruction<00:02:36.430> to<00:02:37.000> a

00:02:37.050 --> 00:02:37.060 
finally we compare a reconstruction to a

00:02:37.060 --> 00:02:39.480 
finally we compare a reconstruction to a
scan<00:02:37.450> of<00:02:37.630> the<00:02:37.960> same<00:02:38.230> person<00:02:38.740> which<00:02:39.220> was

00:02:39.480 --> 00:02:39.490 
scan of the same person which was

00:02:39.490 --> 00:02:42.330 
scan of the same person which was
acquired<00:02:39.880> using<00:02:40.690> a<00:02:40.780> commercial<00:02:41.470> high<00:02:41.770> quality

00:02:42.330 --> 00:02:42.340 
acquired using a commercial high quality

00:02:42.340 --> 00:02:45.690 
acquired using a commercial high quality
3d<00:02:42.820> scanner<00:02:43.390> in<00:02:43.740> the<00:02:44.740> following<00:02:45.190> we<00:02:45.460> show

00:02:45.690 --> 00:02:45.700 
3d scanner in the following we show

00:02:45.700 --> 00:02:48.150 
3d scanner in the following we show
various<00:02:46.290> reconstructed<00:02:47.290> face<00:02:47.500> models<00:02:47.920> and

00:02:48.150 --> 00:02:48.160 
various reconstructed face models and

00:02:48.160 --> 00:02:52.270 
various reconstructed face models and
potential<00:02:49.000> applications

00:02:52.270 --> 00:02:52.280 

00:02:52.280 --> 00:02:55.030 

since<00:02:53.060> the<00:02:53.240> reconstructed<00:02:54.050> face<00:02:54.230> models<00:02:54.710> have

00:02:55.030 --> 00:02:55.040 
since the reconstructed face models have

00:02:55.040 --> 00:02:57.160 
since the reconstructed face models have
one-to-one<00:02:55.610> correspondence<00:02:56.120> with<00:02:56.960> our

00:02:57.160 --> 00:02:57.170 
one-to-one correspondence with our

00:02:57.170 --> 00:02:59.560 
one-to-one correspondence with our
template<00:02:57.680> face<00:02:58.010> we<00:02:58.550> are<00:02:58.670> easily<00:02:58.940> able<00:02:59.330> to

00:02:59.560 --> 00:02:59.570 
template face we are easily able to

00:02:59.570 --> 00:03:06.309 
template face we are easily able to
animate<00:03:00.320> two<00:03:00.530> faces

00:03:06.309 --> 00:03:06.319 

00:03:06.319 --> 00:03:08.890 

here<00:03:06.950> we'll<00:03:07.189> show<00:03:07.219> how<00:03:07.730> we<00:03:07.790> can<00:03:07.909> morph<00:03:08.420> between

00:03:08.890 --> 00:03:08.900 
here we'll show how we can morph between

00:03:08.900 --> 00:03:11.110 
here we'll show how we can morph between
different<00:03:09.319> faces<00:03:09.769> which<00:03:10.370> were<00:03:10.400> captured

00:03:11.110 --> 00:03:11.120 
different faces which were captured

00:03:11.120 --> 00:03:19.230 
different faces which were captured
using<00:03:11.689> our<00:03:11.870> method

00:03:19.230 --> 00:03:19.240 

00:03:19.240 --> 00:03:21.300 

you

