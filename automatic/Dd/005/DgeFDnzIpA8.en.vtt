WEBVTT
Kind: captions
Language: en

00:00:05.480 --> 00:00:07.880 

hello<00:00:06.480> my<00:00:06.720> name<00:00:06.900> is<00:00:06.930> Vicki<00:00:07.170> Ratcliffe<00:00:07.620> and

00:00:07.880 --> 00:00:07.890 
hello my name is Vicki Ratcliffe and

00:00:07.890 --> 00:00:09.860 
hello my name is Vicki Ratcliffe and
together<00:00:08.190> with<00:00:08.340> my<00:00:08.520> supervisor<00:00:09.030> David<00:00:09.360> rebe

00:00:09.860 --> 00:00:09.870 
together with my supervisor David rebe

00:00:09.870 --> 00:00:11.270 
together with my supervisor David rebe
we've<00:00:10.139> been<00:00:10.350> investigating<00:00:10.799> whether

00:00:11.270 --> 00:00:11.280 
we've been investigating whether

00:00:11.280 --> 00:00:13.820 
we've been investigating whether
domestic<00:00:11.670> dogs<00:00:12.150> show<00:00:12.510> hemispheric<00:00:13.110> biases<00:00:13.590> in

00:00:13.820 --> 00:00:13.830 
domestic dogs show hemispheric biases in

00:00:13.830 --> 00:00:15.440 
domestic dogs show hemispheric biases in
responses<00:00:14.309> the<00:00:14.429> different<00:00:14.820> components<00:00:15.360> in

00:00:15.440 --> 00:00:15.450 
responses the different components in

00:00:15.450 --> 00:00:18.109 
responses the different components in
human<00:00:15.780> speech<00:00:16.250> speech<00:00:17.250> is<00:00:17.400> a<00:00:17.430> complex<00:00:17.820> signal

00:00:18.109 --> 00:00:18.119 
human speech speech is a complex signal

00:00:18.119 --> 00:00:20.060 
human speech speech is a complex signal
and<00:00:18.509> when<00:00:18.630> we<00:00:18.750> speak<00:00:19.020> we<00:00:19.259> transmit<00:00:19.710> not<00:00:19.860> only

00:00:20.060 --> 00:00:20.070 
and when we speak we transmit not only

00:00:20.070 --> 00:00:21.979 
and when we speak we transmit not only
verbal<00:00:20.430> information<00:00:21.029> but<00:00:21.509> also<00:00:21.660> information

00:00:21.979 --> 00:00:21.989 
verbal information but also information

00:00:21.989 --> 00:00:24.679 
verbal information but also information
about<00:00:22.349> ourselves<00:00:22.680> such<00:00:23.489> as<00:00:23.640> I<00:00:23.759> gender<00:00:24.329> or

00:00:24.679 --> 00:00:24.689 
about ourselves such as I gender or

00:00:24.689 --> 00:00:27.229 
about ourselves such as I gender or
identity<00:00:25.380> or<00:00:25.649> personality<00:00:26.369> and<00:00:26.820> even<00:00:27.119> our

00:00:27.229 --> 00:00:27.239 
identity or personality and even our

00:00:27.239 --> 00:00:29.690 
identity or personality and even our
emotional<00:00:27.720> state<00:00:28.070> when<00:00:29.070> we<00:00:29.160> hear<00:00:29.369> speech

00:00:29.690 --> 00:00:29.700 
emotional state when we hear speech

00:00:29.700 --> 00:00:32.240 
emotional state when we hear speech
meaningful<00:00:30.509> verbal<00:00:30.930> information<00:00:31.020> encoded<00:00:31.980> in

00:00:32.240 --> 00:00:32.250 
meaningful verbal information encoded in

00:00:32.250 --> 00:00:33.680 
meaningful verbal information encoded in
the<00:00:32.369> phonemic<00:00:32.640> variation<00:00:33.329> of<00:00:33.420> the<00:00:33.510> signal

00:00:33.680 --> 00:00:33.690 
the phonemic variation of the signal

00:00:33.690 --> 00:00:35.780 
the phonemic variation of the signal
this<00:00:34.380> process<00:00:34.829> primarily<00:00:35.309> in<00:00:35.489> the<00:00:35.579> left

00:00:35.780 --> 00:00:35.790 
this process primarily in the left

00:00:35.790 --> 00:00:37.549 
this process primarily in the left
hemisphere<00:00:36.270> of<00:00:36.300> the<00:00:36.480> brain<00:00:36.510> in<00:00:36.929> most<00:00:37.140> people

00:00:37.549 --> 00:00:37.559 
hemisphere of the brain in most people

00:00:37.559 --> 00:00:40.250 
hemisphere of the brain in most people
in<00:00:38.370> contrast<00:00:38.940> to<00:00:39.149> speak<00:00:39.390> related<00:00:39.870> information

00:00:40.250 --> 00:00:40.260 
in contrast to speak related information

00:00:40.260 --> 00:00:42.950 
in contrast to speak related information
such<00:00:40.739> as<00:00:40.769> the<00:00:41.300> emotional<00:00:42.300> prosody<00:00:42.660> is

00:00:42.950 --> 00:00:42.960 
such as the emotional prosody is

00:00:42.960 --> 00:00:44.479 
such as the emotional prosody is
processed<00:00:43.559> primarily<00:00:43.980> in<00:00:44.190> the<00:00:44.309> right

00:00:44.479 --> 00:00:44.489 
processed primarily in the right

00:00:44.489 --> 00:00:46.759 
processed primarily in the right
hemisphere<00:00:44.809> but<00:00:45.809> what<00:00:45.960> happens<00:00:46.260> when<00:00:46.410> animals

00:00:46.759 --> 00:00:46.769 
hemisphere but what happens when animals

00:00:46.769 --> 00:00:48.860 
hemisphere but what happens when animals
hear<00:00:46.949> speech<00:00:47.280> but<00:00:48.089> we<00:00:48.210> know<00:00:48.359> that<00:00:48.390> animals

00:00:48.860 --> 00:00:48.870 
hear speech but we know that animals

00:00:48.870 --> 00:00:50.689 
hear speech but we know that animals
also<00:00:49.289> share<00:00:49.469> hemispheric<00:00:49.949> biases<00:00:50.429> in

00:00:50.689 --> 00:00:50.699 
also share hemispheric biases in

00:00:50.699 --> 00:00:51.880 
also share hemispheric biases in
response<00:00:51.059> to<00:00:51.210> their<00:00:51.359> own<00:00:51.480> species

00:00:51.880 --> 00:00:51.890 
response to their own species

00:00:51.890 --> 00:00:54.410 
response to their own species
vocalizations<00:00:52.890> but<00:00:53.579> what<00:00:53.699> we<00:00:53.820> didn't<00:00:54.120> know<00:00:54.269> it

00:00:54.410 --> 00:00:54.420 
vocalizations but what we didn't know it

00:00:54.420 --> 00:00:55.700 
vocalizations but what we didn't know it
was<00:00:54.539> where<00:00:54.719> the<00:00:54.809> species<00:00:55.260> which<00:00:55.410> have<00:00:55.559> been

00:00:55.700 --> 00:00:55.710 
was where the species which have been

00:00:55.710 --> 00:00:57.560 
was where the species which have been
strongly<00:00:56.100> exposed<00:00:56.609> to<00:00:56.760> speech<00:00:57.030> such<00:00:57.300> as<00:00:57.449> the

00:00:57.560 --> 00:00:57.570 
strongly exposed to speech such as the

00:00:57.570 --> 00:00:59.689 
strongly exposed to speech such as the
domestic<00:00:58.050> dog<00:00:58.230> would<00:00:58.920> also<00:00:59.070> show<00:00:59.399> him<00:00:59.579> as

00:00:59.689 --> 00:00:59.699 
domestic dog would also show him as

00:00:59.699 --> 00:01:01.099 
domestic dog would also show him as
throat<00:00:59.909> biases<00:01:00.359> in<00:01:00.570> response<00:01:00.929> to<00:01:01.019> the

00:01:01.099 --> 00:01:01.109 
throat biases in response to the

00:01:01.109 --> 00:01:02.509 
throat biases in response to the
different<00:01:01.469> components<00:01:02.010> in<00:01:02.159> the<00:01:02.280> speech

00:01:02.509 --> 00:01:02.519 
different components in the speech

00:01:02.519 --> 00:01:05.060 
different components in the speech
signal<00:01:02.730> in<00:01:03.600> order<00:01:03.870> to<00:01:04.019> test<00:01:04.260> this<00:01:04.470> we<00:01:04.769> used<00:01:04.949> a

00:01:05.060 --> 00:01:05.070 
signal in order to test this we used a

00:01:05.070 --> 00:01:06.440 
signal in order to test this we used a
head<00:01:05.220> orienting<00:01:05.760> design<00:01:06.090> which<00:01:06.300> is

00:01:06.440 --> 00:01:06.450 
head orienting design which is

00:01:06.450 --> 00:01:08.990 
head orienting design which is
illustrated<00:01:06.570> here<00:01:07.230> in<00:01:07.500> this<00:01:08.340> design<00:01:08.640> we<00:01:08.850> have

00:01:08.990 --> 00:01:09.000 
illustrated here in this design we have

00:01:09.000 --> 00:01:10.609 
illustrated here in this design we have
two<00:01:09.270> speakers<00:01:09.720> which<00:01:09.960> are<00:01:10.079> placed<00:01:10.320> either

00:01:10.609 --> 00:01:10.619 
two speakers which are placed either

00:01:10.619 --> 00:01:12.590 
two speakers which are placed either
side<00:01:10.860> of<00:01:10.890> the<00:01:11.130> dog<00:01:11.340> and<00:01:11.520> the<00:01:12.000> sound<00:01:12.240> is<00:01:12.390> played

00:01:12.590 --> 00:01:12.600 
side of the dog and the sound is played

00:01:12.600 --> 00:01:14.569 
side of the dog and the sound is played
from<00:01:12.810> both<00:01:13.020> speakers<00:01:13.470> at<00:01:13.649> the<00:01:13.770> same<00:01:13.979> time<00:01:14.280> so

00:01:14.569 --> 00:01:14.579 
from both speakers at the same time so

00:01:14.579 --> 00:01:15.980 
from both speakers at the same time so
that<00:01:14.610> it<00:01:14.790> enters<00:01:15.090> both<00:01:15.299> the<00:01:15.540> dog's<00:01:15.720> ear<00:01:15.869> is

00:01:15.980 --> 00:01:15.990 
that it enters both the dog's ear is

00:01:15.990 --> 00:01:18.260 
that it enters both the dog's ear is
equally<00:01:16.409> the<00:01:17.280> sound<00:01:17.520> input<00:01:17.759> is<00:01:18.060> then

00:01:18.260 --> 00:01:18.270 
equally the sound input is then

00:01:18.270 --> 00:01:20.209 
equally the sound input is then
transferred<00:01:18.780> from<00:01:18.930> each<00:01:19.170> ear<00:01:19.470> across<00:01:19.920> to<00:01:20.130> the

00:01:20.209 --> 00:01:20.219 
transferred from each ear across to the

00:01:20.219 --> 00:01:22.190 
transferred from each ear across to the
opposite<00:01:20.579> hemisphere<00:01:20.700> of<00:01:21.090> the<00:01:21.299> brain<00:01:21.329> and<00:01:21.810> the

00:01:22.190 --> 00:01:22.200 
opposite hemisphere of the brain and the

00:01:22.200 --> 00:01:23.990 
opposite hemisphere of the brain and the
hemisphere<00:01:22.680> which<00:01:22.799> is<00:01:22.920> more<00:01:23.130> specialized<00:01:23.729> in

00:01:23.990 --> 00:01:24.000 
hemisphere which is more specialized in

00:01:24.000 --> 00:01:25.730 
hemisphere which is more specialized in
processing<00:01:24.570> this<00:01:24.659> content<00:01:25.140> works<00:01:25.469> more

00:01:25.730 --> 00:01:25.740 
processing this content works more

00:01:25.740 --> 00:01:27.620 
processing this content works more
efficiently<00:01:25.950> so<00:01:26.430> the<00:01:26.850> sound<00:01:27.090> is<00:01:27.270> heard<00:01:27.390> more

00:01:27.620 --> 00:01:27.630 
efficiently so the sound is heard more

00:01:27.630 --> 00:01:30.740 
efficiently so the sound is heard more
clearly<00:01:27.869> from<00:01:28.170> the<00:01:28.320> opposite<00:01:28.710> ear<00:01:29.329> so<00:01:30.329> if<00:01:30.630> the

00:01:30.740 --> 00:01:30.750 
clearly from the opposite ear so if the

00:01:30.750 --> 00:01:32.510 
clearly from the opposite ear so if the
dog<00:01:30.930> turns<00:01:31.229> to<00:01:31.409> their<00:01:31.560> left<00:01:31.829> in<00:01:32.040> response<00:01:32.399> to

00:01:32.510 --> 00:01:32.520 
dog turns to their left in response to

00:01:32.520 --> 00:01:34.490 
dog turns to their left in response to
the<00:01:32.610> sound<00:01:32.820> this<00:01:33.450> provides<00:01:33.869> a<00:01:34.049> behavioral

00:01:34.490 --> 00:01:34.500 
the sound this provides a behavioral

00:01:34.500 --> 00:01:36.350 
the sound this provides a behavioral
indication<00:01:35.100> that<00:01:35.280> the<00:01:35.399> right<00:01:35.610> hemisphere<00:01:36.180> is

00:01:36.350 --> 00:01:36.360 
indication that the right hemisphere is

00:01:36.360 --> 00:01:38.389 
indication that the right hemisphere is
more<00:01:36.630> specialized<00:01:37.200> in<00:01:37.380> processing<00:01:38.009> the

00:01:38.389 --> 00:01:38.399 
more specialized in processing the

00:01:38.399 --> 00:01:39.709 
more specialized in processing the
information<00:01:38.520> in<00:01:39.030> the<00:01:39.090> sound<00:01:39.329> that<00:01:39.450> they<00:01:39.630> are

00:01:39.709 --> 00:01:39.719 
information in the sound that they are

00:01:39.719 --> 00:01:42.590 
information in the sound that they are
responding<00:01:40.079> to<00:01:40.520> in<00:01:41.520> our<00:01:41.729> study<00:01:42.090> each<00:01:42.329> dog

00:01:42.590 --> 00:01:42.600 
responding to in our study each dog

00:01:42.600 --> 00:01:44.660 
responding to in our study each dog
heard<00:01:42.840> only<00:01:42.990> one<00:01:43.289> sound<00:01:43.619> and<00:01:43.920> we<00:01:44.340> look<00:01:44.490> for

00:01:44.660 --> 00:01:44.670 
heard only one sound and we look for

00:01:44.670 --> 00:01:46.700 
heard only one sound and we look for
consistencies<00:01:45.450> across<00:01:45.990> all<00:01:46.259> of<00:01:46.380> the<00:01:46.469> dogs

00:01:46.700 --> 00:01:46.710 
consistencies across all of the dogs

00:01:46.710 --> 00:01:48.560 
consistencies across all of the dogs
exposed<00:01:47.189> to<00:01:47.310> the<00:01:47.399> same<00:01:47.640> type<00:01:47.880> of<00:01:47.909> sound<00:01:48.270> in

00:01:48.560 --> 00:01:48.570 
exposed to the same type of sound in

00:01:48.570 --> 00:01:51.380 
exposed to the same type of sound in
their<00:01:48.869> head<00:01:49.020> turning<00:01:49.289> responses<00:01:49.979> we<00:01:50.909> had<00:01:51.119> ten

00:01:51.380 --> 00:01:51.390 
their head turning responses we had ten

00:01:51.390 --> 00:01:53.120 
their head turning responses we had ten
different<00:01:51.780> sound<00:01:51.990> conditions<00:01:52.500> in<00:01:52.680> total

00:01:53.120 --> 00:01:53.130 
different sound conditions in total

00:01:53.130 --> 00:01:55.039 
different sound conditions in total
eight<00:01:53.520> of<00:01:53.789> these<00:01:53.969> were<00:01:54.210> related<00:01:54.630> to<00:01:54.719> human

00:01:55.039 --> 00:01:55.049 
eight of these were related to human

00:01:55.049 --> 00:01:57.200 
eight of these were related to human
speech<00:01:55.320> and<00:01:55.649> two<00:01:55.950> were<00:01:56.189> nonverbal<00:01:56.549> controls

00:01:57.200 --> 00:01:57.210 
speech and two were nonverbal controls

00:01:57.210 --> 00:01:59.810 
speech and two were nonverbal controls
in<00:01:57.840> our<00:01:58.049> speech<00:01:58.350> conditions<00:01:59.009> we<00:01:59.219> manipulated

00:01:59.810 --> 00:01:59.820 
in our speech conditions we manipulated

00:01:59.820 --> 00:02:01.789 
in our speech conditions we manipulated
the<00:02:00.030> signals<00:02:00.509> by<00:02:00.750> either<00:02:00.929> enhancing<00:02:01.560> or

00:02:01.789 --> 00:02:01.799 
the signals by either enhancing or

00:02:01.799 --> 00:02:03.950 
the signals by either enhancing or
removing<00:02:02.340> certain<00:02:02.670> information<00:02:02.909> to<00:02:03.719> see<00:02:03.899> if

00:02:03.950 --> 00:02:03.960 
removing certain information to see if

00:02:03.960 --> 00:02:05.419 
removing certain information to see if
this<00:02:04.109> influenced<00:02:04.740> the<00:02:04.829> direction<00:02:04.979> of<00:02:05.310> the

00:02:05.419 --> 00:02:05.429 
this influenced the direction of the

00:02:05.429 --> 00:02:08.240 
this influenced the direction of the
dog's<00:02:05.640> head<00:02:05.880> turning<00:02:06.119> responses<00:02:06.979> we<00:02:07.979> first

00:02:08.240 --> 00:02:08.250 
dog's head turning responses we first

00:02:08.250 --> 00:02:09.620 
dog's head turning responses we first
tested<00:02:08.759> whether<00:02:08.940> dogs<00:02:09.210> would<00:02:09.450> show

00:02:09.620 --> 00:02:09.630 
tested whether dogs would show

00:02:09.630 --> 00:02:12.230 
tested whether dogs would show
lateralized<00:02:10.050> response<00:02:10.590> wires<00:02:10.890> to<00:02:11.670> meaningful

00:02:12.230 --> 00:02:12.240 
lateralized response wires to meaningful

00:02:12.240 --> 00:02:13.559 
lateralized response wires to meaningful
verbal<00:02:12.660> information

00:02:13.559 --> 00:02:13.569 
verbal information

00:02:13.569 --> 00:02:15.179 
verbal information
coded<00:02:13.930> at<00:02:14.019> the<00:02:14.109> segmental<00:02:14.680> level<00:02:14.980> of<00:02:15.099> the

00:02:15.179 --> 00:02:15.189 
coded at the segmental level of the

00:02:15.189 --> 00:02:18.030 
coded at the segmental level of the
speech<00:02:15.430> signal<00:02:15.870> to<00:02:16.870> do<00:02:17.019> this<00:02:17.230> we<00:02:17.469> started<00:02:17.889> with

00:02:18.030 --> 00:02:18.040 
speech signal to do this we started with

00:02:18.040 --> 00:02:20.190 
speech signal to do this we started with
a<00:02:18.069> familiar<00:02:18.430> learn<00:02:18.790> command<00:02:19.150> and<00:02:19.569> we<00:02:19.689> enhanced

00:02:20.190 --> 00:02:20.200 
a familiar learn command and we enhanced

00:02:20.200 --> 00:02:22.410 
a familiar learn command and we enhanced
the<00:02:20.379> salience<00:02:20.799> of<00:02:20.980> the<00:02:21.040> verbal<00:02:21.400> content<00:02:21.909> by

00:02:22.410 --> 00:02:22.420 
the salience of the verbal content by

00:02:22.420 --> 00:02:24.660 
the salience of the verbal content by
either<00:02:22.719> removing<00:02:23.709> the<00:02:23.829> speaker's<00:02:24.219> intonation

00:02:24.660 --> 00:02:24.670 
either removing the speaker's intonation

00:02:24.670 --> 00:02:27.119 
either removing the speaker's intonation
which<00:02:25.389> reduces<00:02:25.959> the<00:02:26.139> emotional<00:02:26.620> content<00:02:26.980> in

00:02:27.119 --> 00:02:27.129 
which reduces the emotional content in

00:02:27.129 --> 00:02:29.190 
which reduces the emotional content in
the<00:02:27.219> signal<00:02:27.599> come<00:02:28.599> on<00:02:28.719> then

00:02:29.190 --> 00:02:29.200 
the signal come on then

00:02:29.200 --> 00:02:32.099 
the signal come on then
or<00:02:30.129> we<00:02:30.430> converted<00:02:30.849> the<00:02:31.060> sound<00:02:31.299> into<00:02:31.689> sine<00:02:31.900> wave

00:02:32.099 --> 00:02:32.109 
or we converted the sound into sine wave

00:02:32.109 --> 00:02:34.470 
or we converted the sound into sine wave
speech<00:02:32.439> what<00:02:33.370> this<00:02:33.519> does<00:02:33.760> is<00:02:33.969> replace<00:02:34.329> the

00:02:34.470 --> 00:02:34.480 
speech what this does is replace the

00:02:34.480 --> 00:02:36.059 
speech what this does is replace the
speaker's<00:02:34.870> performance<00:02:35.379> with<00:02:35.620> sine<00:02:35.859> wave

00:02:36.059 --> 00:02:36.069 
speaker's performance with sine wave

00:02:36.069 --> 00:02:37.920 
speaker's performance with sine wave
tones<00:02:36.400> and<00:02:36.939> removes<00:02:37.329> all<00:02:37.569> of<00:02:37.719> the<00:02:37.810> other

00:02:37.920 --> 00:02:37.930 
tones and removes all of the other

00:02:37.930 --> 00:02:39.959 
tones and removes all of the other
information<00:02:38.139> from<00:02:38.680> the<00:02:38.769> signal<00:02:39.250> so<00:02:39.939> that

00:02:39.959 --> 00:02:39.969 
information from the signal so that

00:02:39.969 --> 00:02:42.059 
information from the signal so that
whilst<00:02:40.269> the<00:02:40.510> meaningful<00:02:41.019> verbal<00:02:41.469> content<00:02:41.919> is

00:02:42.059 --> 00:02:42.069 
whilst the meaningful verbal content is

00:02:42.069 --> 00:02:44.159 
whilst the meaningful verbal content is
retained<00:02:42.519> all<00:02:43.209> of<00:02:43.239> the<00:02:43.450> speaker<00:02:43.719> related

00:02:44.159 --> 00:02:44.169 
retained all of the speaker related

00:02:44.169 --> 00:02:46.050 
retained all of the speaker related
information<00:02:44.499> is<00:02:44.859> removed<00:02:45.310> and<00:02:45.609> the<00:02:45.849> speech

00:02:46.050 --> 00:02:46.060 
information is removed and the speech

00:02:46.060 --> 00:02:50.429 
information is removed and the speech
does<00:02:46.329> not<00:02:46.359> sound<00:02:46.629> voiced<00:02:46.959> like<00:02:49.079> we<00:02:50.079> found<00:02:50.290> the

00:02:50.429 --> 00:02:50.439 
does not sound voiced like we found the

00:02:50.439 --> 00:02:51.809 
does not sound voiced like we found the
dog<00:02:50.680> showed<00:02:50.919> a<00:02:50.950> significant<00:02:51.579> left

00:02:51.809 --> 00:02:51.819 
dog showed a significant left

00:02:51.819 --> 00:02:53.759 
dog showed a significant left
hemispheric<00:02:52.389> bias<00:02:52.750> in<00:02:53.079> response<00:02:53.439> to<00:02:53.680> these

00:02:53.759 --> 00:02:53.769 
hemispheric bias in response to these

00:02:53.769 --> 00:02:55.860 
hemispheric bias in response to these
signals<00:02:54.040> even<00:02:54.669> when<00:02:54.969> the<00:02:55.060> speaker's<00:02:55.449> accent

00:02:55.860 --> 00:02:55.870 
signals even when the speaker's accent

00:02:55.870 --> 00:02:58.789 
signals even when the speaker's accent
was<00:02:56.049> strongly<00:02:56.319> unfamiliar<00:02:57.030> commands<00:02:58.030> in

00:02:58.789 --> 00:02:58.799 
was strongly unfamiliar commands in

00:02:58.799 --> 00:03:02.059 
was strongly unfamiliar commands in
however<00:02:59.799> if<00:03:00.340> we<00:03:00.819> increase<00:03:01.209> the<00:03:01.389> salience<00:03:01.870> of

00:03:02.059 --> 00:03:02.069 
however if we increase the salience of

00:03:02.069 --> 00:03:04.470 
however if we increase the salience of
verbal<00:03:03.069> content<00:03:03.400> which<00:03:03.609> was<00:03:03.790> not<00:03:03.969> meaningful

00:03:04.470 --> 00:03:04.480 
verbal content which was not meaningful

00:03:04.480 --> 00:03:06.959 
verbal content which was not meaningful
to<00:03:04.599> the<00:03:04.689> dogs<00:03:04.930> either<00:03:05.680> by<00:03:06.069> reordering<00:03:06.579> the

00:03:06.959 --> 00:03:06.969 
to the dogs either by reordering the

00:03:06.969 --> 00:03:09.539 
to the dogs either by reordering the
phonemes<00:03:07.419> in<00:03:07.629> the<00:03:07.750> original<00:03:08.199> command<00:03:08.530> for<00:03:09.310> mom

00:03:09.539 --> 00:03:09.549 
phonemes in the original command for mom

00:03:09.549 --> 00:03:13.860 
phonemes in the original command for mom
Ken<00:03:10.079> or<00:03:11.079> by<00:03:11.560> using<00:03:11.859> foreign<00:03:12.699> speech<00:03:13.030> and

00:03:13.860 --> 00:03:13.870 
Ken or by using foreign speech and

00:03:13.870 --> 00:03:16.649 
Ken or by using foreign speech and
iteration<00:03:14.819> we<00:03:15.819> found<00:03:16.090> that<00:03:16.180> they<00:03:16.299> instead

00:03:16.649 --> 00:03:16.659 
iteration we found that they instead

00:03:16.659 --> 00:03:18.360 
iteration we found that they instead
showed<00:03:16.900> a<00:03:17.049> significant<00:03:17.560> right<00:03:17.799> hemispheric

00:03:18.360 --> 00:03:18.370 
showed a significant right hemispheric

00:03:18.370 --> 00:03:20.819 
showed a significant right hemispheric
bias<00:03:18.639> which<00:03:19.359> suggests<00:03:19.540> that<00:03:20.019> verbal<00:03:20.409> content

00:03:20.819 --> 00:03:20.829 
bias which suggests that verbal content

00:03:20.829 --> 00:03:22.530 
bias which suggests that verbal content
must<00:03:21.009> not<00:03:21.189> only<00:03:21.280> be<00:03:21.519> present<00:03:21.970> but<00:03:22.359> also

00:03:22.530 --> 00:03:22.540 
must not only be present but also

00:03:22.540 --> 00:03:24.809 
must not only be present but also
meaningful<00:03:23.079> to<00:03:23.379> the<00:03:23.439> dogs<00:03:23.680> to<00:03:24.459> elicit

00:03:24.809 --> 00:03:24.819 
meaningful to the dogs to elicit

00:03:24.819 --> 00:03:27.080 
meaningful to the dogs to elicit
stronger<00:03:25.269> left<00:03:25.599> hemispheric<00:03:26.109> activation

00:03:27.080 --> 00:03:27.090 
stronger left hemispheric activation

00:03:27.090 --> 00:03:29.610 
stronger left hemispheric activation
coupled<00:03:28.090> with<00:03:28.150> a<00:03:28.389> finding<00:03:28.810> that<00:03:28.930> dogs<00:03:29.139> also

00:03:29.610 --> 00:03:29.620 
coupled with a finding that dogs also

00:03:29.620 --> 00:03:31.379 
coupled with a finding that dogs also
showed<00:03:29.829> a<00:03:29.859> significant<00:03:30.579> right<00:03:30.819> hemispheric

00:03:31.379 --> 00:03:31.389 
showed a significant right hemispheric

00:03:31.389 --> 00:03:33.749 
showed a significant right hemispheric
bias<00:03:31.629> in<00:03:31.989> response<00:03:32.349> to<00:03:32.620> positive<00:03:33.159> emotional

00:03:33.749 --> 00:03:33.759 
bias in response to positive emotional

00:03:33.759 --> 00:03:35.580 
bias in response to positive emotional
speech<00:03:34.000> in<00:03:34.299> which<00:03:34.479> the<00:03:34.659> verbal<00:03:34.989> content<00:03:35.409> had

00:03:35.580 --> 00:03:35.590 
speech in which the verbal content had

00:03:35.590 --> 00:03:38.729 
speech in which the verbal content had
been<00:03:35.739> removed<00:03:36.989> this<00:03:37.989> suggests<00:03:38.139> that<00:03:38.500> the

00:03:38.729 --> 00:03:38.739 
been removed this suggests that the

00:03:38.739 --> 00:03:40.140 
been removed this suggests that the
right<00:03:38.949> hemisphere<00:03:39.489> of<00:03:39.519> the<00:03:39.729> brain<00:03:39.759> is

00:03:40.140 --> 00:03:40.150 
right hemisphere of the brain is

00:03:40.150 --> 00:03:41.999 
right hemisphere of the brain is
specialized<00:03:40.689> in<00:03:40.840> processing<00:03:41.470> the<00:03:41.560> speaker

00:03:41.999 --> 00:03:42.009 
specialized in processing the speaker

00:03:42.009 --> 00:03:44.849 
specialized in processing the speaker
related<00:03:42.400> information<00:03:42.819> in<00:03:43.299> the<00:03:43.540> voice<00:03:43.859> to

00:03:44.849 --> 00:03:44.859 
related information in the voice to

00:03:44.859 --> 00:03:46.890 
related information in the voice to
summarize<00:03:45.280> we<00:03:45.639> found<00:03:45.939> the<00:03:46.120> dog<00:03:46.299> show<00:03:46.569> a<00:03:46.599> left

00:03:46.890 --> 00:03:46.900 
summarize we found the dog show a left

00:03:46.900 --> 00:03:48.929 
summarize we found the dog show a left
hemispheric<00:03:47.470> bias<00:03:47.769> when<00:03:48.159> the<00:03:48.280> salience<00:03:48.759> of

00:03:48.929 --> 00:03:48.939 
hemispheric bias when the salience of

00:03:48.939 --> 00:03:51.210 
hemispheric bias when the salience of
meaningful<00:03:49.479> verbal<00:03:49.900> content<00:03:50.349> is<00:03:50.620> increased

00:03:51.210 --> 00:03:51.220 
meaningful verbal content is increased

00:03:51.220 --> 00:03:54.179 
meaningful verbal content is increased
in<00:03:51.489> the<00:03:51.609> speech<00:03:51.849> signal<00:03:52.259> in<00:03:53.259> contrast<00:03:53.829> they

00:03:54.179 --> 00:03:54.189 
in the speech signal in contrast they

00:03:54.189 --> 00:03:56.610 
in the speech signal in contrast they
show<00:03:54.370> a<00:03:54.400> right<00:03:54.729> hemispheric<00:03:55.329> bias<00:03:55.599> when<00:03:56.409> the

00:03:56.610 --> 00:03:56.620 
show a right hemispheric bias when the

00:03:56.620 --> 00:03:58.080 
show a right hemispheric bias when the
verbal<00:03:56.949> content<00:03:57.430> is<00:03:57.609> either<00:03:57.849> learn

00:03:58.080 --> 00:03:58.090 
verbal content is either learn

00:03:58.090 --> 00:03:59.640 
verbal content is either learn
meaningless<00:03:58.569> to<00:03:58.780> them<00:03:58.930> or<00:03:59.169> removed

00:03:59.640 --> 00:03:59.650 
meaningless to them or removed

00:03:59.650 --> 00:04:01.469 
meaningless to them or removed
altogether<00:03:59.979> from<00:04:00.519> the<00:04:00.639> signal<00:04:01.030> thus

00:04:01.469 --> 00:04:01.479 
altogether from the signal thus

00:04:01.479 --> 00:04:03.149 
altogether from the signal thus
enhancing<00:04:02.049> the<00:04:02.139> salience<00:04:02.500> of<00:04:02.590> the<00:04:02.889> speaker

00:04:03.149 --> 00:04:03.159 
enhancing the salience of the speaker

00:04:03.159 --> 00:04:05.849 
enhancing the salience of the speaker
related<00:04:03.609> information<00:04:04.259> this<00:04:05.259> is<00:04:05.439> particularly

00:04:05.849 --> 00:04:05.859 
related information this is particularly

00:04:05.859 --> 00:04:07.800 
related information this is particularly
interesting<00:04:06.099> because<00:04:06.609> it<00:04:07.000> demonstrates<00:04:07.569> that

00:04:07.800 --> 00:04:07.810 
interesting because it demonstrates that

00:04:07.810 --> 00:04:09.509 
interesting because it demonstrates that
dogs<00:04:08.049> separately<00:04:08.650> process<00:04:09.129> different

00:04:09.509 --> 00:04:09.519 
dogs separately process different

00:04:09.519 --> 00:04:11.279 
dogs separately process different
information<00:04:10.090> in<00:04:10.150> a<00:04:10.269> speech<00:04:10.539> signal<00:04:10.810> and<00:04:11.169> the

00:04:11.279 --> 00:04:11.289 
information in a speech signal and the

00:04:11.289 --> 00:04:13.460 
information in a speech signal and the
way<00:04:11.409> they<00:04:11.590> do<00:04:11.799> this<00:04:11.979> is<00:04:12.340> very<00:04:12.579> similar<00:04:13.120> to<00:04:13.269> the

00:04:13.460 --> 00:04:13.470 
way they do this is very similar to the

00:04:13.470 --> 00:04:22.400 
way they do this is very similar to the
we<00:04:13.680> do<00:04:13.860> it

00:04:22.400 --> 00:04:22.410 

00:04:22.410 --> 00:04:24.470 

you

