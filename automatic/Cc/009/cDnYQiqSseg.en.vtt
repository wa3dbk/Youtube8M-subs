WEBVTT
Kind: captions
Language: en

00:00:02.449 --> 00:00:04.940 

hello<00:00:03.449> and<00:00:03.720> welcome<00:00:04.080> back<00:00:04.110> to<00:00:04.529> this<00:00:04.650> video

00:00:04.940 --> 00:00:04.950 
hello and welcome back to this video

00:00:04.950 --> 00:00:08.120 
hello and welcome back to this video
series<00:00:05.220> on<00:00:05.430> global<00:00:06.060> health<00:00:06.270> ethics<00:00:06.950> this<00:00:07.950> is

00:00:08.120 --> 00:00:08.130 
series on global health ethics this is

00:00:08.130 --> 00:00:09.890 
series on global health ethics this is
the<00:00:08.250> second<00:00:08.670> episode<00:00:09.090> in<00:00:09.120> the<00:00:09.330> series<00:00:09.690> in

00:00:09.890 --> 00:00:09.900 
the second episode in the series in

00:00:09.900 --> 00:00:11.360 
the second episode in the series in
which<00:00:09.960> we're<00:00:10.290> taking<00:00:10.650> a<00:00:10.740> look<00:00:10.860> at<00:00:11.010> some<00:00:11.190> of<00:00:11.280> the

00:00:11.360 --> 00:00:11.370 
which we're taking a look at some of the

00:00:11.370 --> 00:00:13.669 
which we're taking a look at some of the
principles<00:00:11.969> that<00:00:12.210> underpin<00:00:13.019> moral<00:00:13.530> and

00:00:13.669 --> 00:00:13.679 
principles that underpin moral and

00:00:13.679 --> 00:00:15.110 
principles that underpin moral and
ethical<00:00:14.009> issues<00:00:14.370> in<00:00:14.549> the<00:00:14.700> global<00:00:14.969> health

00:00:15.110 --> 00:00:15.120 
ethical issues in the global health

00:00:15.120 --> 00:00:18.529 
ethical issues in the global health
space<00:00:16.250> in<00:00:17.250> this<00:00:17.730> episode<00:00:18.210> we're<00:00:18.390> going<00:00:18.480> to

00:00:18.529 --> 00:00:18.539 
space in this episode we're going to

00:00:18.539 --> 00:00:20.240 
space in this episode we're going to
take<00:00:18.750> a<00:00:18.779> closer<00:00:19.109> look<00:00:19.380> at<00:00:19.590> two<00:00:19.859> pretty

00:00:20.240 --> 00:00:20.250 
take a closer look at two pretty

00:00:20.250 --> 00:00:22.279 
take a closer look at two pretty
controversial<00:00:20.820> and<00:00:21.240> certainly<00:00:21.630> opposing

00:00:22.279 --> 00:00:22.289 
controversial and certainly opposing

00:00:22.289 --> 00:00:25.220 
controversial and certainly opposing
ideas<00:00:22.680> about<00:00:23.670> how<00:00:23.910> to<00:00:23.970> decide<00:00:24.480> what<00:00:24.810> is<00:00:24.930> right

00:00:25.220 --> 00:00:25.230 
ideas about how to decide what is right

00:00:25.230 --> 00:00:27.740 
ideas about how to decide what is right
or<00:00:25.500> moral<00:00:25.859> and<00:00:26.160> then<00:00:26.760> in<00:00:26.910> future<00:00:27.599> episodes

00:00:27.740 --> 00:00:27.750 
or moral and then in future episodes

00:00:27.750 --> 00:00:29.269 
or moral and then in future episodes
we're<00:00:28.260> gonna<00:00:28.349> untangle<00:00:28.710> them<00:00:28.949> a<00:00:29.010> little<00:00:29.189> bit

00:00:29.269 --> 00:00:29.279 
we're gonna untangle them a little bit

00:00:29.279 --> 00:00:31.070 
we're gonna untangle them a little bit
more<00:00:29.460> and<00:00:29.640> try<00:00:30.060> to<00:00:30.119> understand<00:00:30.570> how<00:00:30.750> to<00:00:30.779> apply

00:00:31.070 --> 00:00:31.080 
more and try to understand how to apply

00:00:31.080 --> 00:00:33.020 
more and try to understand how to apply
these<00:00:31.199> ideas<00:00:31.380> in<00:00:32.099> global<00:00:32.579> health<00:00:32.669> ethical

00:00:33.020 --> 00:00:33.030 
these ideas in global health ethical

00:00:33.030 --> 00:00:35.600 
these ideas in global health ethical
issues<00:00:33.620> the<00:00:34.620> first<00:00:34.829> idea<00:00:35.100> that<00:00:35.130> we'll<00:00:35.370> look<00:00:35.460> at

00:00:35.600 --> 00:00:35.610 
issues the first idea that we'll look at

00:00:35.610 --> 00:00:37.970 
issues the first idea that we'll look at
is<00:00:35.790> called<00:00:35.969> utilitarianism<00:00:36.840> this<00:00:37.829> is

00:00:37.970 --> 00:00:37.980 
is called utilitarianism this is

00:00:37.980 --> 00:00:40.220 
is called utilitarianism this is
basically<00:00:38.399> the<00:00:38.940> theory<00:00:39.360> that<00:00:39.510> proposes<00:00:40.200> that

00:00:40.220 --> 00:00:40.230 
basically the theory that proposes that

00:00:40.230 --> 00:00:42.530 
basically the theory that proposes that
the<00:00:40.500> right<00:00:40.739> course<00:00:41.070> of<00:00:41.219> action<00:00:41.460> is<00:00:42.180> the<00:00:42.360> one

00:00:42.530 --> 00:00:42.540 
the right course of action is the one

00:00:42.540 --> 00:00:44.840 
the right course of action is the one
that<00:00:42.690> maximizes<00:00:43.100> utility<00:00:44.100> which<00:00:44.399> is<00:00:44.579> usually

00:00:44.840 --> 00:00:44.850 
that maximizes utility which is usually

00:00:44.850 --> 00:00:46.700 
that maximizes utility which is usually
defined<00:00:44.969> as<00:00:45.329> maximizing<00:00:46.110> happiness<00:00:46.230> or

00:00:46.700 --> 00:00:46.710 
defined as maximizing happiness or

00:00:46.710 --> 00:00:49.340 
defined as maximizing happiness or
reducing<00:00:47.399> suffering<00:00:48.050> now<00:00:49.050> we're<00:00:49.200> going<00:00:49.289> to

00:00:49.340 --> 00:00:49.350 
reducing suffering now we're going to

00:00:49.350 --> 00:00:51.080 
reducing suffering now we're going to
come<00:00:49.500> back<00:00:49.710> to<00:00:49.860> this<00:00:49.980> idea<00:00:50.309> of<00:00:50.340> utilitarian

00:00:51.080 --> 00:00:51.090 
come back to this idea of utilitarian

00:00:51.090 --> 00:00:52.060 
come back to this idea of utilitarian
ethics<00:00:51.180> in<00:00:51.539> just<00:00:51.690> a<00:00:51.750> minute

00:00:52.060 --> 00:00:52.070 
ethics in just a minute

00:00:52.070 --> 00:00:54.950 
ethics in just a minute
for<00:00:53.070> now<00:00:53.280> it's<00:00:53.579> suffice<00:00:53.969> to<00:00:54.180> say<00:00:54.360> that<00:00:54.629> this<00:00:54.780> is

00:00:54.950 --> 00:00:54.960 
for now it's suffice to say that this is

00:00:54.960 --> 00:00:57.920 
for now it's suffice to say that this is
an<00:00:55.050> example<00:00:55.440> of<00:00:55.739> consequentialism<00:00:56.750> and<00:00:57.750> that

00:00:57.920 --> 00:00:57.930 
an example of consequentialism and that

00:00:57.930 --> 00:00:59.569 
an example of consequentialism and that
is<00:00:58.050> to<00:00:58.140> say<00:00:58.350> that<00:00:58.620> consequence

00:00:59.569 --> 00:00:59.579 
is to say that consequence

00:00:59.579 --> 00:01:01.220 
is to say that consequence
consequentialism<00:01:00.300> is<00:01:00.539> basically<00:01:00.989> the<00:01:01.170> idea

00:01:01.220 --> 00:01:01.230 
consequentialism is basically the idea

00:01:01.230 --> 00:01:03.229 
consequentialism is basically the idea
that<00:01:01.559> the<00:01:01.739> moral<00:01:02.039> value<00:01:02.489> of<00:01:02.579> an<00:01:02.699> action<00:01:03.120> is

00:01:03.229 --> 00:01:03.239 
that the moral value of an action is

00:01:03.239 --> 00:01:06.230 
that the moral value of an action is
determined<00:01:03.420> by<00:01:04.080> its<00:01:04.140> consequences<00:01:05.240> let's

00:01:06.230 --> 00:01:06.240 
determined by its consequences let's

00:01:06.240 --> 00:01:07.940 
determined by its consequences let's
take<00:01:06.450> a<00:01:06.479> look<00:01:06.659> at<00:01:06.780> a<00:01:06.810> contrasting<00:01:07.380> idea<00:01:07.710> and

00:01:07.940 --> 00:01:07.950 
take a look at a contrasting idea and

00:01:07.950 --> 00:01:10.820 
take a look at a contrasting idea and
that<00:01:08.640> is<00:01:08.790> a<00:01:08.820> deontological<00:01:09.210> ethics<00:01:10.049> this<00:01:10.650> was

00:01:10.820 --> 00:01:10.830 
that is a deontological ethics this was

00:01:10.830 --> 00:01:14.420 
that is a deontological ethics this was
first<00:01:11.280> proposed<00:01:11.640> by<00:01:11.760> Immanuel<00:01:12.390> Kant<00:01:12.799> yeah<00:01:13.830> the

00:01:14.420 --> 00:01:14.430 
first proposed by Immanuel Kant yeah the

00:01:14.430 --> 00:01:16.640 
first proposed by Immanuel Kant yeah the
morality<00:01:15.000> of<00:01:15.030> an<00:01:15.210> action<00:01:15.630> is<00:01:15.840> based<00:01:16.259> on<00:01:16.530> the

00:01:16.640 --> 00:01:16.650 
morality of an action is based on the

00:01:16.650 --> 00:01:18.590 
morality of an action is based on the
actions<00:01:17.009> adherence<00:01:17.490> to<00:01:17.549> rules<00:01:17.939> and<00:01:18.270> duties

00:01:18.590 --> 00:01:18.600 
actions adherence to rules and duties

00:01:18.600 --> 00:01:21.289 
actions adherence to rules and duties
Kant<00:01:19.560> argues<00:01:20.100> that<00:01:20.430> it's<00:01:20.759> not<00:01:21.150> the

00:01:21.289 --> 00:01:21.299 
Kant argues that it's not the

00:01:21.299 --> 00:01:23.270 
Kant argues that it's not the
consequence<00:01:21.900> of<00:01:22.020> an<00:01:22.140> action<00:01:22.530> that<00:01:22.740> makes<00:01:23.070> them

00:01:23.270 --> 00:01:23.280 
consequence of an action that makes them

00:01:23.280 --> 00:01:25.670 
consequence of an action that makes them
right<00:01:23.490> or<00:01:23.729> wrong<00:01:23.909> but<00:01:24.869> the<00:01:25.020> motives<00:01:25.500> of<00:01:25.619> the

00:01:25.670 --> 00:01:25.680 
right or wrong but the motives of the

00:01:25.680 --> 00:01:27.260 
right or wrong but the motives of the
person<00:01:26.130> who<00:01:26.220> carries<00:01:26.580> them<00:01:26.759> out<00:01:26.850> in<00:01:27.030> the<00:01:27.090> first

00:01:27.260 --> 00:01:27.270 
person who carries them out in the first

00:01:27.270 --> 00:01:27.940 
person who carries them out in the first
place

00:01:27.940 --> 00:01:27.950 
place

00:01:27.950 --> 00:01:30.140 
place
some<00:01:28.950> deontologists<00:01:29.640> are<00:01:29.729> what<00:01:29.850> we<00:01:29.970> call

00:01:30.140 --> 00:01:30.150 
some deontologists are what we call

00:01:30.150 --> 00:01:32.420 
some deontologists are what we call
moral<00:01:30.450> absolutists<00:01:31.409> and<00:01:31.590> they<00:01:31.979> believe<00:01:32.250> that

00:01:32.420 --> 00:01:32.430 
moral absolutists and they believe that

00:01:32.430 --> 00:01:33.530 
moral absolutists and they believe that
they're<00:01:32.640> certain<00:01:32.939> actions<00:01:33.299> which<00:01:33.360> are

00:01:33.530 --> 00:01:33.540 
they're certain actions which are

00:01:33.540 --> 00:01:34.460 
they're certain actions which are
absolutely<00:01:33.960> right

00:01:34.460 --> 00:01:34.470 
absolutely right

00:01:34.470 --> 00:01:36.740 
absolutely right
or<00:01:34.680> absolutely<00:01:35.400> wrong<00:01:35.579> regardless<00:01:36.509> of<00:01:36.659> the

00:01:36.740 --> 00:01:36.750 
or absolutely wrong regardless of the

00:01:36.750 --> 00:01:38.120 
or absolutely wrong regardless of the
consequence<00:01:37.229> of<00:01:37.380> the<00:01:37.470> actions<00:01:37.799> and

00:01:38.120 --> 00:01:38.130 
consequence of the actions and

00:01:38.130 --> 00:01:40.149 
consequence of the actions and
regardless<00:01:38.970> of<00:01:39.090> the<00:01:39.180> intention<00:01:39.600> behind<00:01:39.720> them

00:01:40.149 --> 00:01:40.159 
regardless of the intention behind them

00:01:40.159 --> 00:01:42.590 
regardless of the intention behind them
so<00:01:41.159> what<00:01:41.610> we've<00:01:41.759> got<00:01:41.939> really<00:01:42.270> there<00:01:42.450> and

00:01:42.590 --> 00:01:42.600 
so what we've got really there and

00:01:42.600 --> 00:01:43.850 
so what we've got really there and
there's<00:01:42.780> many<00:01:42.960> more<00:01:43.140> but<00:01:43.350> what<00:01:43.500> we've<00:01:43.649> talked

00:01:43.850 --> 00:01:43.860 
there's many more but what we've talked

00:01:43.860 --> 00:01:45.230 
there's many more but what we've talked
about<00:01:44.070> is<00:01:44.189> three<00:01:44.399> frameworks<00:01:44.850> were<00:01:44.970> talking

00:01:45.230 --> 00:01:45.240 
about is three frameworks were talking

00:01:45.240 --> 00:01:46.700 
about is three frameworks were talking
about<00:01:45.420> ethics<00:01:45.540> the<00:01:45.810> one<00:01:45.960> the<00:01:46.320> first<00:01:46.350> is

00:01:46.700 --> 00:01:46.710 
about ethics the one the first is

00:01:46.710 --> 00:01:49.609 
about ethics the one the first is
utilitarian<00:01:47.399> ethics<00:01:47.490> so<00:01:48.290> whether<00:01:49.290> something

00:01:49.609 --> 00:01:49.619 
utilitarian ethics so whether something

00:01:49.619 --> 00:01:50.630 
utilitarian ethics so whether something
is<00:01:49.649> right<00:01:49.710> or<00:01:49.829> wrong<00:01:50.040> is<00:01:50.189> a<00:01:50.220> function<00:01:50.579> of

00:01:50.630 --> 00:01:50.640 
is right or wrong is a function of

00:01:50.640 --> 00:01:52.520 
is right or wrong is a function of
whether<00:01:50.850> it<00:01:50.970> translates<00:01:51.390> into<00:01:51.630> the<00:01:52.140> greatest

00:01:52.520 --> 00:01:52.530 
whether it translates into the greatest

00:01:52.530 --> 00:01:53.870 
whether it translates into the greatest
good<00:01:52.649> for<00:01:52.710> the<00:01:52.860> most<00:01:53.070> number<00:01:53.220> of<00:01:53.399> people

00:01:53.870 --> 00:01:53.880 
good for the most number of people

00:01:53.880 --> 00:01:56.510 
good for the most number of people
the<00:01:54.420> second<00:01:54.810> is<00:01:55.170> Kant<00:01:55.560> deontological<00:01:55.950> ethics

00:01:56.510 --> 00:01:56.520 
the second is Kant deontological ethics

00:01:56.520 --> 00:01:57.800 
the second is Kant deontological ethics
which<00:01:56.640> basically<00:01:56.939> says<00:01:57.240> whether<00:01:57.450> something's

00:01:57.800 --> 00:01:57.810 
which basically says whether something's

00:01:57.810 --> 00:01:59.510 
which basically says whether something's
right<00:01:57.960> or<00:01:58.110> wrong<00:01:58.229> as<00:01:58.380> a<00:01:58.409> function<00:01:58.860> of<00:01:59.009> the

00:01:59.510 --> 00:01:59.520 
right or wrong as a function of the

00:01:59.520 --> 00:02:02.209 
right or wrong as a function of the
motives<00:01:59.909> behind<00:02:00.659> the<00:02:01.049> action<00:02:01.439> and<00:02:01.590> the<00:02:02.040> third

00:02:02.209 --> 00:02:02.219 
motives behind the action and the third

00:02:02.219 --> 00:02:04.370 
motives behind the action and the third
is<00:02:02.399> this<00:02:02.520> absolutism<00:02:03.149> the<00:02:03.780> certain<00:02:04.110> rules

00:02:04.370 --> 00:02:04.380 
is this absolutism the certain rules

00:02:04.380 --> 00:02:06.920 
is this absolutism the certain rules
there's<00:02:05.159> right<00:02:05.520> actions<00:02:06.390> and<00:02:06.570> wrong<00:02:06.719> actions

00:02:06.920 --> 00:02:06.930 
there's right actions and wrong actions

00:02:06.930 --> 00:02:09.589 
there's right actions and wrong actions
and<00:02:07.439> they're<00:02:07.799> right<00:02:07.979> wrong<00:02:08.369> regardless<00:02:09.090> of

00:02:09.589 --> 00:02:09.599 
and they're right wrong regardless of

00:02:09.599 --> 00:02:11.390 
and they're right wrong regardless of
what<00:02:09.899> happens<00:02:10.229> because<00:02:10.410> of<00:02:10.560> them<00:02:10.800> and<00:02:10.830> regard

00:02:11.390 --> 00:02:11.400 
what happens because of them and regard

00:02:11.400 --> 00:02:13.819 
what happens because of them and regard
of<00:02:11.459> the<00:02:11.579> motors<00:02:11.879> behind<00:02:12.030> them<00:02:12.769> there's<00:02:13.769> a

00:02:13.819 --> 00:02:13.829 
of the motors behind them there's a

00:02:13.829 --> 00:02:16.339 
of the motors behind them there's a
tendency<00:02:14.280> of<00:02:14.519> people<00:02:14.909> when<00:02:15.510> confronted<00:02:16.230> with

00:02:16.339 --> 00:02:16.349 
tendency of people when confronted with

00:02:16.349 --> 00:02:18.920 
tendency of people when confronted with
these<00:02:16.500> two<00:02:17.010> opposing<00:02:17.459> ideas<00:02:18.000> utilitarian

00:02:18.920 --> 00:02:18.930 
these two opposing ideas utilitarian

00:02:18.930 --> 00:02:21.170 
these two opposing ideas utilitarian
ethics<00:02:19.019> and<00:02:19.409> deontological<00:02:19.769> ethics<00:02:20.310> to

00:02:21.170 --> 00:02:21.180 
ethics and deontological ethics to

00:02:21.180 --> 00:02:23.030 
ethics and deontological ethics to
immediately<00:02:21.569> place<00:02:21.900> themselves<00:02:22.140> in<00:02:22.530> one<00:02:22.829> of

00:02:23.030 --> 00:02:23.040 
immediately place themselves in one of

00:02:23.040 --> 00:02:25.640 
immediately place themselves in one of
the<00:02:23.519> two<00:02:23.700> camps<00:02:24.049> we<00:02:25.049> tend<00:02:25.349> to<00:02:25.409> have<00:02:25.620> a

00:02:25.640 --> 00:02:25.650 
the two camps we tend to have a

00:02:25.650 --> 00:02:27.740 
the two camps we tend to have a
knee-jerk<00:02:26.129> philosophical<00:02:27.120> reaction<00:02:27.480> of<00:02:27.569> some

00:02:27.740 --> 00:02:27.750 
knee-jerk philosophical reaction of some

00:02:27.750 --> 00:02:30.229 
knee-jerk philosophical reaction of some
kind<00:02:27.989> and<00:02:28.379> then<00:02:29.069> once<00:02:29.400> we've<00:02:29.849> taken<00:02:29.879> a

00:02:30.229 --> 00:02:30.239 
kind and then once we've taken a

00:02:30.239 --> 00:02:32.000 
kind and then once we've taken a
position<00:02:30.390> we've<00:02:31.140> placed<00:02:31.379> ourselves<00:02:31.560> in<00:02:31.890> one

00:02:32.000 --> 00:02:32.010 
position we've placed ourselves in one

00:02:32.010 --> 00:02:33.380 
position we've placed ourselves in one
of<00:02:32.099> these<00:02:32.189> camps<00:02:32.400> we<00:02:32.549> then<00:02:32.790> try<00:02:33.090> and<00:02:33.239> find

00:02:33.380 --> 00:02:33.390 
of these camps we then try and find

00:02:33.390 --> 00:02:34.970 
of these camps we then try and find
examples<00:02:33.750> and<00:02:33.989> anecdotes<00:02:34.409> to<00:02:34.590> bolster<00:02:34.859> that

00:02:34.970 --> 00:02:34.980 
examples and anecdotes to bolster that

00:02:34.980 --> 00:02:37.910 
examples and anecdotes to bolster that
position<00:02:35.750> I'd<00:02:36.750> like<00:02:36.930> to<00:02:37.049> introduce<00:02:37.230> you<00:02:37.620> to<00:02:37.829> an

00:02:37.910 --> 00:02:37.920 
position I'd like to introduce you to an

00:02:37.920 --> 00:02:39.710 
position I'd like to introduce you to an
interesting<00:02:38.159> thought<00:02:38.730> experiment<00:02:39.030> this<00:02:39.540> was

00:02:39.710 --> 00:02:39.720 
interesting thought experiment this was

00:02:39.720 --> 00:02:40.880 
interesting thought experiment this was
introduced<00:02:40.140> by<00:02:40.319> a<00:02:40.349> lady<00:02:40.530> by<00:02:40.650> the<00:02:40.680> name<00:02:40.769> of

00:02:40.880 --> 00:02:40.890 
introduced by a lady by the name of

00:02:40.890 --> 00:02:44.059 
introduced by a lady by the name of
Philippa<00:02:41.370> foot<00:02:41.579> in<00:02:41.900> 1967<00:02:42.900> and<00:02:43.590> it's<00:02:43.829> called

00:02:44.059 --> 00:02:44.069 
Philippa foot in 1967 and it's called

00:02:44.069 --> 00:02:46.699 
Philippa foot in 1967 and it's called
trolley<00:02:44.609> problem<00:02:45.030> I<00:02:45.299> imagine<00:02:46.139> a<00:02:46.200> trolley<00:02:46.470> or<00:02:46.590> a

00:02:46.699 --> 00:02:46.709 
trolley problem I imagine a trolley or a

00:02:46.709 --> 00:02:49.610 
trolley problem I imagine a trolley or a
train<00:02:47.099> hurtling<00:02:47.940> down<00:02:48.180> a<00:02:48.329> track<00:02:48.420> and<00:02:49.170> imagine

00:02:49.610 --> 00:02:49.620 
train hurtling down a track and imagine

00:02:49.620 --> 00:02:51.229 
train hurtling down a track and imagine
that<00:02:49.739> down<00:02:49.980> the<00:02:50.129> line<00:02:50.159> an<00:02:50.519> evil<00:02:50.819> villain<00:02:51.120> is

00:02:51.229 --> 00:02:51.239 
that down the line an evil villain is

00:02:51.239 --> 00:02:53.270 
that down the line an evil villain is
tied<00:02:51.629> five<00:02:51.959> people<00:02:52.470> to<00:02:52.620> the<00:02:52.739> track<00:02:52.980> and

00:02:53.270 --> 00:02:53.280 
tied five people to the track and

00:02:53.280 --> 00:02:54.649 
tied five people to the track and
they're<00:02:53.549> all<00:02:53.700> going<00:02:53.879> to<00:02:53.970> get<00:02:54.030> killed<00:02:54.359> when<00:02:54.540> the

00:02:54.649 --> 00:02:54.659 
they're all going to get killed when the

00:02:54.659 --> 00:02:57.740 
they're all going to get killed when the
trolley<00:02:54.870> hits<00:02:55.109> them<00:02:55.670> interestingly<00:02:56.670> you<00:02:57.450> find

00:02:57.740 --> 00:02:57.750 
trolley hits them interestingly you find

00:02:57.750 --> 00:03:00.649 
trolley hits them interestingly you find
yourself<00:02:58.049> standing<00:02:58.409> next<00:02:58.620> to<00:02:58.739> a<00:02:58.829> lever<00:02:59.220> and<00:02:59.670> if

00:03:00.649 --> 00:03:00.659 
yourself standing next to a lever and if

00:03:00.659 --> 00:03:02.809 
yourself standing next to a lever and if
you<00:03:00.780> pull<00:03:01.170> that<00:03:01.200> lever<00:03:01.680> the<00:03:02.069> trolley<00:03:02.400> will<00:03:02.700> be

00:03:02.809 --> 00:03:02.819 
you pull that lever the trolley will be

00:03:02.819 --> 00:03:05.390 
you pull that lever the trolley will be
diverted<00:03:03.389> onto<00:03:03.629> an<00:03:03.840> alternative<00:03:04.379> track<00:03:04.590> there

00:03:05.390 --> 00:03:05.400 
diverted onto an alternative track there

00:03:05.400 --> 00:03:07.280 
diverted onto an alternative track there
on<00:03:05.489> this<00:03:05.639> alternative<00:03:06.150> track<00:03:06.389> though<00:03:06.569> there's

00:03:07.280 --> 00:03:07.290 
on this alternative track though there's

00:03:07.290 --> 00:03:08.929 
on this alternative track though there's
another<00:03:07.530> man<00:03:07.739> and<00:03:07.980> he's<00:03:08.099> been<00:03:08.280> tied<00:03:08.519> down<00:03:08.700> and

00:03:08.929 --> 00:03:08.939 
another man and he's been tied down and

00:03:08.939 --> 00:03:10.610 
another man and he's been tied down and
if<00:03:09.510> the<00:03:09.599> trolley<00:03:09.780> hits<00:03:09.989> him<00:03:10.109> he's<00:03:10.260> gonna<00:03:10.379> die

00:03:10.610 --> 00:03:10.620 
if the trolley hits him he's gonna die

00:03:10.620 --> 00:03:12.470 
if the trolley hits him he's gonna die
in<00:03:10.739> fact<00:03:10.980> on<00:03:11.189> that<00:03:11.250> track<00:03:11.939> there's<00:03:12.150> just<00:03:12.299> one

00:03:12.470 --> 00:03:12.480 
in fact on that track there's just one

00:03:12.480 --> 00:03:16.009 
in fact on that track there's just one
man<00:03:12.720> and<00:03:12.989> not<00:03:13.829> five<00:03:14.510> so<00:03:15.510> the<00:03:15.599> question<00:03:15.900> is

00:03:16.009 --> 00:03:16.019 
man and not five so the question is

00:03:16.019 --> 00:03:19.490 
man and not five so the question is
should<00:03:16.519> you<00:03:17.519> pull<00:03:17.879> the<00:03:18.030> lever<00:03:18.239> and<00:03:18.780> so<00:03:19.019> divert

00:03:19.490 --> 00:03:19.500 
should you pull the lever and so divert

00:03:19.500 --> 00:03:21.800 
should you pull the lever and so divert
the<00:03:19.650> trolley<00:03:19.859> from<00:03:20.370> the<00:03:20.459> track<00:03:20.790> on<00:03:21.120> which<00:03:21.480> will

00:03:21.800 --> 00:03:21.810 
the trolley from the track on which will

00:03:21.810 --> 00:03:23.990 
the trolley from the track on which will
kill<00:03:22.019> five<00:03:22.319> people<00:03:22.430> under<00:03:23.430> the<00:03:23.519> track<00:03:23.760> in

00:03:23.990 --> 00:03:24.000 
kill five people under the track in

00:03:24.000 --> 00:03:25.720 
kill five people under the track in
which<00:03:24.150> it<00:03:24.299> will<00:03:24.449> only<00:03:24.629> kill<00:03:24.959> one<00:03:25.139> person

00:03:25.720 --> 00:03:25.730 
which it will only kill one person

00:03:25.730 --> 00:03:28.280 
which it will only kill one person
studies<00:03:26.730> showed<00:03:27.090> that<00:03:27.120> the<00:03:27.540> overwhelming

00:03:28.280 --> 00:03:28.290 
studies showed that the overwhelming

00:03:28.290 --> 00:03:31.640 
studies showed that the overwhelming
majority<00:03:28.409> of<00:03:28.829> people<00:03:28.919> say<00:03:29.519> yes<00:03:30.090> and<00:03:30.470> that's<00:03:31.470> a

00:03:31.640 --> 00:03:31.650 
majority of people say yes and that's a

00:03:31.650 --> 00:03:34.399 
majority of people say yes and that's a
typical<00:03:31.970> utilitarian<00:03:32.970> position<00:03:33.419> one<00:03:34.019> person

00:03:34.399 --> 00:03:34.409 
typical utilitarian position one person

00:03:34.409 --> 00:03:36.309 
typical utilitarian position one person
dying<00:03:34.650> is<00:03:34.829> better<00:03:35.099> than<00:03:35.310> five<00:03:35.519> people<00:03:35.879> dying

00:03:36.309 --> 00:03:36.319 
dying is better than five people dying

00:03:36.319 --> 00:03:38.659 
dying is better than five people dying
now<00:03:37.319> let's<00:03:37.560> make<00:03:37.709> the<00:03:37.919> scenario<00:03:38.400> a<00:03:38.459> little

00:03:38.659 --> 00:03:38.669 
now let's make the scenario a little

00:03:38.669 --> 00:03:40.909 
now let's make the scenario a little
more<00:03:39.000> complicated<00:03:39.709> imagine<00:03:40.709> that<00:03:40.829> the

00:03:40.909 --> 00:03:40.919 
more complicated imagine that the

00:03:40.919 --> 00:03:43.280 
more complicated imagine that the
trolley<00:03:41.159> is<00:03:41.579> once<00:03:41.879> again<00:03:42.030> hurtling<00:03:42.690> down<00:03:42.720> and

00:03:43.280 --> 00:03:43.290 
trolley is once again hurtling down and

00:03:43.290 --> 00:03:45.080 
trolley is once again hurtling down and
there's<00:03:43.560> five<00:03:44.069> people<00:03:44.099> tied<00:03:44.669> to<00:03:44.699> the<00:03:44.879> track

00:03:45.080 --> 00:03:45.090 
there's five people tied to the track

00:03:45.090 --> 00:03:48.170 
there's five people tied to the track
this<00:03:45.930> time<00:03:46.319> however<00:03:47.310> there's<00:03:47.549> no<00:03:47.729> lever<00:03:48.000> to

00:03:48.170 --> 00:03:48.180 
this time however there's no lever to

00:03:48.180 --> 00:03:50.330 
this time however there's no lever to
pull<00:03:48.359> instead<00:03:49.290> you're<00:03:49.769> standing<00:03:50.220> on<00:03:50.310> a

00:03:50.330 --> 00:03:50.340 
pull instead you're standing on a

00:03:50.340 --> 00:03:51.920 
pull instead you're standing on a
footbridge<00:03:50.699> that<00:03:51.180> goes<00:03:51.389> over<00:03:51.659> the<00:03:51.750> track

00:03:51.920 --> 00:03:51.930 
footbridge that goes over the track

00:03:51.930 --> 00:03:55.009 
footbridge that goes over the track
you've<00:03:52.859> got<00:03:53.069> the<00:03:53.190> option<00:03:53.340> of<00:03:53.699> pushing<00:03:54.599> a<00:03:54.690> Tramp

00:03:55.009 --> 00:03:55.019 
you've got the option of pushing a Tramp

00:03:55.019 --> 00:03:56.809 
you've got the option of pushing a Tramp
off<00:03:55.260> the<00:03:55.560> bridge<00:03:55.799> onto<00:03:56.250> the<00:03:56.340> track<00:03:56.579> and

00:03:56.809 --> 00:03:56.819 
off the bridge onto the track and

00:03:56.819 --> 00:03:58.369 
off the bridge onto the track and
thereby<00:03:57.120> stopping<00:03:57.540> the<00:03:57.629> trolley<00:03:57.840> and<00:03:57.959> saving

00:03:58.369 --> 00:03:58.379 
thereby stopping the trolley and saving

00:03:58.379 --> 00:04:00.710 
thereby stopping the trolley and saving
the<00:03:58.470> five<00:03:58.650> people<00:03:58.859> but<00:03:59.549> of<00:03:59.760> course<00:04:00.000> you<00:04:00.629> will

00:04:00.710 --> 00:04:00.720 
the five people but of course you will

00:04:00.720 --> 00:04:03.589 
the five people but of course you will
kill<00:04:00.930> the<00:04:01.079> trim<00:04:01.290> in<00:04:01.620> the<00:04:01.799> process<00:04:02.419> based<00:04:03.419> but

00:04:03.589 --> 00:04:03.599 
kill the trim in the process based but

00:04:03.599 --> 00:04:05.539 
kill the trim in the process based but
the<00:04:03.720> second<00:04:04.079> scenario<00:04:04.199> most<00:04:04.680> people<00:04:05.040> do<00:04:05.220> not

00:04:05.539 --> 00:04:05.549 
the second scenario most people do not

00:04:05.549 --> 00:04:07.039 
the second scenario most people do not
believe<00:04:05.909> that<00:04:06.150> it's<00:04:06.359> right<00:04:06.540> to<00:04:06.690> push<00:04:06.930> the

00:04:07.039 --> 00:04:07.049 
believe that it's right to push the

00:04:07.049 --> 00:04:09.319 
believe that it's right to push the
Tramp<00:04:07.290> off<00:04:07.500> the<00:04:07.650> bridge<00:04:07.940> even<00:04:08.940> though<00:04:09.180> the

00:04:09.319 --> 00:04:09.329 
Tramp off the bridge even though the

00:04:09.329 --> 00:04:10.909 
Tramp off the bridge even though the
neck<00:04:09.540> result<00:04:09.989> of<00:04:10.260> this<00:04:10.440> action<00:04:10.739> would<00:04:10.859> be

00:04:10.909 --> 00:04:10.919 
neck result of this action would be

00:04:10.919 --> 00:04:13.339 
neck result of this action would be
exactly<00:04:11.459> the<00:04:11.609> same<00:04:11.639> as<00:04:12.120> the<00:04:12.840> first<00:04:13.019> scenario

00:04:13.339 --> 00:04:13.349 
exactly the same as the first scenario

00:04:13.349 --> 00:04:15.649 
exactly the same as the first scenario
that<00:04:13.769> is<00:04:13.979> one<00:04:14.639> person<00:04:15.030> would<00:04:15.150> be<00:04:15.269> sacrificed

00:04:15.649 --> 00:04:15.659 
that is one person would be sacrificed

00:04:15.659 --> 00:04:18.529 
that is one person would be sacrificed
in<00:04:16.079> order<00:04:16.169> to<00:04:16.440> save<00:04:16.620> five<00:04:16.829> others<00:04:17.419> killing<00:04:18.419> is

00:04:18.529 --> 00:04:18.539 
in order to save five others killing is

00:04:18.539 --> 00:04:20.209 
in order to save five others killing is
wrong<00:04:18.570> even<00:04:18.989> if<00:04:19.229> the<00:04:19.349> net<00:04:19.500> result<00:04:19.829> is<00:04:19.919> to<00:04:20.070> say

00:04:20.209 --> 00:04:20.219 
wrong even if the net result is to say

00:04:20.219 --> 00:04:21.860 
wrong even if the net result is to say
about<00:04:20.310> it<00:04:20.549> this<00:04:20.760> is<00:04:21.029> a<00:04:21.090> position<00:04:21.690> that

00:04:21.860 --> 00:04:21.870 
about it this is a position that

00:04:21.870 --> 00:04:22.400 
about it this is a position that
consists

00:04:22.400 --> 00:04:22.410 
consists

00:04:22.410 --> 00:04:24.670 
consists
with<00:04:22.710> the<00:04:22.830> deontological<00:04:23.130> ethical<00:04:23.970> framework

00:04:24.670 --> 00:04:24.680 
with the deontological ethical framework

00:04:24.680 --> 00:04:27.350 
with the deontological ethical framework
now<00:04:25.680> as<00:04:25.800> you<00:04:25.920> can<00:04:26.070> imagine<00:04:26.160> this<00:04:26.880> trolley

00:04:27.350 --> 00:04:27.360 
now as you can imagine this trolley

00:04:27.360 --> 00:04:29.030 
now as you can imagine this trolley
experiment<00:04:27.930> can<00:04:28.050> be<00:04:28.170> extended<00:04:28.650> in<00:04:28.740> a<00:04:28.800> number

00:04:29.030 --> 00:04:29.040 
experiment can be extended in a number

00:04:29.040 --> 00:04:31.310 
experiment can be extended in a number
of<00:04:29.130> ways<00:04:29.280> so<00:04:29.910> for<00:04:30.030> example<00:04:30.450> what<00:04:30.900> would<00:04:31.020> you<00:04:31.140> do

00:04:31.310 --> 00:04:31.320 
of ways so for example what would you do

00:04:31.320 --> 00:04:33.650 
of ways so for example what would you do
if<00:04:31.590> you<00:04:32.250> didn't<00:04:32.670> have<00:04:32.700> to<00:04:32.850> push<00:04:33.090> the<00:04:33.270> trample

00:04:33.650 --> 00:04:33.660 
if you didn't have to push the trample

00:04:33.660 --> 00:04:35.600 
if you didn't have to push the trample
footbridge<00:04:34.140> but<00:04:34.680> you<00:04:34.800> could<00:04:34.920> instead<00:04:35.280> put<00:04:35.580> a

00:04:35.600 --> 00:04:35.610 
footbridge but you could instead put a

00:04:35.610 --> 00:04:37.640 
footbridge but you could instead put a
lever<00:04:35.910> that<00:04:36.480> would<00:04:36.630> open<00:04:36.720> a<00:04:36.900> trapdoor<00:04:37.200> through

00:04:37.640 --> 00:04:37.650 
lever that would open a trapdoor through

00:04:37.650 --> 00:04:39.110 
lever that would open a trapdoor through
which<00:04:37.770> the<00:04:37.920> Tramp<00:04:38.130> would<00:04:38.280> fall<00:04:38.520> onto<00:04:38.940> the

00:04:39.110 --> 00:04:39.120 
which the Tramp would fall onto the

00:04:39.120 --> 00:04:39.680 
which the Tramp would fall onto the
tracks

00:04:39.680 --> 00:04:39.690 
tracks

00:04:39.690 --> 00:04:41.690 
tracks
stopping<00:04:40.140> the<00:04:40.200> trolley<00:04:40.440> and<00:04:40.590> saving<00:04:40.980> the<00:04:41.520> five

00:04:41.690 --> 00:04:41.700 
stopping the trolley and saving the five

00:04:41.700 --> 00:04:43.700 
stopping the trolley and saving the five
lives<00:04:41.970> but<00:04:42.570> of<00:04:42.690> course<00:04:42.870> again<00:04:43.110> the<00:04:43.500> Tramp

00:04:43.700 --> 00:04:43.710 
lives but of course again the Tramp

00:04:43.710 --> 00:04:46.610 
lives but of course again the Tramp
would<00:04:43.860> die<00:04:43.980> and<00:04:44.630> what<00:04:45.630> if<00:04:45.780> the<00:04:45.990> Tramp<00:04:46.380> on<00:04:46.530> the

00:04:46.610 --> 00:04:46.620 
would die and what if the Tramp on the

00:04:46.620 --> 00:04:48.380 
would die and what if the Tramp on the
bridge<00:04:46.830> were<00:04:47.190> actually<00:04:47.610> the<00:04:47.730> evil<00:04:47.880> villain

00:04:48.380 --> 00:04:48.390 
bridge were actually the evil villain

00:04:48.390 --> 00:04:49.880 
bridge were actually the evil villain
who<00:04:48.540> tied<00:04:48.780> the<00:04:48.960> five<00:04:49.110> people<00:04:49.380> to<00:04:49.590> the<00:04:49.620> track<00:04:49.860> in

00:04:49.880 --> 00:04:49.890 
who tied the five people to the track in

00:04:49.890 --> 00:04:53.240 
who tied the five people to the track in
the<00:04:50.010> first<00:04:50.070> place<00:04:50.430> or<00:04:51.800> what<00:04:52.800> if<00:04:52.890> it<00:04:52.980> wasn't

00:04:53.240 --> 00:04:53.250 
the first place or what if it wasn't

00:04:53.250 --> 00:04:54.530 
the first place or what if it wasn't
just<00:04:53.370> five<00:04:53.550> people<00:04:53.880> tied<00:04:53.970> to<00:04:54.030> the<00:04:54.180> track<00:04:54.330> what

00:04:54.530 --> 00:04:54.540 
just five people tied to the track what

00:04:54.540 --> 00:04:56.300 
just five people tied to the track what
if<00:04:54.660> it<00:04:54.810> was<00:04:54.990> a<00:04:55.020> hundred<00:04:55.440> people<00:04:55.740> or<00:04:55.890> the<00:04:56.070> entire

00:04:56.300 --> 00:04:56.310 
if it was a hundred people or the entire

00:04:56.310 --> 00:04:57.770 
if it was a hundred people or the entire
population<00:04:56.940> of<00:04:57.000> a<00:04:57.090> small<00:04:57.300> village<00:04:57.480> in<00:04:57.720> a

00:04:57.770 --> 00:04:57.780 
population of a small village in a

00:04:57.780 --> 00:04:59.900 
population of a small village in a
developing<00:04:58.170> country<00:04:58.670> the<00:04:59.670> trolley

00:04:59.900 --> 00:04:59.910 
developing country the trolley

00:04:59.910 --> 00:05:01.580 
developing country the trolley
experiment<00:05:00.450> illustrates<00:05:00.600> the<00:05:01.140> extent<00:05:01.470> to

00:05:01.580 --> 00:05:01.590 
experiment illustrates the extent to

00:05:01.590 --> 00:05:03.350 
experiment illustrates the extent to
which<00:05:01.710> our<00:05:02.010> subjective<00:05:02.280> position<00:05:03.150> on<00:05:03.240> an

00:05:03.350 --> 00:05:03.360 
which our subjective position on an

00:05:03.360 --> 00:05:06.410 
which our subjective position on an
issue<00:05:03.510> of<00:05:03.780> right<00:05:04.050> or<00:05:04.350> wrong<00:05:04.560> is<00:05:05.420> dramatically

00:05:06.410 --> 00:05:06.420 
issue of right or wrong is dramatically

00:05:06.420 --> 00:05:08.270 
issue of right or wrong is dramatically
and<00:05:06.630> compellingly<00:05:07.080> changed<00:05:07.500> by<00:05:07.620> the<00:05:07.680> context

00:05:08.270 --> 00:05:08.280 
and compellingly changed by the context

00:05:08.280 --> 00:05:10.060 
and compellingly changed by the context
in<00:05:08.370> which<00:05:08.400> we<00:05:08.640> make<00:05:08.820> that<00:05:09.000> decision

00:05:10.060 --> 00:05:10.070 
in which we make that decision

00:05:10.070 --> 00:05:13.550 
in which we make that decision
interestingly<00:05:11.210> functional<00:05:12.210> MRI<00:05:12.720> brain<00:05:13.140> scans

00:05:13.550 --> 00:05:13.560 
interestingly functional MRI brain scans

00:05:13.560 --> 00:05:15.440 
interestingly functional MRI brain scans
of<00:05:13.680> individuals<00:05:14.280> faced<00:05:14.610> with<00:05:14.880> these<00:05:15.060> trolley

00:05:15.440 --> 00:05:15.450 
of individuals faced with these trolley

00:05:15.450 --> 00:05:18.470 
of individuals faced with these trolley
experiments<00:05:16.140> show<00:05:16.950> that<00:05:17.060> variation<00:05:18.060> in<00:05:18.240> the

00:05:18.470 --> 00:05:18.480 
experiments show that variation in the

00:05:18.480 --> 00:05:20.990 
experiments show that variation in the
scenario<00:05:19.050> causes<00:05:19.710> completely<00:05:20.460> different

00:05:20.990 --> 00:05:21.000 
scenario causes completely different

00:05:21.000 --> 00:05:22.490 
scenario causes completely different
parts<00:05:21.330> of<00:05:21.390> the<00:05:21.540> brain<00:05:21.750> to<00:05:21.900> be<00:05:21.930> involved<00:05:22.380> with

00:05:22.490 --> 00:05:22.500 
parts of the brain to be involved with

00:05:22.500 --> 00:05:25.310 
parts of the brain to be involved with
decision<00:05:22.830> making<00:05:23.160> and<00:05:23.340> so<00:05:24.200> even<00:05:25.200> though<00:05:25.290> a

00:05:25.310 --> 00:05:25.320 
decision making and so even though a

00:05:25.320 --> 00:05:27.160 
decision making and so even though a
completely<00:05:25.890> different<00:05:26.100> decision<00:05:26.640> gets<00:05:26.820> made

00:05:27.160 --> 00:05:27.170 
completely different decision gets made

00:05:27.170 --> 00:05:29.780 
completely different decision gets made
people<00:05:28.170> always<00:05:28.410> seem<00:05:28.950> to<00:05:28.980> believe<00:05:29.310> that<00:05:29.370> their

00:05:29.780 --> 00:05:29.790 
people always seem to believe that their

00:05:29.790 --> 00:05:31.760 
people always seem to believe that their
latest<00:05:30.120> decision<00:05:30.420> is<00:05:30.750> the<00:05:31.260> morally<00:05:31.590> correct

00:05:31.760 --> 00:05:31.770 
latest decision is the morally correct

00:05:31.770 --> 00:05:34.460 
latest decision is the morally correct
thing<00:05:32.360> now<00:05:33.360> if<00:05:33.510> you<00:05:33.690> were<00:05:33.810> hoping<00:05:34.170> that<00:05:34.290> by<00:05:34.440> the

00:05:34.460 --> 00:05:34.470 
thing now if you were hoping that by the

00:05:34.470 --> 00:05:36.080 
thing now if you were hoping that by the
end<00:05:34.650> of<00:05:34.710> this<00:05:34.830> talk<00:05:35.100> I'd<00:05:35.340> have<00:05:35.490> provided<00:05:35.850> you

00:05:36.080 --> 00:05:36.090 
end of this talk I'd have provided you

00:05:36.090 --> 00:05:38.680 
end of this talk I'd have provided you
with<00:05:36.120> a<00:05:36.330> simple<00:05:36.830> paint-by-numbers<00:05:37.830> their

00:05:38.680 --> 00:05:38.690 
with a simple paint-by-numbers their

00:05:38.690 --> 00:05:41.390 
with a simple paint-by-numbers their
yardstick<00:05:39.690> to<00:05:40.320> resolve<00:05:40.650> ethical<00:05:41.040> dilemmas

00:05:41.390 --> 00:05:41.400 
yardstick to resolve ethical dilemmas

00:05:41.400 --> 00:05:43.670 
yardstick to resolve ethical dilemmas
well<00:05:41.610> I'm<00:05:41.790> sorry<00:05:42.180> to<00:05:42.390> disappoint<00:05:42.660> the<00:05:43.440> truth

00:05:43.670 --> 00:05:43.680 
well I'm sorry to disappoint the truth

00:05:43.680 --> 00:05:47.030 
well I'm sorry to disappoint the truth
is<00:05:43.920> that<00:05:44.510> even<00:05:45.510> though<00:05:45.690> we<00:05:46.290> think<00:05:46.860> we

00:05:47.030 --> 00:05:47.040 
is that even though we think we

00:05:47.040 --> 00:05:48.560 
is that even though we think we
subscribe<00:05:47.490> to<00:05:47.640> some<00:05:47.790> sort<00:05:48.030> of<00:05:48.060> water-type

00:05:48.560 --> 00:05:48.570 
subscribe to some sort of water-type

00:05:48.570 --> 00:05:49.970 
subscribe to some sort of water-type
framework<00:05:49.050> for<00:05:49.200> determining<00:05:49.290> right<00:05:49.770> from

00:05:49.970 --> 00:05:49.980 
framework for determining right from

00:05:49.980 --> 00:05:52.610 
framework for determining right from
wrong<00:05:50.190> there<00:05:51.060> are<00:05:51.240> lots<00:05:51.930> of<00:05:52.080> examples

00:05:52.610 --> 00:05:52.620 
wrong there are lots of examples

00:05:52.620 --> 00:05:55.820 
wrong there are lots of examples
particularly<00:05:53.130> in<00:05:53.460> the<00:05:53.610> extreme<00:05:54.170> where<00:05:55.170> the

00:05:55.820 --> 00:05:55.830 
particularly in the extreme where the

00:05:55.830 --> 00:05:57.370 
particularly in the extreme where the
utilitarian<00:05:56.610> and<00:05:56.850> the<00:05:57.030> deontological

00:05:57.370 --> 00:05:57.380 
utilitarian and the deontological

00:05:57.380 --> 00:05:59.770 
utilitarian and the deontological
ethical<00:05:58.380> frameworks<00:05:58.830> seem<00:05:58.980> to<00:05:59.100> fall<00:05:59.280> apart

00:05:59.770 --> 00:05:59.780 
ethical frameworks seem to fall apart

00:05:59.780 --> 00:06:03.320 
ethical frameworks seem to fall apart
the<00:06:00.780> lesson<00:06:01.140> here<00:06:01.380> is<00:06:01.560> not<00:06:02.490> to<00:06:02.730> be<00:06:02.850> overly

00:06:03.320 --> 00:06:03.330 
the lesson here is not to be overly

00:06:03.330 --> 00:06:04.640 
the lesson here is not to be overly
concrete<00:06:03.720> in<00:06:03.870> your<00:06:03.900> thinking<00:06:04.260> on<00:06:04.320> ethical

00:06:04.640 --> 00:06:04.650 
concrete in your thinking on ethical

00:06:04.650 --> 00:06:06.860 
concrete in your thinking on ethical
issues<00:06:04.950> the<00:06:05.880> starting<00:06:06.240> point<00:06:06.510> should<00:06:06.660> always

00:06:06.860 --> 00:06:06.870 
issues the starting point should always

00:06:06.870 --> 00:06:08.690 
issues the starting point should always
be<00:06:07.110> to<00:06:07.260> have<00:06:07.380> a<00:06:07.410> detailed<00:06:07.920> understanding<00:06:08.070> of

00:06:08.690 --> 00:06:08.700 
be to have a detailed understanding of

00:06:08.700 --> 00:06:10.180 
be to have a detailed understanding of
the<00:06:08.760> issue<00:06:09.000> that<00:06:09.270> you're<00:06:09.390> dealing<00:06:09.570> with<00:06:09.720> and

00:06:10.180 --> 00:06:10.190 
the issue that you're dealing with and

00:06:10.190 --> 00:06:13.260 
the issue that you're dealing with and
understand<00:06:11.190> all<00:06:11.580> the<00:06:11.730> sides<00:06:11.970> of<00:06:12.150> the<00:06:12.210> debate

00:06:13.260 --> 00:06:13.270 
understand all the sides of the debate

00:06:13.270 --> 00:06:16.080 
understand all the sides of the debate
in<00:06:13.720> any<00:06:14.259> ethical<00:06:14.800> debate<00:06:15.009> you've<00:06:15.400> got<00:06:15.580> people

00:06:16.080 --> 00:06:16.090 
in any ethical debate you've got people

00:06:16.090 --> 00:06:17.700 
in any ethical debate you've got people
in<00:06:16.180> both<00:06:16.449> camps<00:06:16.840> that<00:06:16.990> tend<00:06:17.229> to<00:06:17.259> be<00:06:17.410> extremely

00:06:17.700 --> 00:06:17.710 
in both camps that tend to be extremely

00:06:17.710 --> 00:06:20.700 
in both camps that tend to be extremely
emotional<00:06:18.699> now<00:06:19.690> if<00:06:19.780> you<00:06:19.900> want<00:06:20.080> me<00:06:20.289> to<00:06:20.530> take

00:06:20.700 --> 00:06:20.710 
emotional now if you want me to take

00:06:20.710 --> 00:06:22.920 
emotional now if you want me to take
your<00:06:20.919> argument<00:06:21.190> seriously<00:06:21.849> you<00:06:22.270> need<00:06:22.449> to<00:06:22.509> show

00:06:22.920 --> 00:06:22.930 
your argument seriously you need to show

00:06:22.930 --> 00:06:25.010 
your argument seriously you need to show
me<00:06:23.139> that<00:06:23.500> you<00:06:23.650> can<00:06:23.830> argue<00:06:24.069> the<00:06:24.490> counterfactual

00:06:25.010 --> 00:06:25.020 
me that you can argue the counterfactual

00:06:25.020 --> 00:06:27.689 
me that you can argue the counterfactual
if<00:06:26.020> you<00:06:26.199> can't<00:06:26.500> then<00:06:27.099> it's<00:06:27.220> likely<00:06:27.669> that

00:06:27.689 --> 00:06:27.699 
if you can't then it's likely that

00:06:27.699 --> 00:06:29.760 
if you can't then it's likely that
you've<00:06:28.030> taken<00:06:28.270> an<00:06:28.539> unthinking<00:06:29.199> dogmatic

00:06:29.760 --> 00:06:29.770 
you've taken an unthinking dogmatic

00:06:29.770 --> 00:06:31.290 
you've taken an unthinking dogmatic
position<00:06:30.160> based<00:06:30.400> on<00:06:30.580> some<00:06:30.789> sort<00:06:30.940> of<00:06:31.030> knee-jerk

00:06:31.290 --> 00:06:31.300 
position based on some sort of knee-jerk

00:06:31.300 --> 00:06:33.420 
position based on some sort of knee-jerk
philosophical<00:06:32.199> reaction<00:06:32.560> that<00:06:32.680> you<00:06:32.800> have<00:06:33.009> and

00:06:33.420 --> 00:06:33.430 
philosophical reaction that you have and

00:06:33.430 --> 00:06:35.550 
philosophical reaction that you have and
you<00:06:33.910> really<00:06:34.240> need<00:06:34.419> to<00:06:34.539> take<00:06:34.660> a<00:06:34.720> closer<00:06:35.050> look<00:06:35.169> at

00:06:35.550 --> 00:06:35.560 
you really need to take a closer look at

00:06:35.560 --> 00:06:38.249 
you really need to take a closer look at
the<00:06:35.740> other<00:06:35.860> side<00:06:36.099> of<00:06:36.130> the<00:06:36.310> argument<00:06:36.990> ok<00:06:37.990> that's

00:06:38.249 --> 00:06:38.259 
the other side of the argument ok that's

00:06:38.259 --> 00:06:39.510 
the other side of the argument ok that's
the<00:06:38.380> end<00:06:38.410> of<00:06:38.620> this<00:06:38.740> episode<00:06:39.190> and<00:06:39.280> you<00:06:39.370> might

00:06:39.510 --> 00:06:39.520 
the end of this episode and you might

00:06:39.520 --> 00:06:40.890 
the end of this episode and you might
feel<00:06:39.759> like<00:06:39.789> we've<00:06:40.120> raised<00:06:40.419> more<00:06:40.720> questions

00:06:40.890 --> 00:06:40.900 
feel like we've raised more questions

00:06:40.900 --> 00:06:42.870 
feel like we've raised more questions
than<00:06:41.319> we've<00:06:41.440> really<00:06:41.710> answered<00:06:42.039> well<00:06:42.669> don't

00:06:42.870 --> 00:06:42.880 
than we've really answered well don't

00:06:42.880 --> 00:06:45.240 
than we've really answered well don't
panic<00:06:43.030> this<00:06:43.930> is<00:06:44.050> just<00:06:44.199> episode<00:06:44.560> number<00:06:44.710> 2<00:06:45.009> and

00:06:45.240 --> 00:06:45.250 
panic this is just episode number 2 and

00:06:45.250 --> 00:06:47.370 
panic this is just episode number 2 and
there's<00:06:45.370> gonna<00:06:45.460> be<00:06:45.639> more<00:06:45.819> to<00:06:45.909> come<00:06:46.120> in<00:06:46.509> future

00:06:47.370 --> 00:06:47.380 
there's gonna be more to come in future

00:06:47.380 --> 00:06:49.230 
there's gonna be more to come in future
episodes<00:06:47.979> we'll<00:06:48.220> continue<00:06:48.520> to<00:06:48.789> try<00:06:48.940> and<00:06:49.060> build

00:06:49.230 --> 00:06:49.240 
episodes we'll continue to try and build

00:06:49.240 --> 00:06:51.059 
episodes we'll continue to try and build
up<00:06:49.419> the<00:06:49.569> toolbox<00:06:50.020> of<00:06:50.169> ideas<00:06:50.470> and<00:06:50.650> frameworks

00:06:51.059 --> 00:06:51.069 
up the toolbox of ideas and frameworks

00:06:51.069 --> 00:06:52.469 
up the toolbox of ideas and frameworks
and<00:06:51.190> hopefully<00:06:51.490> empower<00:06:51.789> you<00:06:51.970> to<00:06:52.000> make<00:06:52.180> more

00:06:52.469 --> 00:06:52.479 
and hopefully empower you to make more

00:06:52.479 --> 00:06:55.399 
and hopefully empower you to make more
sense<00:06:52.720> of<00:06:52.990> global<00:06:53.530> health<00:06:53.650> ethical<00:06:54.099> issues

00:06:55.399 --> 00:06:55.409 
sense of global health ethical issues

00:06:55.409 --> 00:06:57.749 
sense of global health ethical issues
tune<00:06:56.409> in<00:06:56.500> to<00:06:56.590> the<00:06:56.680> next<00:06:56.710> video<00:06:56.949> and<00:06:57.310> we'll<00:06:57.610> take

00:06:57.749 --> 00:06:57.759 
tune in to the next video and we'll take

00:06:57.759 --> 00:06:59.149 
tune in to the next video and we'll take
a<00:06:57.819> closer<00:06:58.060> look<00:06:58.210> at<00:06:58.419> some<00:06:58.509> of<00:06:58.569> these<00:06:58.720> issues

00:06:59.149 --> 00:06:59.159 
a closer look at some of these issues

00:06:59.159 --> 00:07:01.110 
a closer look at some of these issues
remember<00:07:00.159> that<00:07:00.280> if<00:07:00.370> you<00:07:00.460> subscribe<00:07:00.909> to<00:07:00.940> this

00:07:01.110 --> 00:07:01.120 
remember that if you subscribe to this

00:07:01.120 --> 00:07:02.490 
remember that if you subscribe to this
channel<00:07:01.419> you'll<00:07:01.630> get<00:07:01.720> an<00:07:01.870> email<00:07:02.020> alert

00:07:02.490 --> 00:07:02.500 
channel you'll get an email alert

00:07:02.500 --> 00:07:04.920 
channel you'll get an email alert
whenever<00:07:02.830> a<00:07:02.889> new<00:07:02.979> video<00:07:03.310> is<00:07:03.460> posted<00:07:03.610> as<00:07:04.090> always

00:07:04.920 --> 00:07:04.930 
whenever a new video is posted as always

00:07:04.930 --> 00:07:06.029 
whenever a new video is posted as always
thank<00:07:05.139> you<00:07:05.199> very<00:07:05.349> much<00:07:05.530> to<00:07:05.650> the<00:07:05.710> happy<00:07:05.919> hour

00:07:06.029 --> 00:07:06.039 
thank you very much to the happy hour

00:07:06.039 --> 00:07:07.110 
thank you very much to the happy hour
end<00:07:06.280> god<00:07:06.400> comment<00:07:06.699> wrapping<00:07:07.030> with

00:07:07.110 --> 00:07:07.120 
end god comment wrapping with

00:07:07.120 --> 00:07:09.690 
end god comment wrapping with
presentation

