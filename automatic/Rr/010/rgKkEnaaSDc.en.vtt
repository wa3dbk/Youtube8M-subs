WEBVTT
Kind: captions
Language: en

00:00:00.050 --> 00:00:02.929 

we<00:00:01.050> present<00:00:01.290> a<00:00:01.589> 3d<00:00:02.070> facial<00:00:02.490> performance

00:00:02.929 --> 00:00:02.939 
we present a 3d facial performance

00:00:02.939 --> 00:00:05.300 
we present a 3d facial performance
capture<00:00:03.270> an<00:00:03.510> animation<00:00:03.870> system<00:00:04.380> designed<00:00:05.190> for

00:00:05.300 --> 00:00:05.310 
capture an animation system designed for

00:00:05.310 --> 00:00:07.610 
capture an animation system designed for
virtual<00:00:05.790> reality<00:00:06.240> applications<00:00:06.470> using<00:00:07.470> head

00:00:07.610 --> 00:00:07.620 
virtual reality applications using head

00:00:07.620 --> 00:00:10.990 
virtual reality applications using head
mounted<00:00:07.919> displays

00:00:10.990 --> 00:00:11.000 

00:00:11.000 --> 00:00:13.600 

our<00:00:11.690> goal<00:00:11.930> is<00:00:12.200> to<00:00:12.410> enable<00:00:12.740> 3d<00:00:13.130> facial

00:00:13.600 --> 00:00:13.610 
our goal is to enable 3d facial

00:00:13.610 --> 00:00:15.520 
our goal is to enable 3d facial
performance-driven<00:00:14.120> animation<00:00:14.810> in<00:00:14.990> real

00:00:15.520 --> 00:00:15.530 
performance-driven animation in real

00:00:15.530 --> 00:00:18.900 
performance-driven animation in real
time<00:00:15.800> for<00:00:16.370> face<00:00:16.580> to<00:00:16.730> face<00:00:16.880> interactions

00:00:18.900 --> 00:00:18.910 
time for face to face interactions

00:00:18.910 --> 00:00:20.890 
time for face to face interactions
traditional<00:00:19.910> facial<00:00:20.240> performance<00:00:20.660> capture

00:00:20.890 --> 00:00:20.900 
traditional facial performance capture

00:00:20.900 --> 00:00:22.870 
traditional facial performance capture
methods<00:00:21.200> are<00:00:21.290> not<00:00:21.470> viable<00:00:21.860> because<00:00:22.490> a<00:00:22.520> user's

00:00:22.870 --> 00:00:22.880 
methods are not viable because a user's

00:00:22.880 --> 00:00:24.640 
methods are not viable because a user's
face<00:00:23.090> is<00:00:23.330> largely<00:00:23.689> occluded<00:00:24.200> by<00:00:24.350> the

00:00:24.640 --> 00:00:24.650 
face is largely occluded by the

00:00:24.650 --> 00:00:27.190 
face is largely occluded by the
head-mounted<00:00:24.860> display<00:00:25.660> our<00:00:26.660> system<00:00:27.050> is

00:00:27.190 --> 00:00:27.200 
head-mounted display our system is

00:00:27.200 --> 00:00:28.690 
head-mounted display our system is
capable<00:00:27.380> of<00:00:27.710> capturing<00:00:27.950> upper<00:00:28.520> face

00:00:28.690 --> 00:00:28.700 
capable of capturing upper face

00:00:28.700 --> 00:00:31.089 
capable of capturing upper face
expressions<00:00:29.270> in<00:00:29.419> the<00:00:29.779> occluded<00:00:30.140> area<00:00:30.410> as<00:00:30.590> well

00:00:31.089 --> 00:00:31.099 
expressions in the occluded area as well

00:00:31.099 --> 00:00:32.440 
expressions in the occluded area as well
as<00:00:31.250> lower<00:00:31.400> face<00:00:31.640> expressions<00:00:32.150> and<00:00:32.270> mouth

00:00:32.440 --> 00:00:32.450 
as lower face expressions and mouth

00:00:32.450 --> 00:00:35.200 
as lower face expressions and mouth
movement<00:00:33.220> we<00:00:34.220> can<00:00:34.400> track<00:00:34.579> a<00:00:34.610> variety<00:00:35.180> of

00:00:35.200 --> 00:00:35.210 
movement we can track a variety of

00:00:35.210 --> 00:00:37.620 
movement we can track a variety of
expressions<00:00:35.510> including<00:00:36.170> smiles<00:00:36.550> crowns

00:00:37.620 --> 00:00:37.630 
expressions including smiles crowns

00:00:37.630 --> 00:00:40.450 
expressions including smiles crowns
eyebrow<00:00:38.630> raises<00:00:39.020> and<00:00:39.230> ice<00:00:39.560> winds<00:00:39.829> with

00:00:40.450 --> 00:00:40.460 
eyebrow raises and ice winds with

00:00:40.460 --> 00:00:45.100 
eyebrow raises and ice winds with
arbitrary<00:00:40.910> head<00:00:41.180> motion<00:00:43.690> our<00:00:44.690> facial

00:00:45.100 --> 00:00:45.110 
arbitrary head motion our facial

00:00:45.110 --> 00:00:46.810 
arbitrary head motion our facial
tracking<00:00:45.320> system<00:00:45.800> consists<00:00:46.400> of<00:00:46.460> novel

00:00:46.810 --> 00:00:46.820 
tracking system consists of novel

00:00:46.820 --> 00:00:49.870 
tracking system consists of novel
hardware<00:00:47.150> and<00:00:47.360> software<00:00:47.510> the<00:00:48.880> hardware

00:00:49.870 --> 00:00:49.880 
hardware and software the hardware

00:00:49.880 --> 00:00:52.090 
hardware and software the hardware
component<00:00:50.630> is<00:00:50.930> based<00:00:51.230> on<00:00:51.470> an<00:00:51.590> existing

00:00:52.090 --> 00:00:52.100 
component is based on an existing

00:00:52.100 --> 00:00:54.640 
component is based on an existing
virtual<00:00:52.400> reality<00:00:53.120> head-mounted<00:00:53.390> display<00:00:53.930> the

00:00:54.640 --> 00:00:54.650 
virtual reality head-mounted display the

00:00:54.650 --> 00:00:58.300 
virtual reality head-mounted display the
oculus<00:00:55.070> rift<00:00:55.160> dk2<00:00:56.590> the<00:00:57.590> head<00:00:57.740> mounted<00:00:58.010> display

00:00:58.300 --> 00:00:58.310 
oculus rift dk2 the head mounted display

00:00:58.310 --> 00:01:01.180 
oculus rift dk2 the head mounted display
is<00:00:58.580> augmented<00:00:59.240> with<00:00:59.690> a<00:00:59.720> short-range<00:01:00.290> RGB<00:01:00.950> def

00:01:01.180 --> 00:01:01.190 
is augmented with a short-range RGB def

00:01:01.190 --> 00:01:03.250 
is augmented with a short-range RGB def
camera<00:01:01.580> attached<00:01:02.240> such<00:01:02.630> that<00:01:02.810> it<00:01:02.900> can<00:01:03.020> record

00:01:03.250 --> 00:01:03.260 
camera attached such that it can record

00:01:03.260 --> 00:01:06.010 
camera attached such that it can record
the<00:01:03.410> lower<00:01:03.560> face<00:01:03.830> and<00:01:04.239> eight<00:01:05.239> strain<00:01:05.659> gauges

00:01:06.010 --> 00:01:06.020 
the lower face and eight strain gauges

00:01:06.020 --> 00:01:07.749 
the lower face and eight strain gauges
embedded<00:01:06.530> in<00:01:06.650> the<00:01:06.799> foam<00:01:06.950> liner<00:01:07.250> of<00:01:07.490> the<00:01:07.610> head

00:01:07.749 --> 00:01:07.759 
embedded in the foam liner of the head

00:01:07.759 --> 00:01:10.480 
embedded in the foam liner of the head
mounted<00:01:07.970> display<00:01:08.860> our<00:01:09.860> approach<00:01:10.220> leverages

00:01:10.480 --> 00:01:10.490 
mounted display our approach leverages

00:01:10.490 --> 00:01:12.880 
mounted display our approach leverages
the<00:01:10.880> contact<00:01:11.329> between<00:01:11.630> the<00:01:11.840> face<00:01:12.110> and<00:01:12.140> the<00:01:12.439> hmd

00:01:12.880 --> 00:01:12.890 
the contact between the face and the hmd

00:01:12.890 --> 00:01:16.090 
the contact between the face and the hmd
as<00:01:13.100> a<00:01:13.340> source<00:01:13.640> of<00:01:13.729> additional<00:01:13.909> data

00:01:16.090 --> 00:01:16.100 
as a source of additional data

00:01:16.100 --> 00:01:19.130 
as a source of additional data
here<00:01:17.100> you<00:01:17.579> can<00:01:17.729> see<00:01:17.939> in<00:01:18.090> blue<00:01:18.240> the<00:01:18.810> camera

00:01:19.130 --> 00:01:19.140 
here you can see in blue the camera

00:01:19.140 --> 00:01:21.139 
here you can see in blue the camera
mount<00:01:19.320> attached<00:01:19.649> to<00:01:19.799> the<00:01:19.890> headset<00:01:20.250> and<00:01:20.460> in

00:01:21.139 --> 00:01:21.149 
mount attached to the headset and in

00:01:21.149 --> 00:01:23.240 
mount attached to the headset and in
yellow<00:01:21.329> the<00:01:22.110> approximate<00:01:22.590> position<00:01:22.920> of<00:01:23.070> the

00:01:23.240 --> 00:01:23.250 
yellow the approximate position of the

00:01:23.250 --> 00:01:24.740 
yellow the approximate position of the
eight<00:01:23.369> strain<00:01:23.729> gauges<00:01:24.060> on<00:01:24.240> the<00:01:24.270> foam<00:01:24.479> lining

00:01:24.740 --> 00:01:24.750 
eight strain gauges on the foam lining

00:01:24.750 --> 00:01:29.870 
eight strain gauges on the foam lining
of<00:01:24.990> the<00:01:25.140> head<00:01:25.259> mounted<00:01:25.500> display<00:01:28.520> the<00:01:29.520> software

00:01:29.870 --> 00:01:29.880 
of the head mounted display the software

00:01:29.880 --> 00:01:31.490 
of the head mounted display the software
component<00:01:30.330> is<00:01:30.420> the<00:01:30.720> novel<00:01:31.020> approach<00:01:31.320> to

00:01:31.490 --> 00:01:31.500 
component is the novel approach to

00:01:31.500 --> 00:01:32.960 
component is the novel approach to
facial<00:01:31.770> performance<00:01:32.159> capture<00:01:32.369> that

00:01:32.960 --> 00:01:32.970 
facial performance capture that

00:01:32.970 --> 00:01:34.450 
facial performance capture that
integrates<00:01:33.390> two<00:01:33.630> sources<00:01:33.960> of<00:01:34.140> information

00:01:34.450 --> 00:01:34.460 
integrates two sources of information

00:01:34.460 --> 00:01:36.590 
integrates two sources of information
optical<00:01:35.460> data<00:01:35.580> and<00:01:35.880> strain<00:01:36.420> gauge

00:01:36.590 --> 00:01:36.600 
optical data and strain gauge

00:01:36.600 --> 00:01:39.410 
optical data and strain gauge
measurements<00:01:37.610> our<00:01:38.610> facial<00:01:39.030> performance

00:01:39.410 --> 00:01:39.420 
measurements our facial performance

00:01:39.420 --> 00:01:41.539 
measurements our facial performance
capture<00:01:39.630> pipeline<00:01:40.170> consists<00:01:41.009> of<00:01:41.100> an<00:01:41.220> offline

00:01:41.539 --> 00:01:41.549 
capture pipeline consists of an offline

00:01:41.549 --> 00:01:43.789 
capture pipeline consists of an offline
training<00:01:41.939> stage<00:01:42.149> and<00:01:42.420> an<00:01:42.899> online<00:01:43.049> operation

00:01:43.789 --> 00:01:43.799 
training stage and an online operation

00:01:43.799 --> 00:01:49.490 
training stage and an online operation
stage<00:01:45.530> in<00:01:47.119> the<00:01:48.119> training<00:01:48.420> phase<00:01:48.600> we<00:01:49.140> remove

00:01:49.490 --> 00:01:49.500 
stage in the training phase we remove

00:01:49.500 --> 00:01:52.219 
stage in the training phase we remove
the<00:01:49.740> hmd<00:01:50.220> cover<00:01:50.520> and<00:01:50.759> display<00:01:51.119> and<00:01:51.390> use<00:01:51.990> depth

00:01:52.219 --> 00:01:52.229 
the hmd cover and display and use depth

00:01:52.229 --> 00:01:53.870 
the hmd cover and display and use depth
data<00:01:52.440> to<00:01:52.709> track<00:01:52.890> the<00:01:52.979> user's<00:01:53.250> entire<00:01:53.580> face

00:01:53.870 --> 00:01:53.880 
data to track the user's entire face

00:01:53.880 --> 00:01:55.430 
data to track the user's entire face
while<00:01:54.420> wearing<00:01:54.720> just<00:01:54.899> the<00:01:55.020> foam<00:01:55.170> liner

00:01:55.430 --> 00:01:55.440 
while wearing just the foam liner

00:01:55.440 --> 00:01:58.580 
while wearing just the foam liner
containing<00:01:55.920> the<00:01:55.979> strain<00:01:56.190> gauges<00:01:57.170> we<00:01:58.170> train<00:01:58.409> a

00:01:58.580 --> 00:01:58.590 
containing the strain gauges we train a

00:01:58.590 --> 00:02:00.469 
containing the strain gauges we train a
linear<00:01:58.860> regression<00:01:59.340> model<00:01:59.670> to<00:01:59.819> map<00:02:00.270> the

00:02:00.469 --> 00:02:00.479 
linear regression model to map the

00:02:00.479 --> 00:02:01.850 
linear regression model to map the
strain<00:02:00.720> signals<00:02:01.140> and<00:02:01.319> the<00:02:01.470> blend<00:02:01.679> shape

00:02:01.850 --> 00:02:01.860 
strain signals and the blend shape

00:02:01.860 --> 00:02:03.559 
strain signals and the blend shape
coefficients<00:02:02.429> from<00:02:02.819> the<00:02:02.909> unincluded<00:02:03.270> lower

00:02:03.559 --> 00:02:03.569 
coefficients from the unincluded lower

00:02:03.569 --> 00:02:05.570 
coefficients from the unincluded lower
region<00:02:03.929> of<00:02:04.049> the<00:02:04.110> face<00:02:04.289> to<00:02:05.039> the<00:02:05.159> blend<00:02:05.369> shape

00:02:05.570 --> 00:02:05.580 
region of the face to the blend shape

00:02:05.580 --> 00:02:07.130 
region of the face to the blend shape
coefficients<00:02:06.149> for<00:02:06.390> the<00:02:06.450> occluded<00:02:06.929> upper

00:02:07.130 --> 00:02:07.140 
coefficients for the occluded upper

00:02:07.140 --> 00:02:10.729 
coefficients for the occluded upper
region<00:02:08.209> to<00:02:09.209> account<00:02:09.479> for<00:02:09.569> shifts<00:02:10.080> in<00:02:10.200> hmd

00:02:10.729 --> 00:02:10.739 
region to account for shifts in hmd

00:02:10.739 --> 00:02:12.890 
region to account for shifts in hmd
position<00:02:11.130> between<00:02:11.430> uses<00:02:11.790> we<00:02:12.390> introduce<00:02:12.780> a

00:02:12.890 --> 00:02:12.900 
position between uses we introduce a

00:02:12.900 --> 00:02:14.240 
position between uses we introduce a
calibration<00:02:13.260> step<00:02:13.620> at<00:02:13.799> the<00:02:13.860> beginning<00:02:13.950> of

00:02:14.240 --> 00:02:14.250 
calibration step at the beginning of

00:02:14.250 --> 00:02:16.460 
calibration step at the beginning of
each<00:02:14.340> online<00:02:14.610> operation<00:02:15.380> for<00:02:16.380> this

00:02:16.460 --> 00:02:16.470 
each online operation for this

00:02:16.470 --> 00:02:18.380 
each online operation for this
calibration<00:02:17.040> the<00:02:17.459> user<00:02:17.640> repeats<00:02:18.060> a<00:02:18.150> small

00:02:18.380 --> 00:02:18.390 
calibration the user repeats a small

00:02:18.390 --> 00:02:25.970 
calibration the user repeats a small
subset<00:02:18.840> of<00:02:18.870> the<00:02:19.110> training<00:02:19.200> expressions

00:02:25.970 --> 00:02:25.980 

00:02:25.980 --> 00:02:28.740 

during<00:02:26.980> online<00:02:27.370> operation<00:02:27.910> we<00:02:28.390> track<00:02:28.629> the

00:02:28.740 --> 00:02:28.750 
during online operation we track the

00:02:28.750 --> 00:02:30.360 
during online operation we track the
lower<00:02:28.930> region<00:02:29.290> of<00:02:29.379> the<00:02:29.440> face<00:02:29.620> using<00:02:30.250> the

00:02:30.360 --> 00:02:30.370 
lower region of the face using the

00:02:30.370 --> 00:02:32.910 
lower region of the face using the
camera<00:02:30.760> mounted<00:02:31.090> on<00:02:31.180> the<00:02:31.269> HMD<00:02:31.750> these<00:02:32.650> lower

00:02:32.910 --> 00:02:32.920 
camera mounted on the HMD these lower

00:02:32.920 --> 00:02:35.040 
camera mounted on the HMD these lower
face<00:02:33.220> blend<00:02:33.580> shape<00:02:33.790> coefficients<00:02:34.420> and<00:02:34.569> the

00:02:35.040 --> 00:02:35.050 
face blend shape coefficients and the

00:02:35.050 --> 00:02:37.050 
face blend shape coefficients and the
upper<00:02:35.290> face<00:02:35.440> strain<00:02:35.769> signals<00:02:36.160> are<00:02:36.550> then<00:02:36.790> used

00:02:37.050 --> 00:02:37.060 
upper face strain signals are then used

00:02:37.060 --> 00:02:38.580 
upper face strain signals are then used
with<00:02:37.480> the<00:02:37.599> regression<00:02:37.900> model<00:02:38.290> to<00:02:38.410> determine

00:02:38.580 --> 00:02:38.590 
with the regression model to determine

00:02:38.590 --> 00:02:42.119 
with the regression model to determine
the<00:02:39.099> full<00:02:39.220> face<00:02:39.489> blend<00:02:39.760> shades<00:02:40.709> head<00:02:41.709> position

00:02:42.119 --> 00:02:42.129 
the full face blend shades head position

00:02:42.129 --> 00:02:43.680 
the full face blend shades head position
and<00:02:42.310> orientation<00:02:42.790> are<00:02:43.090> obtained

00:02:43.680 --> 00:02:43.690 
and orientation are obtained

00:02:43.690 --> 00:02:45.930 
and orientation are obtained
independently<00:02:44.349> from<00:02:44.769> the<00:02:44.920> external<00:02:45.250> oculus

00:02:45.930 --> 00:02:45.940 
independently from the external oculus

00:02:45.940 --> 00:02:49.050 
independently from the external oculus
rift<00:02:46.150> dk2<00:02:46.629> tracking<00:02:47.170> camera<00:02:47.879> because<00:02:48.879> the

00:02:49.050 --> 00:02:49.060 
rift dk2 tracking camera because the

00:02:49.060 --> 00:02:50.699 
rift dk2 tracking camera because the
face<00:02:49.239> tracking<00:02:49.660> camera<00:02:49.840> is<00:02:50.170> stationary

00:02:50.699 --> 00:02:50.709 
face tracking camera is stationary

00:02:50.709 --> 00:02:52.559 
face tracking camera is stationary
relative<00:02:50.920> to<00:02:51.220> the<00:02:51.310> headset<00:02:51.459> large<00:02:52.330> head

00:02:52.559 --> 00:02:52.569 
relative to the headset large head

00:02:52.569 --> 00:02:54.270 
relative to the headset large head
motion<00:02:52.720> does<00:02:53.170> not<00:02:53.319> degrade<00:02:53.470> facial<00:02:54.099> tracking

00:02:54.270 --> 00:02:54.280 
motion does not degrade facial tracking

00:02:54.280 --> 00:03:03.380 
motion does not degrade facial tracking
performance

00:03:03.380 --> 00:03:03.390 

00:03:03.390 --> 00:03:11.590 

you

00:03:11.590 --> 00:03:11.600 

00:03:11.600 --> 00:03:14.890 

as<00:03:11.660> expected<00:03:12.700> combining<00:03:13.700> the<00:03:13.910> RGB<00:03:14.300> depth<00:03:14.690> data

00:03:14.890 --> 00:03:14.900 
as expected combining the RGB depth data

00:03:14.900 --> 00:03:17.140 
as expected combining the RGB depth data
with<00:03:15.650> the<00:03:15.800> strain<00:03:16.040> signal<00:03:16.400> produces<00:03:16.910> the<00:03:17.030> best

00:03:17.140 --> 00:03:17.150 
with the strain signal produces the best

00:03:17.150 --> 00:03:22.510 
with the strain signal produces the best
full-face<00:03:17.720> results<00:03:20.740> the<00:03:21.740> calibration<00:03:22.280> step

00:03:22.510 --> 00:03:22.520 
full-face results the calibration step

00:03:22.520 --> 00:03:24.130 
full-face results the calibration step
improves<00:03:22.940> the<00:03:23.060> tracking<00:03:23.270> quality<00:03:23.750> and

00:03:24.130 --> 00:03:24.140 
improves the tracking quality and

00:03:24.140 --> 00:03:25.810 
improves the tracking quality and
ensures<00:03:24.410> greater<00:03:24.710> accuracy<00:03:25.100> in<00:03:25.460> the<00:03:25.550> final

00:03:25.810 --> 00:03:25.820 
ensures greater accuracy in the final

00:03:25.820 --> 00:03:27.310 
ensures greater accuracy in the final
blend<00:03:25.940> shapes<00:03:26.180> computed<00:03:26.750> for<00:03:26.900> the<00:03:26.990> occluded

00:03:27.310 --> 00:03:27.320 
blend shapes computed for the occluded

00:03:27.320 --> 00:03:36.520 
blend shapes computed for the occluded
region<00:03:27.590> of<00:03:27.680> the<00:03:27.770> face

00:03:36.520 --> 00:03:36.530 

00:03:36.530 --> 00:03:38.750 

various<00:03:37.530> avatars<00:03:37.830> can<00:03:38.220> be<00:03:38.310> used<00:03:38.520> at<00:03:38.640> our

00:03:38.750 --> 00:03:38.760 
various avatars can be used at our

00:03:38.760 --> 00:03:40.369 
various avatars can be used at our
system<00:03:39.060> for<00:03:39.330> face-to-face<00:03:39.510> interaction<00:03:40.260> in

00:03:40.369 --> 00:03:40.379 
system for face-to-face interaction in

00:03:40.379 --> 00:03:51.350 
system for face-to-face interaction in
VR

00:03:51.350 --> 00:03:51.360 

00:03:51.360 --> 00:04:22.440 

you

00:04:22.440 --> 00:04:22.450 

00:04:22.450 --> 00:04:24.510 

you

