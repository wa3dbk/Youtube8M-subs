WEBVTT
Kind: captions
Language: en

00:00:05.509 --> 00:00:08.360 

to<00:00:06.509> create<00:00:06.779> a<00:00:06.870> new<00:00:07.049> type<00:00:07.289> of<00:00:07.319> VR<00:00:07.740> experience

00:00:08.360 --> 00:00:08.370 
to create a new type of VR experience

00:00:08.370 --> 00:00:10.640 
to create a new type of VR experience
that<00:00:08.640> transcends<00:00:09.240> typical<00:00:09.809> polygonal<00:00:10.259> models

00:00:10.640 --> 00:00:10.650 
that transcends typical polygonal models

00:00:10.650 --> 00:00:12.829 
that transcends typical polygonal models
we<00:00:11.190> have<00:00:11.309> created<00:00:11.759> a<00:00:11.879> system<00:00:12.210> to<00:00:12.330> capture<00:00:12.540> and

00:00:12.829 --> 00:00:12.839 
we have created a system to capture and

00:00:12.839 --> 00:00:14.720 
we have created a system to capture and
render<00:00:13.110> animated<00:00:13.679> stop-motion<00:00:14.280> characters

00:00:14.720 --> 00:00:14.730 
render animated stop-motion characters

00:00:14.730 --> 00:00:17.630 
render animated stop-motion characters
for<00:00:15.000> a<00:00:15.030> real-time<00:00:15.509> virtual<00:00:15.780> experience<00:00:16.640> our

00:00:17.630 --> 00:00:17.640 
for a real-time virtual experience our

00:00:17.640 --> 00:00:20.570 
for a real-time virtual experience our
system<00:00:18.029> captures<00:00:18.449> a<00:00:18.480> ring<00:00:18.810> of<00:00:19.019> 360<00:00:19.920> images<00:00:20.340> for

00:00:20.570 --> 00:00:20.580 
system captures a ring of 360 images for

00:00:20.580 --> 00:00:22.339 
system captures a ring of 360 images for
each<00:00:20.670> frame<00:00:21.029> of<00:00:21.180> an<00:00:21.269> animated<00:00:21.630> sequence<00:00:21.930> and

00:00:22.339 --> 00:00:22.349 
each frame of an animated sequence and

00:00:22.349 --> 00:00:24.710 
each frame of an animated sequence and
is<00:00:22.590> integrated<00:00:22.890> with<00:00:23.279> dragon<00:00:23.699> frame<00:00:23.939> to<00:00:24.510> allow

00:00:24.710 --> 00:00:24.720 
is integrated with dragon frame to allow

00:00:24.720 --> 00:00:26.660 
is integrated with dragon frame to allow
animators<00:00:25.289> to<00:00:25.439> time<00:00:25.710> scrub<00:00:26.070> and<00:00:26.250> determine

00:00:26.660 --> 00:00:26.670 
animators to time scrub and determine

00:00:26.670 --> 00:00:30.109 
animators to time scrub and determine
best<00:00:26.880> poses<00:00:28.009> we<00:00:29.009> then<00:00:29.189> employ<00:00:29.519> image<00:00:29.880> based

00:00:30.109 --> 00:00:30.119 
best poses we then employ image based

00:00:30.119 --> 00:00:32.269 
best poses we then employ image based
rendering<00:00:30.240> techniques<00:00:30.960> in<00:00:31.560> this<00:00:31.830> case<00:00:32.040> we

00:00:32.269 --> 00:00:32.279 
rendering techniques in this case we

00:00:32.279 --> 00:00:34.040 
rendering techniques in this case we
billboard<00:00:32.730> left<00:00:33.060> and<00:00:33.240> right<00:00:33.360> images<00:00:33.780> that

00:00:34.040 --> 00:00:34.050 
billboard left and right images that

00:00:34.050 --> 00:00:35.810 
billboard left and right images that
follow<00:00:34.260> the<00:00:34.500> user<00:00:34.620> and<00:00:34.980> correspond<00:00:35.579> to<00:00:35.670> the

00:00:35.810 --> 00:00:35.820 
follow the user and correspond to the

00:00:35.820 --> 00:00:37.610 
follow the user and correspond to the
correct<00:00:36.120> horizontal<00:00:36.719> camera<00:00:37.020> angle<00:00:37.350> for<00:00:37.500> each

00:00:37.610 --> 00:00:37.620 
correct horizontal camera angle for each

00:00:37.620 --> 00:00:41.389 
correct horizontal camera angle for each
eye<00:00:39.200> the<00:00:40.200> results<00:00:40.590> are<00:00:40.710> so<00:00:40.860> realistically

00:00:41.389 --> 00:00:41.399 
eye the results are so realistically

00:00:41.399 --> 00:00:43.100 
eye the results are so realistically
captivating<00:00:42.120> as<00:00:42.239> they<00:00:42.360> maintain<00:00:42.570> the<00:00:42.989> high

00:00:43.100 --> 00:00:43.110 
captivating as they maintain the high

00:00:43.110 --> 00:00:45.110 
captivating as they maintain the high
level<00:00:43.469> of<00:00:43.559> geometric<00:00:44.100> detail<00:00:44.460> and<00:00:44.730> surface

00:00:45.110 --> 00:00:45.120 
level of geometric detail and surface

00:00:45.120 --> 00:00:46.610 
level of geometric detail and surface
reflectance<00:00:45.600> properties<00:00:46.140> of<00:00:46.320> the<00:00:46.440> rich

00:00:46.610 --> 00:00:46.620 
reflectance properties of the rich

00:00:46.620 --> 00:00:49.549 
reflectance properties of the rich
handcrafted<00:00:47.309> puppets<00:00:48.020> specular<00:00:49.020> reflection

00:00:49.549 --> 00:00:49.559 
handcrafted puppets specular reflection

00:00:49.559 --> 00:00:51.709 
handcrafted puppets specular reflection
iridescent<00:00:50.280> subsurface<00:00:50.820> scattering<00:00:51.030> even

00:00:51.709 --> 00:00:51.719 
iridescent subsurface scattering even

00:00:51.719 --> 00:00:53.360 
iridescent subsurface scattering even
the<00:00:51.809> apparent<00:00:52.260> geometry<00:00:52.770> from<00:00:52.949> a<00:00:53.039> threat<00:00:53.280> of

00:00:53.360 --> 00:00:53.370 
the apparent geometry from a threat of

00:00:53.370 --> 00:00:56.450 
the apparent geometry from a threat of
yarn<00:00:53.579> can<00:00:53.850> be<00:00:53.969> seen<00:00:54.920> part<00:00:55.920> pipeline<00:00:56.280> is

00:00:56.450 --> 00:00:56.460 
yarn can be seen part pipeline is

00:00:56.460 --> 00:00:58.220 
yarn can be seen part pipeline is
computationally<00:00:57.210> very<00:00:57.239> light<00:00:57.719> and<00:00:57.899> has<00:00:57.989> been

00:00:58.220 --> 00:00:58.230 
computationally very light and has been

00:00:58.230 --> 00:01:01.099 
computationally very light and has been
integrated<00:00:58.770> within<00:00:59.070> unity<00:00:59.430> 3d<00:01:00.109> stop-motion

00:01:01.099 --> 00:01:01.109 
integrated within unity 3d stop-motion

00:01:01.109 --> 00:01:02.810 
integrated within unity 3d stop-motion
artists<00:01:01.440> can<00:01:01.590> quickly<00:01:01.800> create<00:01:02.039> rich<00:01:02.399> models

00:01:02.810 --> 00:01:02.820 
artists can quickly create rich models

00:01:02.820 --> 00:01:04.189 
artists can quickly create rich models
that<00:01:03.000> are<00:01:03.090> easily<00:01:03.269> dropped<00:01:03.690> into<00:01:03.989> traditional

00:01:04.189 --> 00:01:04.199 
that are easily dropped into traditional

00:01:04.199 --> 00:01:05.870 
that are easily dropped into traditional
virtual<00:01:04.830> environments<00:01:05.370> running<00:01:05.640> in<00:01:05.730> real

00:01:05.870 --> 00:01:05.880 
virtual environments running in real

00:01:05.880 --> 00:01:09.050 
virtual environments running in real
time<00:01:06.180> an<00:01:07.100> important<00:01:08.100> result<00:01:08.370> of<00:01:08.490> this<00:01:08.640> work<00:01:08.820> is

00:01:09.050 --> 00:01:09.060 
time an important result of this work is

00:01:09.060 --> 00:01:11.120 
time an important result of this work is
experiential<00:01:09.600> we<00:01:10.560> have<00:01:10.680> found<00:01:10.860> that<00:01:10.980> these

00:01:11.120 --> 00:01:11.130 
experiential we have found that these

00:01:11.130 --> 00:01:12.860 
experiential we have found that these
rich<00:01:11.340> renderings<00:01:11.790> create<00:01:12.150> a<00:01:12.180> magical<00:01:12.660> place

00:01:12.860 --> 00:01:12.870 
rich renderings create a magical place

00:01:12.870 --> 00:01:14.990 
rich renderings create a magical place
that<00:01:13.110> immediately<00:01:13.530> surrounds<00:01:14.100> a<00:01:14.160> user<00:01:14.370> we

00:01:14.990 --> 00:01:15.000 
that immediately surrounds a user we

00:01:15.000 --> 00:01:18.110 
that immediately surrounds a user we
call<00:01:15.240> this<00:01:15.420> place<00:01:15.660> near-field<00:01:16.320> vr<00:01:17.120> this

00:01:18.110 --> 00:01:18.120 
call this place near-field vr this

00:01:18.120 --> 00:01:19.700 
call this place near-field vr this
result<00:01:18.510> correlates<00:01:18.990> with<00:01:19.170> findings<00:01:19.590> in

00:01:19.700 --> 00:01:19.710 
result correlates with findings in

00:01:19.710 --> 00:01:21.830 
result correlates with findings in
perceptual<00:01:20.220> psychology<00:01:20.550> users<00:01:21.330> make<00:01:21.540> sense

00:01:21.830 --> 00:01:21.840 
perceptual psychology users make sense

00:01:21.840 --> 00:01:23.450 
perceptual psychology users make sense
of<00:01:21.990> space<00:01:22.290> through<00:01:22.530> multiple<00:01:22.950> cues<00:01:23.220> including

00:01:23.450 --> 00:01:23.460 
of space through multiple cues including

00:01:23.460 --> 00:01:26.270 
of space through multiple cues including
occlusion<00:01:24.270> parallax<00:01:25.050> texture<00:01:25.710> gradients<00:01:26.130> and

00:01:26.270 --> 00:01:26.280 
occlusion parallax texture gradients and

00:01:26.280 --> 00:01:28.820 
occlusion parallax texture gradients and
material<00:01:26.760> properties<00:01:27.470> because<00:01:28.470> virtual

00:01:28.820 --> 00:01:28.830 
material properties because virtual

00:01:28.830 --> 00:01:30.410 
material properties because virtual
reality<00:01:29.190> allows<00:01:29.430> for<00:01:29.700> lateral<00:01:30.060> head<00:01:30.210> motion

00:01:30.410 --> 00:01:30.420 
reality allows for lateral head motion

00:01:30.420 --> 00:01:32.330 
reality allows for lateral head motion
and<00:01:30.780> stereoscopic<00:01:31.500> vision<00:01:31.650> rendering

00:01:32.330 --> 00:01:32.340 
and stereoscopic vision rendering

00:01:32.340 --> 00:01:34.040 
and stereoscopic vision rendering
techniques<00:01:32.730> must<00:01:32.910> pay<00:01:33.120> attention<00:01:33.240> to<00:01:33.720> visual

00:01:34.040 --> 00:01:34.050 
techniques must pay attention to visual

00:01:34.050 --> 00:01:35.720 
techniques must pay attention to visual
details<00:01:34.440> which<00:01:34.680> inform<00:01:35.100> these<00:01:35.250> perceptual

00:01:35.720 --> 00:01:35.730 
details which inform these perceptual

00:01:35.730 --> 00:01:37.460 
details which inform these perceptual
mechanisms<00:01:36.330> many<00:01:36.810> of<00:01:36.930> which<00:01:37.080> are<00:01:37.200> at<00:01:37.290> their

00:01:37.460 --> 00:01:37.470 
mechanisms many of which are at their

00:01:37.470 --> 00:01:40.850 
mechanisms many of which are at their
strongest<00:01:37.680> in<00:01:38.100> the<00:01:38.310> near<00:01:38.460> field<00:01:39.860> these

00:01:40.850 --> 00:01:40.860 
strongest in the near field these

00:01:40.860 --> 00:01:42.650 
strongest in the near field these
metallic<00:01:41.430> test<00:01:41.670> blocks<00:01:41.970> highlight<00:01:42.480> the

00:01:42.650 --> 00:01:42.660 
metallic test blocks highlight the

00:01:42.660 --> 00:01:44.360 
metallic test blocks highlight the
perspective<00:01:43.170> distortion<00:01:43.290> which<00:01:43.800> occurs<00:01:44.130> due

00:01:44.360 --> 00:01:44.370 
perspective distortion which occurs due

00:01:44.370 --> 00:01:46.130 
perspective distortion which occurs due
to<00:01:44.400> mismatch<00:01:44.880> between<00:01:45.090> the<00:01:45.330> captured<00:01:45.840> cameras

00:01:46.130 --> 00:01:46.140 
to mismatch between the captured cameras

00:01:46.140 --> 00:01:48.770 
to mismatch between the captured cameras
position<00:01:46.590> and<00:01:46.770> that<00:01:47.190> of<00:01:47.310> the<00:01:47.400> moving<00:01:47.700> user<00:01:47.910> the

00:01:48.770 --> 00:01:48.780 
position and that of the moving user the

00:01:48.780 --> 00:01:50.630 
position and that of the moving user the
left<00:01:49.020> block<00:01:49.260> uses<00:01:49.800> viewpoints<00:01:50.370> that<00:01:50.550> are

00:01:50.630 --> 00:01:50.640 
left block uses viewpoints that are

00:01:50.640 --> 00:01:52.970 
left block uses viewpoints that are
behind<00:01:50.790> the<00:01:51.150> user<00:01:51.300> the<00:01:51.960> right<00:01:52.170> block<00:01:52.410> employs

00:01:52.970 --> 00:01:52.980 
behind the user the right block employs

00:01:52.980 --> 00:01:55.040 
behind the user the right block employs
rebuild<00:01:53.430> horizontal<00:01:54.030> pixels<00:01:54.240> to<00:01:54.570> synthesize

00:01:55.040 --> 00:01:55.050 
rebuild horizontal pixels to synthesize

00:01:55.050 --> 00:01:57.680 
rebuild horizontal pixels to synthesize
the<00:01:55.230> correct<00:01:55.530> camera<00:01:55.920> viewpoint<00:01:56.510> note<00:01:57.510> the

00:01:57.680 --> 00:01:57.690 
the correct camera viewpoint note the

00:01:57.690 --> 00:01:59.450 
the correct camera viewpoint note the
incorrect<00:01:58.110> vertical<00:01:58.590> parallax<00:01:59.070> and<00:01:59.310> the

00:01:59.450 --> 00:01:59.460 
incorrect vertical parallax and the

00:01:59.460 --> 00:02:01.480 
incorrect vertical parallax and the
perspective<00:02:00.000> difference<00:02:00.390> on<00:02:00.570> the<00:02:00.690> deep<00:02:00.870> holes

00:02:01.480 --> 00:02:01.490 
perspective difference on the deep holes

00:02:01.490 --> 00:02:03.440 
perspective difference on the deep holes
but<00:02:02.490> if<00:02:02.610> we<00:02:02.730> leave<00:02:02.910> behind<00:02:03.090> these

00:02:03.440 --> 00:02:03.450 
but if we leave behind these

00:02:03.450 --> 00:02:05.120 
but if we leave behind these
oppressively<00:02:03.900> euclidean<00:02:04.560> blocks<00:02:04.860> for

00:02:05.120 --> 00:02:05.130 
oppressively euclidean blocks for

00:02:05.130 --> 00:02:06.680 
oppressively euclidean blocks for
organic<00:02:05.550> shapes<00:02:05.790> the<00:02:06.060> distortion<00:02:06.540> is<00:02:06.660> less

00:02:06.680 --> 00:02:06.690 
organic shapes the distortion is less

00:02:06.690 --> 00:02:08.839 
organic shapes the distortion is less
distinct<00:02:07.380> the<00:02:08.069> difference<00:02:08.399> between<00:02:08.580> the

00:02:08.839 --> 00:02:08.849 
distinct the difference between the

00:02:08.849 --> 00:02:10.550 
distinct the difference between the
simple<00:02:09.179> texture<00:02:09.570> model<00:02:09.810> on<00:02:09.930> the<00:02:10.050> left<00:02:10.080> and<00:02:10.500> the

00:02:10.550 --> 00:02:10.560 
simple texture model on the left and the

00:02:10.560 --> 00:02:12.080 
simple texture model on the left and the
reeben<00:02:10.860> model<00:02:11.220> on<00:02:11.310> the<00:02:11.430> right<00:02:11.610> is<00:02:11.820> only

00:02:12.080 --> 00:02:12.090 
reeben model on the right is only

00:02:12.090 --> 00:02:14.360 
reeben model on the right is only
noticed<00:02:12.510> on<00:02:12.690> close<00:02:12.990> inspection<00:02:13.170> and<00:02:14.040> isn't

00:02:14.360 --> 00:02:14.370 
noticed on close inspection and isn't

00:02:14.370 --> 00:02:15.180 
noticed on close inspection and isn't
primarily<00:02:14.550> fell

00:02:15.180 --> 00:02:15.190 
primarily fell

00:02:15.190 --> 00:02:17.130 
primarily fell
as<00:02:15.250> a<00:02:15.310> form<00:02:15.610> of<00:02:15.700> stereoscopic<00:02:16.120> flatness<00:02:16.930> and

00:02:17.130 --> 00:02:17.140 
as a form of stereoscopic flatness and

00:02:17.140 --> 00:02:19.110 
as a form of stereoscopic flatness and
the<00:02:17.200> shortened<00:02:17.560> nose<00:02:17.770> more<00:02:18.520> subtle<00:02:18.910> than<00:02:19.000> we

00:02:19.110 --> 00:02:19.120 
the shortened nose more subtle than we

00:02:19.120 --> 00:02:21.450 
the shortened nose more subtle than we
had<00:02:19.240> expected<00:02:20.010> there<00:02:21.010> is<00:02:21.100> a<00:02:21.130> trade-off

00:02:21.450 --> 00:02:21.460 
had expected there is a trade-off

00:02:21.460 --> 00:02:23.190 
had expected there is a trade-off
between<00:02:21.610> accurate<00:02:22.360> light<00:02:22.510> field<00:02:22.780> rendering

00:02:23.190 --> 00:02:23.200 
between accurate light field rendering

00:02:23.200 --> 00:02:25.350 
between accurate light field rendering
and<00:02:23.350> system<00:02:23.680> performance<00:02:23.830> we<00:02:24.790> are<00:02:24.910> working<00:02:25.090> to

00:02:25.350 --> 00:02:25.360 
and system performance we are working to

00:02:25.360 --> 00:02:26.820 
and system performance we are working to
determine<00:02:25.510> the<00:02:25.870> relative<00:02:26.260> importance<00:02:26.740> of

00:02:26.820 --> 00:02:26.830 
determine the relative importance of

00:02:26.830 --> 00:02:29.360 
determine the relative importance of
different<00:02:27.190> cues<00:02:27.430> for<00:02:27.670> the<00:02:27.730> experience<00:02:28.240> of<00:02:28.420> VR

00:02:29.360 --> 00:02:29.370 
different cues for the experience of VR

00:02:29.370 --> 00:02:31.860 
different cues for the experience of VR
hand<00:02:30.370> tracking<00:02:30.790> can<00:02:30.970> be<00:02:31.000> used<00:02:31.300> to<00:02:31.510> reach<00:02:31.660> out

00:02:31.860 --> 00:02:31.870 
hand tracking can be used to reach out

00:02:31.870 --> 00:02:33.990 
hand tracking can be used to reach out
and<00:02:32.050> hold<00:02:32.260> these<00:02:32.470> image<00:02:32.770> based<00:02:32.980> puppets<00:02:33.400> the

00:02:33.990 --> 00:02:34.000 
and hold these image based puppets the

00:02:34.000 --> 00:02:35.790 
and hold these image based puppets the
feeling<00:02:34.390> is<00:02:34.480> strangely<00:02:34.900> intimate<00:02:35.380> and<00:02:35.500> that

00:02:35.790 --> 00:02:35.800 
feeling is strangely intimate and that

00:02:35.800 --> 00:02:37.800 
feeling is strangely intimate and that
is<00:02:35.920> the<00:02:36.010> power<00:02:36.220> of<00:02:36.370> near-field<00:02:36.790> VR<00:02:37.240> it

00:02:37.800 --> 00:02:37.810 
is the power of near-field VR it

00:02:37.810 --> 00:02:39.780 
is the power of near-field VR it
leverages<00:02:38.320> a<00:02:38.350> user's<00:02:38.770> personal<00:02:39.190> space<00:02:39.490> to

00:02:39.780 --> 00:02:39.790 
leverages a user's personal space to

00:02:39.790 --> 00:02:41.520 
leverages a user's personal space to
provide<00:02:40.090> an<00:02:40.270> experience<00:02:40.480> of<00:02:40.960> perceptual

00:02:41.520 --> 00:02:41.530 
provide an experience of perceptual

00:02:41.530 --> 00:02:45.030 
provide an experience of perceptual
intimacy<00:02:42.810> this<00:02:43.810> intimacy<00:02:44.320> demands<00:02:44.950> the

00:02:45.030 --> 00:02:45.040 
intimacy this intimacy demands the

00:02:45.040 --> 00:02:46.830 
intimacy this intimacy demands the
specular<00:02:45.460> reflections<00:02:46.000> iridescent

00:02:46.830 --> 00:02:46.840 
specular reflections iridescent

00:02:46.840 --> 00:02:49.080 
specular reflections iridescent
subsurface<00:02:47.500> scattering<00:02:47.910> transparency<00:02:48.910> and

00:02:49.080 --> 00:02:49.090 
subsurface scattering transparency and

00:02:49.090 --> 00:02:51.330 
subsurface scattering transparency and
fine<00:02:49.630> geometric<00:02:50.230> details<00:02:50.620> and<00:02:50.890> image<00:02:51.130> based

00:02:51.330 --> 00:02:51.340 
fine geometric details and image based

00:02:51.340 --> 00:02:54.420 
fine geometric details and image based
techniques<00:02:51.730> provide<00:02:52.860> the<00:02:53.860> technique<00:02:54.250> has

00:02:54.420 --> 00:02:54.430 
techniques provide the technique has

00:02:54.430 --> 00:02:56.070 
techniques provide the technique has
artifacts<00:02:54.970> that<00:02:55.210> we<00:02:55.330> wish<00:02:55.510> to<00:02:55.630> address<00:02:55.810> by

00:02:56.070 --> 00:02:56.080 
artifacts that we wish to address by

00:02:56.080 --> 00:02:57.720 
artifacts that we wish to address by
implementing<00:02:56.470> vertical<00:02:56.920> parallax<00:02:57.370> float

00:02:57.720 --> 00:02:57.730 
implementing vertical parallax float

00:02:57.730 --> 00:02:59.700 
implementing vertical parallax float
fields<00:02:58.120> and<00:02:58.270> relighting<00:02:58.690> yet<00:02:59.290> our<00:02:59.410> current

00:02:59.700 --> 00:02:59.710 
fields and relighting yet our current

00:02:59.710 --> 00:03:01.740 
fields and relighting yet our current
results<00:03:00.040> indicate<00:03:00.430> it's<00:03:00.700> simple<00:03:01.150> angular<00:03:01.570> ly

00:03:01.740 --> 00:03:01.750 
results indicate it's simple angular ly

00:03:01.750 --> 00:03:03.990 
results indicate it's simple angular ly
dependent<00:03:02.230> textures<00:03:02.650> coupled<00:03:03.370> with<00:03:03.400> organic

00:03:03.990 --> 00:03:04.000 
dependent textures coupled with organic

00:03:04.000 --> 00:03:06.150 
dependent textures coupled with organic
stop-motion<00:03:04.630> art<00:03:04.840> can<00:03:05.410> play<00:03:05.590> a<00:03:05.620> role<00:03:05.860> in<00:03:05.920> the

00:03:06.150 --> 00:03:06.160 
stop-motion art can play a role in the

00:03:06.160 --> 00:03:08.160 
stop-motion art can play a role in the
design<00:03:06.430> of<00:03:06.460> virtual<00:03:06.880> environments<00:03:07.420> the

00:03:08.160 --> 00:03:08.170 
design of virtual environments the

00:03:08.170 --> 00:03:10.140 
design of virtual environments the
pipeline<00:03:08.560> provides<00:03:08.950> a<00:03:09.100> compelling<00:03:09.520> subset<00:03:10.120> of

00:03:10.140 --> 00:03:10.150 
pipeline provides a compelling subset of

00:03:10.150 --> 00:03:12.150 
pipeline provides a compelling subset of
visceral<00:03:10.570> cues<00:03:10.870> often<00:03:11.380> missed<00:03:11.620> with<00:03:11.800> religion

00:03:12.150 --> 00:03:12.160 
visceral cues often missed with religion

00:03:12.160 --> 00:03:14.250 
visceral cues often missed with religion
or<00:03:12.190> representations<00:03:13.030> it<00:03:13.510> is<00:03:13.540> very<00:03:13.870> kind<00:03:14.170> to

00:03:14.250 --> 00:03:14.260 
or representations it is very kind to

00:03:14.260 --> 00:03:18.090 
or representations it is very kind to
the<00:03:14.320> GPU<00:03:15.300> in<00:03:16.300> the<00:03:16.480> meantime<00:03:16.690> we<00:03:17.410> enjoy<00:03:17.620> walking

00:03:18.090 --> 00:03:18.100 
the GPU in the meantime we enjoy walking

00:03:18.100 --> 00:03:19.530 
the GPU in the meantime we enjoy walking
around<00:03:18.340> our<00:03:18.460> menagerie<00:03:18.910> of<00:03:19.060> characters<00:03:19.510> that

00:03:19.530 --> 00:03:19.540 
around our menagerie of characters that

00:03:19.540 --> 00:03:21.479 
around our menagerie of characters that
tap<00:03:19.989> near<00:03:20.230> field<00:03:20.500> perception<00:03:21.040> to

00:03:21.479 --> 00:03:21.489 
tap near field perception to

00:03:21.489 --> 00:03:22.860 
tap near field perception to
algorithmically<00:03:22.120> bring<00:03:22.420> life<00:03:22.600> to

00:03:22.860 --> 00:03:22.870 
algorithmically bring life to

00:03:22.870 --> 00:03:27.750 
algorithmically bring life to
graphically<00:03:23.230> rich<00:03:23.470> nuanced<00:03:24.070> characters<00:03:26.760> but

00:03:27.750 --> 00:03:27.760 
graphically rich nuanced characters but

00:03:27.760 --> 00:03:30.420 
graphically rich nuanced characters but
wait<00:03:28.060> there's<00:03:28.540> more<00:03:28.930> we<00:03:29.800> have<00:03:29.920> developed<00:03:30.370> a

00:03:30.420 --> 00:03:30.430 
wait there's more we have developed a

00:03:30.430 --> 00:03:32.550 
wait there's more we have developed a
unique<00:03:30.459> redirected<00:03:31.450> walking<00:03:31.810> toolkit<00:03:32.320> which

00:03:32.550 --> 00:03:32.560 
unique redirected walking toolkit which

00:03:32.560 --> 00:03:34.410 
unique redirected walking toolkit which
enables<00:03:33.040> the<00:03:33.160> exploration<00:03:33.850> of<00:03:33.970> the<00:03:34.060> entire

00:03:34.410 --> 00:03:34.420 
enables the exploration of the entire

00:03:34.420 --> 00:03:36.390 
enables the exploration of the entire
menagerie<00:03:34.989> and<00:03:35.170> a<00:03:35.350> small<00:03:35.680> physical<00:03:35.920> space

00:03:36.390 --> 00:03:36.400 
menagerie and a small physical space

00:03:36.400 --> 00:03:38.640 
menagerie and a small physical space
like<00:03:37.060> a<00:03:37.090> living<00:03:37.360> room<00:03:37.630> or<00:03:37.720> even<00:03:37.810> on<00:03:38.260> a<00:03:38.290> stage<00:03:38.620> in

00:03:38.640 --> 00:03:38.650 
like a living room or even on a stage in

00:03:38.650 --> 00:03:40.350 
like a living room or even on a stage in
front<00:03:38.980> of<00:03:39.130> hundreds<00:03:39.489> of<00:03:39.550> computer<00:03:39.970> graphics

00:03:40.350 --> 00:03:40.360 
front of hundreds of computer graphics

00:03:40.360 --> 00:03:43.530 
front of hundreds of computer graphics
enthusiasts<00:03:41.160> first<00:03:42.160> we<00:03:42.700> place<00:03:42.940> waypoints

00:03:43.530 --> 00:03:43.540 
enthusiasts first we place waypoints

00:03:43.540 --> 00:03:44.970 
enthusiasts first we place waypoints
near<00:03:43.720> areas<00:03:44.050> of<00:03:44.170> interest<00:03:44.530> that<00:03:44.650> we<00:03:44.800> wish<00:03:44.950> to

00:03:44.970 --> 00:03:44.980 
near areas of interest that we wish to

00:03:44.980 --> 00:03:47.490 
near areas of interest that we wish to
sequentially<00:03:45.700> visit<00:03:46.000> we<00:03:46.840> then<00:03:47.019> plug<00:03:47.260> in<00:03:47.410> the

00:03:47.490 --> 00:03:47.500 
sequentially visit we then plug in the

00:03:47.500 --> 00:03:49.199 
sequentially visit we then plug in the
dimensions<00:03:47.920> of<00:03:48.100> our<00:03:48.190> walkable<00:03:48.610> tract<00:03:48.910> space

00:03:49.199 --> 00:03:49.209 
dimensions of our walkable tract space

00:03:49.209 --> 00:03:51.479 
dimensions of our walkable tract space
set<00:03:49.900> perceptual<00:03:50.410> thresholds<00:03:50.950> and<00:03:51.040> load<00:03:51.370> the

00:03:51.479 --> 00:03:51.489 
set perceptual thresholds and load the

00:03:51.489 --> 00:03:54.300 
set perceptual thresholds and load the
experience<00:03:52.030> as<00:03:52.470> the<00:03:53.470> user<00:03:53.680> begins<00:03:54.010> traveling

00:03:54.300 --> 00:03:54.310 
experience as the user begins traveling

00:03:54.310 --> 00:03:56.040 
experience as the user begins traveling
from<00:03:54.670> one<00:03:54.880> point<00:03:55.060> of<00:03:55.209> interest<00:03:55.450> to<00:03:55.630> the<00:03:55.720> next

00:03:56.040 --> 00:03:56.050 
from one point of interest to the next

00:03:56.050 --> 00:03:58.020 
from one point of interest to the next
our<00:03:56.230> algorithm<00:03:56.800> injects<00:03:57.160> translations<00:03:57.850> and

00:03:58.020 --> 00:03:58.030 
our algorithm injects translations and

00:03:58.030 --> 00:03:59.880 
our algorithm injects translations and
rotations<00:03:58.120> that<00:03:58.780> cause<00:03:59.019> the<00:03:59.260> user<00:03:59.380> to<00:03:59.560> walk<00:03:59.739> on

00:03:59.880 --> 00:03:59.890 
rotations that cause the user to walk on

00:03:59.890 --> 00:04:01.680 
rotations that cause the user to walk on
a<00:03:59.920> real<00:04:00.190> path<00:04:00.430> that<00:04:00.970> is<00:04:01.090> different<00:04:01.450> from<00:04:01.540> the

00:04:01.680 --> 00:04:01.690 
a real path that is different from the

00:04:01.690 --> 00:04:03.840 
a real path that is different from the
perceived<00:04:02.080> virtual<00:04:02.530> path<00:04:02.850> effectively

00:04:03.840 --> 00:04:03.850 
perceived virtual path effectively

00:04:03.850 --> 00:04:05.400 
perceived virtual path effectively
compressing<00:04:04.390> real<00:04:04.570> motions<00:04:04.989> to<00:04:05.170> a<00:04:05.200> given

00:04:05.400 --> 00:04:05.410 
compressing real motions to a given

00:04:05.410 --> 00:04:08.160 
compressing real motions to a given
bounding<00:04:05.800> box<00:04:06.030> our<00:04:07.030> system<00:04:07.450> is<00:04:07.540> unique<00:04:07.840> as<00:04:08.050> it

00:04:08.160 --> 00:04:08.170 
bounding box our system is unique as it

00:04:08.170 --> 00:04:10.199 
bounding box our system is unique as it
dynamically<00:04:08.680> optimizes<00:04:09.400> parameters<00:04:09.940> among

00:04:10.199 --> 00:04:10.209 
dynamically optimizes parameters among

00:04:10.209 --> 00:04:12.420 
dynamically optimizes parameters among
multiple<00:04:10.450> redirection<00:04:11.260> techniques<00:04:11.650> based<00:04:12.280> on

00:04:12.420 --> 00:04:12.430 
multiple redirection techniques based on

00:04:12.430 --> 00:04:15.810 
multiple redirection techniques based on
the<00:04:12.489> user's<00:04:12.820> real<00:04:13.000> path<00:04:14.340> near-field<00:04:15.340> VR

00:04:15.810 --> 00:04:15.820 
the user's real path near-field VR

00:04:15.820 --> 00:04:17.909 
the user's real path near-field VR
relies<00:04:16.269> on<00:04:16.299> cues<00:04:16.840> that<00:04:17.019> are<00:04:17.109> not<00:04:17.290> only<00:04:17.470> visual

00:04:17.909 --> 00:04:17.919 
relies on cues that are not only visual

00:04:17.919 --> 00:04:19.890 
relies on cues that are not only visual
but<00:04:18.160> also<00:04:18.310> body<00:04:18.669> based<00:04:18.970> including

00:04:19.890 --> 00:04:19.900 
but also body based including

00:04:19.900 --> 00:04:22.500 
but also body based including
proprioceptive<00:04:20.530> and<00:04:20.739> vestibular<00:04:21.510> natural

00:04:22.500 --> 00:04:22.510 
proprioceptive and vestibular natural

00:04:22.510 --> 00:04:23.940 
proprioceptive and vestibular natural
walking<00:04:22.870> is<00:04:22.990> key<00:04:23.260> to<00:04:23.410> retaining<00:04:23.740> these

00:04:23.940 --> 00:04:23.950 
walking is key to retaining these

00:04:23.950 --> 00:04:25.469 
walking is key to retaining these
physical<00:04:24.430> cues<00:04:24.700> while<00:04:24.910> moving<00:04:25.240> through<00:04:25.450> a

00:04:25.469 --> 00:04:25.479 
physical cues while moving through a

00:04:25.479 --> 00:04:28.110 
physical cues while moving through a
virtual<00:04:25.930> world<00:04:26.050> our<00:04:26.950> redirection<00:04:27.700> toolkit

00:04:28.110 --> 00:04:28.120 
virtual world our redirection toolkit

00:04:28.120 --> 00:04:28.620 
virtual world our redirection toolkit
Preserve

00:04:28.620 --> 00:04:28.630 
Preserve

00:04:28.630 --> 00:04:30.780 
Preserve
the<00:04:28.750> experience<00:04:29.260> of<00:04:29.380> near-field<00:04:29.770> VR<00:04:30.190> by

00:04:30.780 --> 00:04:30.790 
the experience of near-field VR by

00:04:30.790 --> 00:04:32.760 
the experience of near-field VR by
enabling<00:04:31.060> natural<00:04:31.690> locomotion<00:04:32.260> large

00:04:32.760 --> 00:04:32.770 
enabling natural locomotion large

00:04:32.770 --> 00:04:35.640 
enabling natural locomotion large
virtual<00:04:33.070> worlds

