WEBVTT
Kind: captions
Language: en

00:00:04.009 --> 00:00:06.469 

we've<00:00:05.009> developed<00:00:05.430> a<00:00:05.490> method<00:00:05.760> for<00:00:05.910> capturing<00:00:06.300> a

00:00:06.469 --> 00:00:06.479 
we've developed a method for capturing a

00:00:06.479 --> 00:00:08.780 
we've developed a method for capturing a
person's<00:00:06.840> 3d<00:00:07.319> body<00:00:07.589> shape<00:00:07.890> using<00:00:08.370> a<00:00:08.460> single

00:00:08.780 --> 00:00:08.790 
person's 3d body shape using a single

00:00:08.790 --> 00:00:11.000 
person's 3d body shape using a single
Kinect<00:00:09.120> that's<00:00:09.570> competitive<00:00:10.200> in<00:00:10.379> terms<00:00:10.830> of

00:00:11.000 --> 00:00:11.010 
Kinect that's competitive in terms of

00:00:11.010 --> 00:00:13.009 
Kinect that's competitive in terms of
recovered<00:00:11.490> anthropometric<00:00:12.120> and<00:00:12.540> tailoring

00:00:13.009 --> 00:00:13.019 
recovered anthropometric and tailoring

00:00:13.019 --> 00:00:15.169 
recovered anthropometric and tailoring
measurements<00:00:13.530> with<00:00:14.009> laser<00:00:14.250> scanners<00:00:14.700> costing

00:00:15.169 --> 00:00:15.179 
measurements with laser scanners costing

00:00:15.179 --> 00:00:18.440 
measurements with laser scanners costing
orders<00:00:15.450> of<00:00:15.599> magnitude<00:00:16.110> more<00:00:16.790> the<00:00:17.790> user<00:00:17.970> stands

00:00:18.440 --> 00:00:18.450 
orders of magnitude more the user stands

00:00:18.450 --> 00:00:20.359 
orders of magnitude more the user stands
in<00:00:18.630> front<00:00:18.750> of<00:00:18.930> the<00:00:19.050> Kinect<00:00:19.349> is<00:00:19.560> directed<00:00:20.220> into

00:00:20.359 --> 00:00:20.369 
in front of the Kinect is directed into

00:00:20.369 --> 00:00:25.589 
in front of the Kinect is directed into
several<00:00:20.820> poses

00:00:25.589 --> 00:00:25.599 

00:00:25.599 --> 00:00:29.759 

the<00:00:26.410> Kinect<00:00:26.710> gives<00:00:27.009> us<00:00:27.189> a<00:00:27.339> 3d<00:00:27.699> point<00:00:27.970> cloud

00:00:29.759 --> 00:00:29.769 
the Kinect gives us a 3d point cloud

00:00:29.769 --> 00:00:33.740 
the Kinect gives us a 3d point cloud
we<00:00:30.769> then<00:00:30.980> estimate<00:00:31.429> the<00:00:31.490> ground<00:00:31.820> plane

00:00:33.740 --> 00:00:33.750 
we then estimate the ground plane

00:00:33.750 --> 00:00:36.050 
we then estimate the ground plane
and<00:00:34.340> segment<00:00:35.340> the<00:00:35.460> subject<00:00:35.850> from<00:00:35.970> the

00:00:36.050 --> 00:00:36.060 
and segment the subject from the

00:00:36.060 --> 00:00:38.750 
and segment the subject from the
background<00:00:36.710> we<00:00:37.710> fit<00:00:37.890> a<00:00:37.920> scape<00:00:38.220> model<00:00:38.550> to<00:00:38.730> a

00:00:38.750 --> 00:00:38.760 
background we fit a scape model to a

00:00:38.760 --> 00:00:40.700 
background we fit a scape model to a
combination<00:00:39.150> of<00:00:39.600> silhouette<00:00:40.170> and<00:00:40.350> depth

00:00:40.700 --> 00:00:40.710 
combination of silhouette and depth

00:00:40.710 --> 00:00:43.970 
combination of silhouette and depth
evidence<00:00:41.360> the<00:00:42.360> scape<00:00:42.630> model<00:00:42.930> is<00:00:43.080> a<00:00:43.110> learned<00:00:43.440> 3d

00:00:43.970 --> 00:00:43.980 
evidence the scape model is a learned 3d

00:00:43.980 --> 00:00:46.040 
evidence the scape model is a learned 3d
model<00:00:44.340> of<00:00:44.489> the<00:00:44.670> human<00:00:44.820> body<00:00:45.120> that<00:00:45.390> factors<00:00:45.780> non

00:00:46.040 --> 00:00:46.050 
model of the human body that factors non

00:00:46.050 --> 00:00:48.260 
model of the human body that factors non
rigid<00:00:46.230> deformations<00:00:46.980> due<00:00:47.250> to<00:00:47.400> posed<00:00:47.640> from

00:00:48.260 --> 00:00:48.270 
rigid deformations due to posed from

00:00:48.270 --> 00:00:51.710 
rigid deformations due to posed from
those<00:00:48.510> due<00:00:48.750> to<00:00:48.900> intrinsic<00:00:49.260> shape<00:00:50.270> we<00:00:51.270> optimize

00:00:51.710 --> 00:00:51.720 
those due to intrinsic shape we optimize

00:00:51.720 --> 00:00:53.869 
those due to intrinsic shape we optimize
over<00:00:52.020> a<00:00:52.140> space<00:00:52.440> that<00:00:52.739> includes<00:00:53.100> a<00:00:53.400> separate

00:00:53.869 --> 00:00:53.879 
over a space that includes a separate

00:00:53.879 --> 00:00:56.180 
over a space that includes a separate
pose<00:00:54.180> for<00:00:54.600> each<00:00:54.750> frame<00:00:55.080> and<00:00:55.320> a<00:00:55.800> single

00:00:56.180 --> 00:00:56.190 
pose for each frame and a single

00:00:56.190 --> 00:01:03.149 
pose for each frame and a single
intrinsic<00:00:56.730> body<00:00:56.910> shape<00:00:57.210> across<00:00:57.480> all<00:00:57.750> frames

00:01:03.149 --> 00:01:03.159 

00:01:03.159 --> 00:01:05.830 

our<00:01:04.159> cost<00:01:04.460> function<00:01:04.670> begins<00:01:05.180> with<00:01:05.420> a<00:01:05.449> novel

00:01:05.830 --> 00:01:05.840 
our cost function begins with a novel

00:01:05.840 --> 00:01:07.930 
our cost function begins with a novel
formulation<00:01:06.530> of<00:01:06.710> bi-directional<00:01:06.979> silhouette

00:01:07.930 --> 00:01:07.940 
formulation of bi-directional silhouette

00:01:07.940 --> 00:01:09.670 
formulation of bi-directional silhouette
overlap<00:01:08.420> that<00:01:09.049> uses<00:01:09.409> implicit

00:01:09.670 --> 00:01:09.680 
overlap that uses implicit

00:01:09.680 --> 00:01:11.859 
overlap that uses implicit
correspondences<00:01:10.670> to<00:01:10.909> allow<00:01:11.180> for<00:01:11.420> analytic

00:01:11.859 --> 00:01:11.869 
correspondences to allow for analytic

00:01:11.869 --> 00:01:14.380 
correspondences to allow for analytic
derivatives<00:01:12.549> however<00:01:13.549> a<00:01:13.729> good<00:01:14.000> fit<00:01:14.210> to

00:01:14.380 --> 00:01:14.390 
derivatives however a good fit to

00:01:14.390 --> 00:01:16.390 
derivatives however a good fit to
silhouette<00:01:14.810> evidence<00:01:15.200> can<00:01:15.470> still<00:01:15.680> be<00:01:15.799> wildly

00:01:16.390 --> 00:01:16.400 
silhouette evidence can still be wildly

00:01:16.400 --> 00:01:18.760 
silhouette evidence can still be wildly
wrong<00:01:16.640> in<00:01:16.880> pose<00:01:17.090> and<00:01:17.360> shape<00:01:17.570> the<00:01:18.350> addition<00:01:18.680> of

00:01:18.760 --> 00:01:18.770 
wrong in pose and shape the addition of

00:01:18.770 --> 00:01:20.320 
wrong in pose and shape the addition of
the<00:01:18.890> connect<00:01:19.190> take<00:01:19.549> the<00:01:19.700> image<00:01:20.000> fitting

00:01:20.320 --> 00:01:20.330 
the connect take the image fitting

00:01:20.330 --> 00:01:22.810 
the connect take the image fitting
framework<00:01:20.750> allows<00:01:21.500> us<00:01:21.710> to<00:01:21.860> directly<00:01:22.340> penalize

00:01:22.810 --> 00:01:22.820 
framework allows us to directly penalize

00:01:22.820 --> 00:01:24.730 
framework allows us to directly penalize
deviations<00:01:23.570> in<00:01:23.750> depth<00:01:23.960> between<00:01:24.409> the<00:01:24.500> point

00:01:24.730 --> 00:01:24.740 
deviations in depth between the point

00:01:24.740 --> 00:01:26.820 
deviations in depth between the point
cloud<00:01:24.920> and<00:01:24.950> the<00:01:25.250> front<00:01:25.490> face<00:01:25.670> of<00:01:25.700> the<00:01:25.940> model

00:01:26.820 --> 00:01:26.830 
cloud and the front face of the model

00:01:26.830 --> 00:01:29.680 
cloud and the front face of the model
our<00:01:27.830> method<00:01:28.159> is<00:01:28.250> tolerant<00:01:28.909> of<00:01:28.970> a<00:01:29.090> wide<00:01:29.330> range

00:01:29.680 --> 00:01:29.690 
our method is tolerant of a wide range

00:01:29.690 --> 00:01:31.990 
our method is tolerant of a wide range
of<00:01:29.899> input<00:01:30.080> poses<00:01:30.649> and<00:01:30.920> the<00:01:30.950> scape<00:01:31.670> model

00:01:31.990 --> 00:01:32.000 
of input poses and the scape model

00:01:32.000 --> 00:01:33.880 
of input poses and the scape model
allows<00:01:32.330> us<00:01:32.539> to<00:01:32.720> synthesize<00:01:33.259> the<00:01:33.500> recovered

00:01:33.880 --> 00:01:33.890 
allows us to synthesize the recovered

00:01:33.890 --> 00:01:38.230 
allows us to synthesize the recovered
body<00:01:33.920> shaping<00:01:34.580> new<00:01:34.820> poses<00:01:36.820> because<00:01:37.820> the<00:01:37.970> model

00:01:38.230 --> 00:01:38.240 
body shaping new poses because the model

00:01:38.240 --> 00:01:39.880 
body shaping new poses because the model
is<00:01:38.330> learned<00:01:38.570> from<00:01:38.840> a<00:01:38.960> data<00:01:39.170> set<00:01:39.440> of<00:01:39.470> several

00:01:39.880 --> 00:01:39.890 
is learned from a data set of several

00:01:39.890 --> 00:01:41.529 
is learned from a data set of several
thousand<00:01:40.280> individuals<00:01:40.399> with<00:01:41.090> both<00:01:41.240> laser

00:01:41.529 --> 00:01:41.539 
thousand individuals with both laser

00:01:41.539 --> 00:01:43.990 
thousand individuals with both laser
scans<00:01:41.929> and<00:01:42.200> measurements<00:01:42.710> available<00:01:42.850> we<00:01:43.850> can

00:01:43.990 --> 00:01:44.000 
scans and measurements available we can

00:01:44.000 --> 00:01:45.669 
scans and measurements available we can
regress<00:01:44.450> from<00:01:44.750> model<00:01:45.050> parameters<00:01:45.500> to

00:01:45.669 --> 00:01:45.679 
regress from model parameters to

00:01:45.679 --> 00:01:48.249 
regress from model parameters to
measurements<00:01:46.190> we<00:01:47.090> tested<00:01:47.450> this<00:01:47.600> on<00:01:47.869> fifths

00:01:48.249 --> 00:01:48.259 
measurements we tested this on fifths

00:01:48.259 --> 00:01:50.139 
measurements we tested this on fifths
both<00:01:48.440> with<00:01:48.740> and<00:01:49.009> without<00:01:49.100> the<00:01:49.429> depth<00:01:49.610> term<00:01:49.909> and

00:01:50.139 --> 00:01:50.149 
both with and without the depth term and

00:01:50.149 --> 00:01:52.210 
both with and without the depth term and
compared<00:01:50.899> to<00:01:51.080> the<00:01:51.170> model<00:01:51.380> fit<00:01:51.619> directly<00:01:51.800> to

00:01:52.210 --> 00:01:52.220 
compared to the model fit directly to

00:01:52.220 --> 00:01:54.249 
compared to the model fit directly to
laser<00:01:52.399> scans<00:01:52.789> as<00:01:53.090> well<00:01:53.600> as<00:01:53.780> to<00:01:53.899> a<00:01:53.929> commercial

00:01:54.249 --> 00:01:54.259 
laser scans as well as to a commercial

00:01:54.259 --> 00:01:56.620 
laser scans as well as to a commercial
laser<00:01:54.649> scan<00:01:55.009> measurement<00:01:55.490> solution<00:01:56.000> our

00:01:56.620 --> 00:01:56.630 
laser scan measurement solution our

00:01:56.630 --> 00:01:58.210 
laser scan measurement solution our
results<00:01:57.020> using<00:01:57.200> a<00:01:57.380> single<00:01:57.740> connect<00:01:58.039> are

00:01:58.210 --> 00:01:58.220 
results using a single connect are

00:01:58.220 --> 00:02:00.249 
results using a single connect are
competitive<00:01:58.789> with<00:01:59.030> systems<00:01:59.450> costing<00:01:59.899> orders

00:02:00.249 --> 00:02:00.259 
competitive with systems costing orders

00:02:00.259 --> 00:02:03.190 
competitive with systems costing orders
of<00:02:00.440> magnitude<00:02:00.950> more

